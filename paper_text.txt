1
LLM Post-Training: A Deep Dive into Reasoning
Large Language Models
Komal Kumar∗, Tajamul Ashraf∗, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal,
Mubarak Shah, Ming-Hsuan Yang, Phillip H.S. Torr, Fahad Shahbaz Khan, Salman Khan
Abstract—LargeLanguageModels(LLMs)havetransformedthenaturallanguageprocessinglandscapeandbroughttolifediverse
applications.Pretrainingonvastweb-scaledatahaslaidthefoundationforthesemodels,yettheresearchcommunityisnow
increasinglyshiftingfocustowardpost-trainingtechniquestoachievefurtherbreakthroughs.Whilepretrainingprovidesabroad
linguisticfoundation,post-trainingmethodsenableLLMstorefinetheirknowledge,improvereasoning,enhancefactualaccuracy,and
alignmoreeffectivelywithuserintentsandethicalconsiderations.Fine-tuning,reinforcementlearning,andtest-timescalinghave
emergedascriticalstrategiesforoptimizingLLMsperformance,ensuringrobustness,andimprovingadaptabilityacrossvarious
real-worldtasks.Thissurveyprovidesasystematicexplorationofpost-trainingmethodologies,analyzingtheirroleinrefiningLLMs
beyondpretraining,addressingkeychallengessuchascatastrophicforgetting,rewardhacking,andinference-timetrade-offs.We
highlightemergingdirectionsinmodelalignment,scalableadaptation,andinference-timereasoning,andoutlinefutureresearch
directions.Wealsoprovideapublicrepositorytocontinuallytrackdevelopmentsinthisfast-evolvingfield:
https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.
IndexTerms—ReasoningModels,LargeLanguageModels,ReinforcementLearning,RewardModeling,Test-timeScaling
✦
1 Introduction
C LLM post-training alignment
ontemporary Large Language Models (LLMs) exhibit
GPT-3
remarkable capabilities across a vast spectrum of tasks, Algorithmic categorization Claude2 GPT-4, 4O, O1
encompassingnotonlytextgeneration[1,2,3]andquestion- Algorithms Claude3 Mistral Large 2
answering[4,5,6,7],butalsosophisticatedmulti-stepreason- Qwen-32B-Preview Gemini 1.5
ing[8,9,10,11].Theypowerapplicationsinnaturallanguage LLMs DeepSeek-R1 AlphaGo Beam Search
understanding[12,13,14,15,16,17],contentgeneration[18, LlaMA 3.3 Monte Carlo Search
GPT-O1, O3 LlaMA 3.1 Best-of-N Search
19,20,21,22,23,24,25],automatedreasoning[26,27,28,29], Chain-of-Thought
Mistral Large 2 Tree-of-Thoughts
andmultimodalinteractions[30,31?,33].Byleveragingvast
Qwen-32B-Preview Confidence Sampling Consistency Decoding
self-supervised training corpora, these models often approxi-
DeepSeek-R1 TRPO Search Against Verifiers
t M
l
o a d T
m i t f u m a
e
o
f p i h
c c
s
a
n o p
p
t
t i
i d
t
c r
c D
s
n
i
u
e
r e
e y e
r
n
e
a
a a
o
i e
b d c
h s t
r
t l s
t v a i t
s i i l u p
h s
c
e
y o i
t l
m
t
v a i
r r e o
n
f
t
i o ,
e
r
l
. n g
i e s a
o u
n
t i c
” a n
W
s
m
t
c g h
c
t
) d h h -
a
o
h i e
l a
o
o
h
e
l
i r a
o l
k
h
p
n
r s r
i y c
n
u l
t e e e
u
t
o e
c d
t
c a
i m n
i
c
c
s
o t m b
e o t
o
c
m
a
m i
h c x e h
g p l c
n r p
i
e t
i n
e
o a r t
u
n
- e s t r
e y i n y
l e n c
g t
e i
s t
k i
i
o
s
n d
i s
a
e o s n .
m e f
i
t e
n
l
n t v
, , d
r
r
r t T
o
e
l
u
e
[ e
a o t d
3 h
d
a
a
a g (
h s g e
4 c c e
s i
l g
e i l s
- , h
o
o
i
y
c s
l
i
w
c t
e
r a n
3 m i
o
e o
l
5
c i h
c
u
v
n r
m t r
a
, a
e e
e
i r
o l
g n
n
n l
3 d
s a
o m
p e
6
f s
n
i
m
e s
s
p
e
n o
, g l e
r [
n y
r n
e a
4
3
e e
t
o L
t n
x i
i
1
7 t
n
s
n
n
d L
i r e
,
,
p c
, n t e
g u M
r
e l
3
4
L a g f a
a c s
e 8
2
i L s
i
t
i e
n
[
r ]
s n ,
. e M
4
,
r
r s
e 4 7
e
r
d
f
l
w
m
e m
d
3 u ,
o e r
s , h
g
n
e m i
a p 3
s t i
y
m
d 4 i 4
c
o
l o o
n 4 a
e a
, n
a
L
n
s ,
a
m
l
s
i a
L
s
4
d n
4 e a
s t
e M
c
8
i
s 5
r
n
n o p
s ,
a
, t
“
t
n g r
t o
t h
a 4 4
o
c h
s i
p
a
l a
n
6 9
i n o
a l
s
i
l
n ] ] y
g e
c
r
t
- -
. . E D S X t i A a s A
L
O t r i
l
l L l
a
i M N
C
Q B n B
M l
E E g i E w
a
n
L A
- R
u
R 3 e i
l
7 L T
d a
T . n
3
B 0 M
M e .2 3 A G . 5 3 r o . S 3 k o G n e
K
n m
A L n
e
P E o
O i
o d
t
D n r
n
w w a o
f
i d
i f
s p
B
m l
1 l
- - t e
i . e
t i
n 5
t
R
R d l p e
h
o l
e L
a a g r t
a S
-
A
t e n
B
s
v
E
e
i
I
o
a
k
i l F
n
o f t
n
- c r
d
C h C R r V L l i o t R H a i n q n E F i u
T
i n
r P
I
ai ni n
l
g
N e l g
a
a O F D
r
P R O
m F
P
O
G E R O
u
R
f
O
.
C L
l f E l
P
l
O E
i M
P p G
n f
t
f
O i
e
R m
i o c
P iz
P d i
a O
e
t
o e
i R o
n l l
n
i
e
c t
w
y
ard P
T
oli
u
c
§
y
n R
C
4
o
e i §
n
n i
fi
n
d
3 g
e D
f
n
o
e ce c
r
o
c
d
e
in R e g a so n § in g 5 S c
t
a lin
r g P L a o i L
S e a rc h
n L s M L i M t n g K
Fig. 1: A taxonomy of post-training approaches for LLMs
• ∗Equal contribution. Corresponding authors (Email: ko- (LLMs), categorized into Fine-tuning, Reinforcement Learn-
mal.kumar@mbzuai.ac.ae,tajamul.ashraf@mbzuai.ac.ae)
ing, and Test-time Scaling methods. We summarize the key
• KomalKumar,TajamulAshraf,OmkarThawakar,RaoMuham-
mad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan, and techniques used in recent LLM models, such as GPT-4 [39],
SalmanKhanarewithMohamedbinZayedUniversityofArtificial LLaMA3.3[13],andDeepseekR1[40].
Intelligence,AbuDhabi,UAE.
• MubarakShahiswiththeCenterforResearchinComputerVision
attheUniversityofCentralFlorida,Orlando,FL32816,USA.
• Ming-HsuanYangiswiththeUniversityofCaliforniaatMerced, producecompellingoutputswhilestillstumblingonrelatively
Merced,CA95343USA,andalsowithGoogleDeepMind,Moun-
simple logical tasks. Unlike symbolic reasoning that manipu-
tainView,CA94043,USA.
• PhilipH.S.TorriswiththeDepartmentofEngineeringScience, latesexplicitrulesandfacts,LLMsoperatein animplicit and
UniversityofOxford,OxfordOX12JD,UK. probabilistic manner [50, 42, 51]. For the scope of this work,
5202
raM
42
]LC.sc[
2v12312.2052:viXra
2
‘reasoning’inLLMsreferstotheirabilitytogeneratelogically fromavastvocabulary,andtheirevolvingstatecomprisesan
coherentresponsesbasedonstatisticalpatternsindatarather ever-growing text sequence [16, 59, 76, 57]. This complicates
than explicit logical inference or symbolic manipulation. Ad- planning and credit assignment, as the impact of token se-
ditionally, models trained purely via next-token prediction lection may only emerge later. Feedback in language-based
can fail to align with user expectations or ethical standards, RL is also sparse [77], subjective, and delayed, relying on
especially in ambiguous or malicious scenarios [4, 52]. These heuristic evaluations and user preferences rather than clear
issues underscore the need for specialized strategies that ad- performance metrics [78, 79, 58]. Additionally, LLMs must
dressreliability,bias,andcontextsensitivityinLLMoutputs. balance multiple, sometimes conflicting, objectives, unlike
LLMs training can be broadly categorized into two stages: conventional RL, which typically optimizes for a single goal.
pre-training,whichgenerallyreliesonanext-tokenprediction Hybrid approaches combining process-based rewards (e.g.,
objective over large-scale corpora, and post-training, encom- chain-of-thought reasoning) with outcome-based evaluations
passing multiple rounds of fine-tuning and alignment. Post- (e.g., response quality) help refine learning [8, 80, 81]. Thus,
training mechanisms aim to mitigate LLMs limitations by RL for LLMs requires specialized optimization techniques to
refining model behavior and aligning outputs with human handle high-dimensional outputs, non-stationary objectives,
intent,mitigatingbiasesorinaccuracies[53]. and complex reward structures, ensuring responses remain
Adapting LLMs to domain-specific tasks often involves contextuallyrelevantandalignedwithuserexpectations.
techniques like fine-tuning [54, 55, 56], which enables task-
specific learning but risks overfitting and incurs high com-
putational costs. To address these challenges, approaches Reinforcement in LLMs extends beyond con-
such as Reinforcement Learning (RL) [57, 58, 59] en- ventional RL as it navigates vast action spaces,
hance adaptability by leveraging dynamic feedback and op- handlessubjectiveanddelayedrewards,andbal-
timizing sequential decision-making. Additionally, advances ances multiple objectives, necessitating special-
in scaling techniques, including Low-Rank Adaptation izedoptimizationtechniques.
(LoRA) [60], adapters [365? ], and Retrieval-Augmented Gen-
eration (RAG) [61, 62, 63], improve both computational effi- c)TestTimeScalinginLLMs:TestTimeScalingisoptimiz-
ciency and factual accuracy. These strategies, coupled with ingmodelperformanceandefficiencywithoutalteringthecore
distributedtrainingframeworks,facilitatelarge-scaledeploy- architecture.Itenablesbettergeneralizationwhileminimizing
ment and further boost the usability of LLMs across diverse computationaloverhead.Itiscrucialforenhancingtheperfor-
applications(Figure1).Throughthesetargetedpost-training manceandefficiencyof LLMs.Ithelpsimprovegeneralization
interventions,LLMsbecomebetteralignedwithhumanintent across tasks but introduces significant computational chal-
and ethical requirements, ultimately enhancing their real- lenges[82,83].Balancingperformanceandresourceefficiency
world applicability. Below, we summarize key post-training requires targeted strategies at inference. Techniques like
stages. CoT[8]reasoningandTree-of-Thought(ToT)[84]frameworks
a) Fine-Tuning in LLMs: Fine-tuning adapts pre-trained enhance multi-step reasoning by breaking down complex
LLMstospecifictasksordomainsbyupdatingparameterson problems into sequential or tree-structured steps. Addition-
curated datasets [64, 65, 66, 54, 55, 67, 56]. While LLMs gen- ally, search-based techniques[85, 86, 87, 88] enable iterative
eralizewellafterlarge-scalepretraining,fine-tuningenhances exploration of possible outputs, helping refine responses and
performanceintaskslikesentimentanalysis[68,69],question ensure higher factual accuracy. These approaches, combined
answering, and domain-specific applications such as medical withmethodslikeLoRA[60],adapters,andRAG[61,62,89],op-
diagnosis [70, 71, 72]. This process, typically supervised, timizethemodel’sabilitytohandlecomplex,domain-specific
alignsmodelswithtaskrequirementsbutposeschallengeslike tasksatscale.RAGenhancesfactualaccuracybydynamically
overfitting, high computational costs, and sensitivity to data retrievingexternalknowledge,mitigatinglimitationsofstatic
biases[56,31,16].Tothisend,parameter-efficienttechniques training data [62, 24, 90]. Distributed training frameworks
like LoRA [60] and adapters learn task-specific adaptation by leverageparallelprocessingtomanagethehighcomputational
updating explicit parameters, significantly reducing compu- demands of large-scale models. Test-time scaling optimizes
tational overhead. As models specialize, they may struggle inferencebyadjustingparametersdynamicallybasedontask
without-of-domaingeneralization,underscoringthetrade-off complexity [83, 91]. Modifying depth, width, or active layers
betweenspecificityandversatility. balancescomputationalefficiencyandoutputquality,making
itvaluableinresource-limitedorvariableconditions.Despite
advancements,scalingpresentschallengessuchasdiminishing
Fine-tuning tailors LLMs for specific tasks, returns, longer inference times, and environmental impact,
improving performance but risking overfitting,
especially when search techniques are performed at test time
highcomputecosts,andreducedgeneralization.
rather than during training [82]. Ensuring accessibility and
feasibility is essential to maintain high-quality, efficient LLM
b) Reinforcement Learning in LLMs: In conventional RL, deployment.
an agent interacts with a structured environment, taking
discrete actions to transition between states while maximiz-
ing cumulative rewards [73]. RL domains—such as robotics, Test-time scaling enhances the adaptability
boardgames,andcontrolsystems—featurewell-definedstate- of LLMs by dynamically adjusting computational
action spaces and clear objectives [74, 75]. RL in LLMs differs resourcesduringinference.
significantly. Instead of a finite action set, LLMs select tokens
3
1.1 Prior Surveys
Token-wise training can ensure fluency but
Recent surveys on RL and LLMs provide valuable insights may cause cascading errors due to uncorrected
but often focus on specific aspects, leaving key post-training
mistakesininference.
components underexplored [51, 92, 93, 94]. Many works ex-
amine RL techniques like Reinforcement Learning from Hu- As these models scale, they exhibit emergent reasoning
man Feedback (RLHF) [58], Reinforcement Learning from AI
abilities, particularly when trained on diverse data that in-
Feedback (RLAIF) [95], and Direct Preference Optimization
clude code and mathematical content [108, 8]. However, de-
(DPO) [57], yet they overlook fine-tuning, scaling, and critical
spite their impressive capabilities, LLMs struggle to maintain
benchmarks essential for real-world applications. Further-
coherence and contextual relevance over long sequences. Ad-
more,thesestudieshavenotexploredthepotentialof RLeven
dressing these limitations necessitates a structured approach
without human annotation supervised finetuning in various
tosequencegeneration,whichnaturallyalignswithRL.
frameworks such as DeepSeek R1 with GRPO [59]. Other sur-
SinceLLMsgeneratetextautoregressively—whereeachto-
veys explore LLMs in traditional RL tasks, such as multi-task
ken prediction depends on previously generated tokens—this
learninganddecision-making,buttheyprimarilyclassifyLLM
processcanbemodeledasasequentialdecision-makingprob-
functionalities rather than addressing test-time scaling and
lemwithinaMarkovDecisionProcess(MDP)[109].Inthisset-
integrated post-training strategies [96, 97]. Similarly, studies
ting,thestates representsthesequenceoftokensgenerated
on LLM reasoning [98, 99, 100, 55, 101, 102, 103, 104] discuss t
so far, theaction a is the next token, anda rewardR(s ,a )
learning-to-reason techniques but lack structured guidance t t t
evaluates the quality of the output. An LLM ’s policy π is
on combining fine-tuning, RL, and scaling. The absence of θ
optimizedtomaximizetheexpectedreturn:
tutorials, along with reviews of software libraries and imple-
mentation tools, further limits their practicality. In contrast, hX ∞ i
thissurveyoffersacomprehensiveviewofLLMpost-trainingas J(π
θ
)=E γtR(s
t
,a
t
) ,
shown inFigure 1by systematicallycoveringfine-tuning,RL, t=0
andscalingasinterconnectedoptimizationstrategies.Weoffer where γ is the discount factor that determines how strongly
practical resources—benchmarks, datasets, and tutorials—to future rewards influence current decisions. A higher γ places
aidLLMrefinementforreal-worldapplications. greater importance on long-term rewards. The primary ob-
jective in RL is to learn a policy that maximizes the ex-
pected cumulative reward, often referred to as the return.
1.2 Contributions This requires balancing exploration—trying new actions to
Thekeycontributionsofthissurveyareasfollows: discover their effects—and exploitation—leveraging known
We provide a comprehensive and systematic review of actions that yield high rewards. While LLMs optimize a like-
• post-training methodologies for LLMs, covering fine- lihood function using static data, RL instead optimizes the
expectedreturnthroughdynamicinteractions.Toensurethat
tuning,RL,andscalingasintegralcomponentsofmodel
optimization. LLMs generate responses that are not only statistically likely
but also aligned with human preferences, it is essential to go
•
Weofferastructuredtaxonomyofpost-trainingtech-
beyond static optimization methods. While likelihood-based
niques, clarifying their roles and interconnections, and
training captures patterns from vast corpora, it lacks the
presentinsightsintoopenchallengesandfutureresearch
adaptabilityneededforrefiningdecision-makingininteractive
directionsinoptimizingLLMsforreal-worlddeployment.
settings. By leveraging structured approaches to maximizing
Our survey provides practical guidance by introducing
• long-term objectives, models can dynamically adjust their
key benchmarks, datasets, and evaluation metrics
strategies,balancingexplorationandexploitationtoimprove
essential for assessing post-training effectiveness, ensur-
reasoning,coherence,andalignment[110,111,49,48].
ingastructuredframeworkforreal-worldapplications.
LLMs exhibit emergent abilities due to scale,
2 Background while RL refines and aligns them for better rea-
soningandinteraction.
The LLMs have transformed reasoning by learning to predict
the next token in a sequence based on vast amounts of text
data [105, 4] using Maximum Likelihood Estimation (MLE)
[106, 3, 107], which maximizes the probability of generating 2.1 RL based Sequential Reasoning.
the correct sequence given an input. This is achieved by The chain-of-thought reasoning employed in modern LLMs is
minimizingthenegativelog-likelihood: naturally framed as an RL problem. In this perspective, each
intermediatereasoningstepistreatedasanactioncontribut-
X T ingtoafinalanswer.TheobjectivefunctionJ(π )represents
L =− logP (y |y ,X). θ
MLE θ t <t the expected reward of the policy π , capturing how well
θ
t=1 themodelperformsovermultiplereasoningsteps.Thepolicy
Here, X represents the input, such as a prompt or context. gradientupdateisgivenby:
Y = (y ,y ,...,y ) is the corresponding target output se-
1 2 T " T #
quence, and P θ (y t | y <t ,X) denotes the model’s predicted ∇ J(π )=E X ∇ logπ (x |x )A(s ,a ) ,
probabilityfortokeny ,givenprecedingtokens. θ θ τ θ θ t 1:t−1 t t
t
t=1
4
where the advantage function A(s ,a ) distributes credit to own highest-scoring output, ensuring that updates directly
t t
individual steps, ensuring that the overall reasoning process improve performance relative to what the model currently
is refined through both immediate and delayed rewards. considersitsbestresponse.Thegradientupdatefollows:
Suchformulations,includingstep-wiserewarddecomposition (cid:16) (cid:17)
[112,113],havebeencrucialforenhancingtheinterpretability ∇ θ J(π θ )≈ r(ys)−r(yˆ) ∇ θ logπ θ (ys),
and performance of LLMs on complex reasoning tasks. In where ys is a sampled sequence, yˆis the greedy output, and
traditionalRLformulations,anagenthas:
r(y) represents an evaluation metric such as BLEU [118] for
Valuefunction:V(s) = E(cid:2)futurereturn | s (cid:3) , translation or CIDEr [119] for image captioning. Since the
learning signal is based on the difference r(ys) − r(yˆ), the
Action-value(Q-)function:Q(s,a) = E(cid:2)futurereturn | s,a (cid:3) , model is explicitly trained to generate outputs that score
higher than its own baseline under the evaluation metric.
Advantagefunction:A(s,a) = Q(s,a)−V(s).
If the sampled output outperforms the greedy output, the
In words, A(s,a) measures how much better or worse it is to model reinforces it; otherwise, it discourages that sequence.
takeaspecificactionainstatescomparedtowhattheagent This direct feedback loop ensures that training aligns with
wouldnormally expect(itsbaselineV(s)). the desired evaluation criteria rather than just maximizing
likelihood. By leveraging the model’s own best predictions
2.2 Early RL Methods for Language Modeling. as a baseline, SCST effectively reduces variance and stabilizes
Here, we briefly overview pioneering methods that laid the trainingwhileoptimizingreal-worldperformancemetrics.
groundwork for applying RL to language generation tasks. Minimum Risk Training (MRT). MRT [151] directly mini-
These initial efforts train a decision-making model (policy mizestheexpectedriskovertheoutputdistribution.Givena
(p )) by directly adjusting its parameters to maximize re- task-specific loss ∆(y,y∗) comparing the generated output y
θ
wards.Somepolicygradientapproachesareexplainedbelow: withthereferencey∗,theMRTobjectiveisdefinedas:
Policy Gradient (REINFORCE). The REINFORCE algo- L (θ)= X p (y|x)∆(y,y∗).
rithm[114,115]isamethodusedtoimprovedecision-making MRT θ
by adjusting the model’s strategy (policy) based on rewards y∈Y
receivedfromitsactions.Insteadofdirectlylearningthebest This formulation incorporates evaluation metrics (e.g., 1 −
actionforeverysituation,thealgorithmrefineshowlikelydif- BLEU) directly into training, enabling fine-grained adjust-
ferentactionsaretobechosen,graduallyimprovingoutcomes mentsofthepolicy.
overtime.Ateachstep,themodelupdatesitsparameters(θ) Advantage Actor-Critic (A2C/A3C). RL methods like
basedonhowwellitspastdecisionsperformed: REINFORCE [114] rely solely on policy gradients, which suf-
fer from high variance, leading to unstable and inefficient
(cid:16) (cid:17)X T learning. Since the reward signal fluctuates across different
θ←θ+α G−b ∇ logπ (a |s ).
θ θ t t trajectories, updates may be noisy, causing slow or erratic
t=1 convergence. To mitigate this, Actor-Critic methods [152,
Here: G represents the total reward the model accumulates 153, 154, 155] combine two components as follows: an actor
over an episode, b is a baseline value that helps reduce and a critic. The actor is a policy π (a | s ) that selects
variance, making learning more stable, ∇ θ logπ θ (a t | s t ) actions a at state s , while the crit θ ic i t s a v t alue function
measureshowmuchasmallchangeinθaffectstheprobability
V (s )tha
t
tevaluatest
t
heexpectedreturnofastate.Thecritic
of choosing action a t given state s t , α is the learning rate, pr ϕ ovi t des a more stable learning signal, reducing variance in
controllinghowmuchthepolicyupdatesateachstep.
policy updates and enabling efficient learning in continuous
actionspaces.Actorupdatesareguidedbythepolicygradient
Optimizing actions based on long-term re- theorem, where the advantage function A(s t ,a t ) defined in
wards,whichaccountforthecumulativebenefits
Sec.2.1,determineshowmuchbetteranactiona
t
iscompared
ofasequenceofreasoningstepsratherthanjust
totheexpectedvalueofstates
t
.Thepolicywiththelearning
immediate outcomes, is fundamental in recent
rateαisupdatedas:
LLMs. This approach allows models to explore θ←θ+αA(s ,a )∇ logπ (a |s ).
t t θ θ t t
multiplereasoningpathsmoreeffectively.
Meanwhile, the critic is updated using temporal difference
learning, minimizing the squared error between its estimate
Curriculum Learning with MIXER.. Ranzato et al. [116]
andtheactualreturn:
introducesagradualtransitionfrommaximumlikelihoodesti-
mation(MLE)toRL.Theoveralllossisaweightedcombination:
ϕ←ϕ−β∇
(cid:16)
V (s )−G
(cid:17)2
.
ϕ ϕ t t
L=λ(t)L +(cid:0)1−λ(t)(cid:1) L ,
MLE RL where β is a learning rate for critic. To enhance stability
where λ(t) decreases with training time. This curriculum and efficiency, several improvements have been proposed.
helps the model ease into the RL objective and mitigate the Eligibility traces allow learning from recent states, enabling
mismatchbetweentrainingandinference. faster convergence. Function approximation with neural net-
Self-Critical Sequence Training (SCST). SCST [117] re- works ensures effective handling of high-dimensional inputs.
fines the policy gradient method by comparing the model’s Advanced variants such as Natural Gradient methods [156]
sampled outputs against its own best (greedy) predictions. adjustupdatesusingtheFisherInformationMatrix,improv-
Instead of using an arbitrary baseline, SCST uses the model’s ingconvergencespeed.
5
RLEnhancedLLMs Developer Source #Params RLMethods Fine-Tuning ArchitectureType Model TTS
DeepSeek-V2[16] Deepseek Link 236B-A21B GRPO DPO+GRPO MoE Open ✓
GPT 4.5[120] OpenAI Link - RLHF,PPO,RBRM SFT+RLHF MoE Closed ✓
Gemini[15] Google Link - RLHF SFT+RLHF SingleModel Closed ✗
Claude 3.7[121] Anthropic Link - RLAIF SFT+RLAIF SingleModel Closed ✗
Reka[122] Reka Link 7B,21B RLHF,PPO SFT+RLHF SingleModel Closed ✗
DeepSeekR1[40] Deepseek Link 240B-A22B GRPO DPO+GRPO MoE Open ✓
Nemotron-4 340B[123] NVIDIA Link 340B DPO,RPO DPO+RPO SingleModel Closed ✗
Falcon[124] TII Link 40B - SFT SingleModel Open ✗
GPT-4[39] OpenAI Link - RLHF,PPO,RBRM SFT+RLHF MoE Closed ✓
Llama 3[13] Meta Link 8B,70B,405B DPO SFT+DPO SingleModel Open ✗
Qwen2[125] Alibaba Link (0.5-72)B,57B-A14B DPO SFT+DPO SingleModel Open ✓
Gemma2[14] Google Link 2B,9B,27B RLHF SFT+RLHF SingleModel Open ✗
Starling-7B[26] Berkeley Link 7B RLAIF,PPO SFT+RLAIF SingleModel Open ✗
Moshi[126] Kyutai Link 7B - - Multi-modal Open ✓
Athene-70B[127] Nexusflow Link 70B RLHF SFT+RLHF SingleModel Open ✗
GPT-3.5[39] OpenAI Link 3.5B,175B RLHF,PPO SFT+RLHF MoE Closed ✓
Hermes 3[128] Nous Link 8B,70B,405B DPO SFT+DPO SingleModel Open ✗
Zed[129] ZedAI Link 500B RLHF RLHF Multi-modal Open ✓
PaLM 2[130] Google Link - RLHF - SingleModel Closed ✓
InternLM2[131] SAIL Link 1.8B,7B,20B RLHF,PPO SFT+RLHF SingleModel Closed ✗
Supernova[132] NovaAI Link 220B RLHF RLHF Multi-modal Open ✓
Grok3[133] Grok-3 Link 175B - DPO Dense Open ✓
Pixtral[134] MistralAI Link 12B,123B - PEFT Multimodal Open ✓
Minimaxtext[135] MiniMax Link 456B - SFT SingleModel Closed ✗
Amazonnova[136] Amazon Link - DPO,RLHF,RLAIF SFT SingleModel Closed ✗
Fugakullm[137] Fujitsu Link 13B - - SingleModel Closed ✗
Nova[138] Rubik’sAI Link - - SFT Proprietary Closed ✗
03[139] OpenAI Link - RLthroughCoT RLthroughCoT SingleModel Closed ✓
Dbrx[140] Databricks Link 136B - SFT SingleModel Open ✗
Instruct-GPT[58] OpenAI Link 1.3B,6B,175B RLHF,PPO SFT+RLHF SingleModel Closed ✗
Openassistant[141] LAION Link 17B - SFT SingleModel Open ✗
ChatGLM[142] ZhipuAI Link 6B,9B ChatGLM-RLHF SFT+RLHF SingleModel Open ✗
Zephyr[143] Argilla Link 141B-A39B ORPO DPO+ORPO MoE Open ✓
phi-3[17] Microsoft Link 3.8B,7B,14B DPO SFT+DPO SingleModel Closed ✗
Jurassic[144] AI21Labs Link - - SFT Proprietary Closed ✗
Kimi K1.5[145] MoonshotAI Link 150B - RLHF Multi-modal Open ✓
Phi-4[146] Microsoft Link 28B,70B,140B DPO SFT+DPO SingleModel Closed ✗
Chameleon[147] MetaAI Link 34B - SFT SingleModel Open ✗
Cerebrasgpt[148] Cerebras Link 13B - SFT SingleModel Open ✗
Bloomberggpt[149] BloombergL.P. Link 50B - SFT SingleModel Closed ✗
Chinchilla[150] DeepMind Link 70B RLHF,PPO SFT SingleModel Closed ✗
TABLE 1: An overview of reinforcement learning-enhanced LLMs, where ’141B-A39B’ denotes a Mixture of Experts (MoE)
modelwith141billiontotalparameters,ofwhich39billionareutilizedduringinference.TTSstandsforTest-TimeScaling.
A notable early example is Barto’s Actor-Critic model 3 Reinforced LLMs
[157], where the critic uses a linear function V ϕ (s t ) and Fromamethodologicalperspective,theintegrationof RLinto
the actor follows a linear policy. Modern methods like A2C
LLMreasoningtypicallyfollowsthreecoresteps:
(Advantage Actor-Critic) [154] and A3C (Asynchronous Ad-
vantageActor-Critic)[155]extendthisapproachbyparalleliz-
1) Supervised Fine-Tuning (SFT): Commences with a
pretrained language model that is subsequently refined
ing training across multiple environments, leading to faster
on a supervised dataset of high-quality, human-crafted
and more stable learning. By leveraging the critic’s value
examples.Thisphaseensuresthemodelacquiresabase-
estimation, actor-critic methods stabilize learning, improve
linecompliancewithformatandstyleguidelines.
sample efficiency, and accelerate convergence, making them
moreeffectiveforcomplexdecision-makingtasks.
2) Reward Model (RM) Training: Generated outputs
from the fine-tuned model are collected and subjected
to human preference labeling. The reward model is then
trained to replicate these label-based scores or rankings,
Connection with Modern Methods. The aforemen- effectively learning a continuous reward function that
tioned early RL methods—REINFORCE [114], MIXER [116], mapsresponsetexttoascalarvalue.
SeqGAN[158],SCST[117],MRT[151],andactor-criticalgorithms 3) RL Fine-Tuning: Finally, the main language model is
established the mathematical foundations for sequential rea- optimized via a policy gradient algorithm most e.g PPO
soning in LLMs. These methods provided initial solutions to tomaximizetherewardmodel’soutput.Byiteratingthis
challenges such as exposure bias and high variance. Mod- loop, the LLM learns to produce responses that humans
ern techniques such as large-scale RL from Human Feedback find preferable along key dimensions such as accuracy,
(RLHF) using PPO [73] and advanced reward models, e.g., helpfulness,andstylisticcoherence.
Group Relative Policy Optimization (GRPO) [159] build di- 4) Reward Modeling and Alignment: Sophisticated
rectly upon these ideas. By integrating sophisticated reward reward functions are developed—drawing from human
signals and leveraging efficient policy updates, contemporary preferences, adversarial feedback, or automated met-
LLMs achieve improved reasoning, safety, and alignment with rics—to guide the model toward outputs that are coher-
humanvaluesandpavethewayforrobustmulti-stepreason- ent, safe, and contextually appropriate. These rewards
ing and improved quality of generated text. Table 1 provides are critical for effective credit assignment across multi-
an overview of recent models, including their parameters, stepreasoningprocesses.
architecture types, and the distilled RL methods employed, Early approaches to aligning LLMs with human preferences
alongwithlinksforeasyaccess. leveraged classical RL algorithms, such as PPO [73] and Trust
6
Region Policy Optimization (TRPO) [160], which optimize a prefery toy ,wedenoteitasy ≻y .UnderBradley–Terry,
j k j k
policy by maximizing the expected cumulative reward while theprobabilityofy beingpreferredovery isgivenby:
j k
enforcing constraints on policy updates via a surrogate ob-
jective function and KL-divergence regularization [161]. Im- exp(cid:0) R (x, y )(cid:1)
P (cid:0) y ≻y | x;θ (cid:1) = θ j .
proved alternatives to these methods for scalable preference- j k exp(cid:0) R (x, y )(cid:1) + exp(cid:0) R (x, y )(cid:1)
θ j θ k
based optimization have emerged, such as Direct Preference
Optimization (DPO) [57, 162] and Group Relative Policy WetrainR
θ
bymaximizing thelikelihoodofobservedprefer-
Optimization (GRPO) [159, 59, 16], which reformulate the ences(orequivalentlyminimizingthenegativelog-likelihood):
alignment objective as a ranking-based contrastive loss func-
tion [163] over human-labeled preference data. Unlike PPO L BT (θ)=− X logP (cid:0) y j ≻y k |x;θ (cid:1) .
and TRPO [160], which rely on explicit reward models and (x,yj≻yk)∈D
critic networks, DPO and GRPO directly optimize the policy
by leveraging log-likelihood ratios and group-wise reward II. Plackett–Luce Model1 (Rankings). When full or
comparisons, respectively, eliminating the need for explicit partialrankings ofmresponsesareavailable,i.e.,
value function approximation while preserving preference-
consistent learning dynamics. This transition from classical y ≻y ≻···≻y ,
j1 j2 jm
RL-based alignment to preference-based direct optimization
introducesnovelformulationssuchascontrastiverankingloss, the Plackett–Luce model [165] factorizes the probability of
policylikelihoodratioregularization,andgroupedadvantage thisrankingas:
estimation,whichareexplainedinsubsequentsections.
P (cid:0) y ,...,y | x;θ (cid:1) = Y m exp(cid:0) R θ (x,y jℓ )(cid:1) .
j1 jm Pm exp(cid:0) R (x,y )(cid:1)
3.1 Reward modeling ℓ=1 k=ℓ θ jk
LetX bethespaceofpossiblequeries(e.g.,userprompts).For
Itsnegativelog-likelihoodis:
eachqueryx∈X,wecollectoneormorecandidateresponses
{ q y u j e } r m j y = x x 1 . w T h y e p re ica m ll x y, is th t e h s e e n r u es m p b o e n r se o s f a c r a e nd g i e d n a e t r e at r e e d sp b o y ns a es la fo n r - L PL (θ) = − X X m log Pm exp e (cid:0) x R p θ (cid:0) ( R x, ( y x jℓ , ) y (cid:1) )(cid:1) ! .
guagemodelorpolicyunderdifferentsamplingorprompting (x,rank)∈Dℓ=1 k=ℓ θ jk
conditions. Human annotators provide preference judgments
Inpractice,oneminimizesthesum(oraverage)ofthechosen
fortheseresponses.Thesecantakevariousforms:
ranking-basedlossoverallpreferencedata:
• Pairwise preference: For two responses y j and y k to
p th r e efe s r a r m ed e t q o u y er k y . x, an annotator indicates whether y j is L(θ) = |D 1 | X L ranking (cid:16) θ;x,{y j },prefs (cid:17) ,
•
Rankings: A partial or total ordering of the candidate (x,{yj},prefs)∈D
responses,e.g.y j1 ≻y j2 ≻···≻y jmx . whereL
ranking
couldbeeitherL
BT
orL
PL
.Whilethereward
We denote such human preference data by {r j } for each model R θ (x,y) provides a scalar reward signal reflecting
response or pair, where r j might be a label, a rank, or an human preferences, this connects to common RL concepts,
index indicating preference level. The overall dataset D then especiallytheadvantagefunction.
consistsofN annotatedexamples:
n oN
D = (xi,{yi}mi ,{preferencesi}) . Reward modeling uses ranking-based losses
j j=1
i=1 to learn a function from human preferences for
In practice, a large number of queries x are sampled from policyoptimization.
real or simulated user requests. Candidate responses {y }mx
j j=1
aregeneratedbyeithersamplingfromabaselanguagemodel
RewardmodelingTypes.Rewardscanbecategorizedinto
or using beam search or other decoding strategies. Human
explicitandimplicitapproaches.
annotators then provide pairwise or ranking feedback on
whichresponsesarebetter(orworse)accordingtopredefined
criteria (e.g., quality, correctness, helpfulness, etc). We train 3.1.1 Explicit Reward Modeling
a parametric model Reward Model (R (x,y)), referred to as Explicit reward modeling defines reward functions directly
θ
therewardmodel,tomapeach(query,response)pair(x,y)to based on predefined rules, heuristics, or human annotations.
a scalar score. The goal is for R to reflect the alignment or This reward structure involves direct, numeric signals from
θ
preferencelevel,suchthat: humans or from specialized AI modules trained to approx-
imate human judgments (e.g., ranking or pairwise compari-
R :X ×Y → R.
θ son). This method can produce precise reward estimates but
HereY isthespaceofallpossibleresponses. may be time-consuming or costly at scale. Illustrative use
To train R , we use the human preference labels in D to cases include ‘red-teaming’ exercises where experts rate the
θ
defineasuitableranking-based loss,asexplainedbelow. severity of toxic outputs, or domain-specialist tasks in which
I.Bradley–TerryModel(Pairwise).Forpairwisepref- correctnessmustbevalidatedbyasubjectmatterexpert.
erences,Bradley-Terrymodel[164]isoftenused.Supposethe
datasetindicatesthat,foragivenqueryx,humanannotators
1.https://hturner.github.io/PlackettLuce/
7
Tree of Reward Model Proximal Regularization
SFT
Thoughts n Training PO KL-Divergence
u
m
a n
ot
ati o
Hn
n
a Preference Reference Reward Direct
CoT Prompting
pairs: (x, y⁺, y⁻) policy SFT Δℒ(x, y⁺, y⁻) Optimization
Reasoning and Expert policy Adversarial
SFT REINFORCE PO
Acting demonstrations Reward Signal
LLM Post Multiple paths Advantage PPO+KL KL constraint
Self-feedback
training using policy Estimation Regularization Regularization
Episodic Offline Terminal Value Function V-guided loss
Memory Agent trajectories rewards {0,1} Training PO+TTS
Policy Long CoT Rejection RL helpfulness
Self- Relative PO
examples+SFT Sampling & SFT alignment PO
consistency
gninosaer
emit
ecnerefnI
3.2.3§
6.2.3§
4.2.3§
5.2.3§
7.2.3§
8.2.3§
FHLR
OPD
FIALR
OPRT
OERO
OPRG
Fig. 2: Overview of Large Language Models (LLMs) reasoning methods, showcasing pathways for enhancing reasoning
capabilities through approaches like Chain-of-Thought (CoT) prompting, self-feedback, and episodic memory. The diagram
highlights multiple reinforcement learning-based optimization techniques, including GRPO, RLHF, DPO, and RLAIF, for fine-
tuningreasoningmodelswithrewardmechanismsandpreference-basedlearning.
3.1.2 Implicit Reward Modeling debugging, in which the path to the answer is as significant
Implicit reward modeling infers rewards indirectly from ob- asthefinalstatement.Insuchproblems,therewardassigned
served behaviors, interactions, or preference signals, often in individual steps encourages transparency and robust step-
leveraging machine learning techniques to uncover latent re- by-stepreasoning.However,itrequiresamorecomplexanno-
ward structures. It derives its signals from user interaction tationprocess,e.g.,requires“gold”reasoningstepsorpartial
metrics such as upvotes, acceptance rates, click-through pat- creditscoring.Processrewardscanbecombinedwithoutcome
terns, or session engagement times. While it can accumulate rewardsforastrongmulti-phasetrainingsignal.
vast datasets with minimal overhead, this approach risks
fostering behaviors that exploit engagement heuristics at the
expenseofcontentqualityorveracity. Policy Reward Modeling(PRM)withlast-stepag-
Reward Function.Definingarewardfunctionfortextgen- gregation outperforms Outcome Reward Modeling
eration tasks is an ill-posed problem [166, 167]. The existing
(ORM)byleveragingfinal-stepevaluationstoopti-
mizepolicyupdatesmoreeffectively.
RL methods in LLMs either focus on the generation process
outcome(OutcomeRewardModeling)orthe(ProcessReward
Modeling), to shape LLM behaviors. We explain these two
rewardmodelingparadigmsbelow. 3.1.5 Iterative RL with Adaptive Reward Models
AdaptiveRewardModelsisatrainingmethodologydesigned
3.1.3 Outcome Reward Modeling to continuously improve the performance of LLMs by iter-
Measures the end result (e.g., whether the final answer is atively refining the reward models and the policy model.
factually correct or solves the user’s query). This model Thisapproachaddressesthechallengesofrewardhackingand
is straightforward to implement but may offer limited in- reward model drift, which can occur when the reward model
sight into how the conclusion was reached. It is prevalent becomes misaligned with the desired objectives during large-
in short-response tasks, where the user’s primary concern is scale RL training. The RL process is divided into multiple
the correctness or succinctness of the final statement. For iterations, where the model is trained in cycles. After each
long-responsetasks,outcomebasedrewardcanleadtocredit iteration, the reward model is updated based on the latest
assignment problem, i.e., which specific actions or states lead model behavior and human feedback. The reward model is
toaparticularrewardoutcome. not static but evolves over time to better align with human
preferences and task requirements. This adaptation ensures
3.1.4 Process Reward Modeling that the reward signals remain accurate and relevant as the
Assigns feedback at intermediate reasoning steps, incentiviz- modelimproves.Repeattheiterativeprocessuntilthemodel’s
ing coherent, logically consistent, and well-structured chains performance plateaus or meets the desired benchmarks. The
of thought. This approach is particularly valuable for tasks rewardmodelandpolicymodelco-evolve,witheachiteration
involving mathematical derivations, legal arguments, or code bringingthemclosertooptimalalignment.
8
3.2 Policy Optimization preferred responses (according to human labels) relative to
Once we have a trained reward model R (x,y) that captures dispreferred ones.Thekeyideaistolookattheoddsratio:
θ
humanpreferences,wecanintegrateitintoaRLframeworkto π (y |x)
optimize apolicyπ ϕ .Inessence,wereplace(oraugment)the π ϕ ϕ (y k j |x) ,
environment’s native reward signal with R (x,y) so that the
agentfocusesonproducingresponsesythat
θ
humanspreferfor
wherey
j
isthepreferred responseandy
k
istheless-preferred
agivenqueryx.
responseforagivenqueryx.
IntypicalRLnotation:
PairwisePreferenceProbability.Inmanydirectpref-
erenceapproaches(e.g.,Bradley–Terrystyle),onewrites
• Each state s here can be interpreted as the partial dia-
logueorpartialgenerationprocessforthenexttoken(in P (cid:0) y ≻y | x (cid:1) = σ (cid:16) ln π ϕ (y j |x)(cid:17) = 1 ,
languagemodeling). ϕ j k π ϕ (y k |x) 1+exp (cid:16) lnπϕ(yk|x) (cid:17)
•
Eachactionaisthenexttoken(ornextchunkoftext)to πϕ(yj|x)
begenerated. whereσ(·)isthelogistic(sigmoid)function.Intuitively,ifthe
• Thepolicy π ϕ (a|s)isaconditionaldistributionoverthe policyπ ϕ assignshigherprobabilitytoy j thantoy k ,theodds
nexttoken,parameterizedbyϕ. πϕ(yj|x) exceed 1, making y more likely to be the preferred
Weseektofindϕthatmaximizes theexpectedrewardunder o π uϕt (y cko | m x) eunderthemodel. j
R .Concretely,letxbeauserquery,andlety ∼π (·|x)be InORPO,onetypicallydefinesanegativelog-likelihood loss
θ ϕ
thegeneratedresponse.Weaimtosolve: forallpairs{(x,y ≻y )}inthedataset:
j k
max E h E (cid:2) R (x,y)(cid:3)i . L (ϕ) = − X log (cid:16) P (cid:0) y ≻y | x (cid:1)(cid:17) .
ϕ
x∼X y∼πϕ(·|x) θ ORPO ϕ j k
(x,yj≻yk)∈D
Thismeansthatonaverage,overuserqueriesxandresponses
Substitutingthelogisticformgives:
ydrawnfromthepolicyπ ,wewanttherewardmodel’sscore
ϕ
R θ (x,y)tobeashighaspossible. L (ϕ) = − X log (cid:16) π ϕ (y j |x) (cid:17) ,
Policy Gradient and Advantage.Themodernalgorithms ORPO π (y |x) + π (y |x)
ϕ j ϕ k
(e.g.,PPO[73],GRPO[59],TRPO[160])relyonpolicygradients. (x,yj≻yk)∈D
Figure 5 presents a structured comparison of the these main whichcanalsobeinterpretedasmaximizingthelogoddsratio
RL frameworks. Each framework builds upon different princi- forthecorrect(preferred)labelineachpairwisecomparison.
plesforpolicylearning,referencemodeling,andrewardcom- Interpretation via Odds Ratios. By treating each
h p o u w tat m io u n c . h R b ec e a tt ll e t r h a a n tt a h c e t a io d n va a nt i a s g t e h f a u n nc t t h io e n b A as ( e s l , i a n ) e q e u x a p n e t c ifi te e d s O p R re P f O er p en u c sh e e l s ab t e h l e ( p y j ol ≻ icy y k t ) o a i s nc a r c e o a n se st i r t a s in p t ro o b n a t b h il e it o y dd m s a π π sϕs ϕ ( ( o y y k j n | | x x y ) ) j ,
return V(s). At a high level, we update the policy π ϕ in the whiledecreasingitony k .Whenviewedinlogarithmicspace:
d ad ir v e a c n ti t o a n ge th a a n t d in d c e r c e r a e s a e s s es π ϕ i ( t a fo | r s) ne fo g r at a iv c e ti - o a n d s va a nt w a i g t e h a p c o t s i i o t n iv s e . ln (cid:16) π π ϕ ϕ ( ( y y k j| | x x ) ) (cid:17) ,
Formally,theadvantageA attimetcanbewrittenas:
t ahighervaluecorrespondstoagreaterlikelihoodofselecting
A t =Q(s t ,a t ) − V(s t ), y j over y k . Hence, minimizing L ORPO (ϕ) aligns π ϕ with the
human-labeledpreferences.
where Q(s ,a ) is the expected future return (sum of future
t t
rewards,includingR )startingfroms whentakingactiona .
θ t t . Odds Ratio Preference Optimization (ORPO)
WhenusingtherewardmodelR :
θ is potentially less flexible for combining multi-
1) We interpret R θ (x,y) as the immediate or terminal re- plerewardsignals.
wardforthegeneratedresponsey.
2) The policy’s future returns thus factor in how likely
subsequenttokensaretobepositivelyscoredbyR . 3.2.2 Proximal Policy Optimization (PPO) in LLMs
θ
3) The advantage function still captures how much better A popular method for policy optimization is PPO [73], a
a particular generation step is compared to the baseline strategy adapted to align LLMs with human feedback. Given
performanceV(s ). a policy π parameterized by θ and a reward function R,
t θ
PPO updates the policy by optimizing a clipped objective
thatbalancesexplorationandstability.Specifically,ifr (θ)=
Therewardmodellearnsrelativepreferences t
πθ(at|st) denotes the probability ratio for an action a in
ratherthanabsolutescores.Thisavoidstheneed s π tθaretfe (a s t|s ,tt ) heclippedPPOobjectiveis: t
forcalibratedhumanratingsandfocusesonpair- t
wisecomparisons. LPPO(θ)=E h min(cid:0) r (θ)A , clip(r (θ),1−ϵ,1+ϵ)A (cid:1)i ,
t t t t t
whereA isanestimatoroftheadvantagefunctionandϵisa
t
3.2.1 Odds Ratio Preference Optimization (ORPO) hyperparameter controlling the allowable deviation from the
The simplest method is ORPO [168] which directly optimizing previouspolicy.A iscomputedusingGeneralizedAdvantage
t
a policy from pairwise human preferences. Instead of first Estimation (GAE) [169] based on rewards and a learned value
learningaseparaterewardmodelandthenrunningstandard function. The clipping objective of PPO restricts how dras-
RL, ORPO updates the policy to increase the likelihood of tically the updated policy distribution can diverge from the
9
original policy. This moderation averts catastrophic shifts in policy updates while ensuring they remain within a con-
languagegenerationandpreservestrainingstability. strainedtrustregion,measuredbyKLdivergence.
PolicyOptimizationwithKLPenalty.DuringRLfine- InsteadofusingaclippedobjectivelikePPO,TRPOenforces
tuningwithPPO,thepolicyπisoptimizedtomaximizereward a hard constraint on policy updates by solving the following
whilestayingclosetothebasemodelρ.Themodifiedreward optimizationproblem:
functionincludesaKLdivergencepenalty:
(cid:20) π (a |s ) (cid:21)
J(π)=E (x,y)∼D (cid:2) r(x,y)−βKL(cid:0) π(·|x)∥ρ(·|x)(cid:1)(cid:3) , m θ ax E t π θo θ ld (a t t | t s t ) A t
whereβcontrolsthepenaltystrength.TheKLtermKL(π∥ρ)
subjecttotheconstraint:
prevents over-optimization to the proxy reward r(x,y) (i.e.,
rewardhacking).
E [D (π (·|s )∥π (·|s ))]≤δ.
t KL θold t θ t
The KL penalty is a regularization, which whereδ isahyperparameterthatcontrolshowmuchthenew
ensure policy retains the base model’s linguistic policycandivergefromtheoldone.
coherenceandavoidsdegenerateoutputs. UnlikePPO,whichapproximatesthisconstraintusingclip-
ping,TRPOdirectlysolvesaconstrainedoptimizationproblem,
ensuring each update does not move too far in policy space.
3.2.3 Reinforcement Learning from Human Feedback (RLHF)
However,solvingthisconstrainedproblemrequirescomputa-
RLHF [58] refines LLMs through direct human preference sig- tionally expensive second-order optimization techniques like
nals, making them more aligned with human expectations.
conjugate gradient methods, making TRPO less efficient for
Theprocessinvolvesthreemainsteps.First,SFTisperformed
large-scale models like LLMs. In practice, PPO is preferred
on a pretrained model using high-quality labeled data to
over TRPO due to its simplicity, ease of implementation, and
establish strong linguistic and factual capabilities. Second, a
comparableperformanceinlarge-scaleapplicationslikeRLHF.
rewardfunctionRistrainedusinghuman-annotatedrankings
However, TRPO remains an important theoretical foundation
ofgeneratedresponses,allowingittopredictpreferencesand
forstablepolicyoptimizationindeepreinforcementlearning.
provide a scalar reward signal. Third, PPO is employed in the
RLHF[58]pipelinebyusinghuman-providedpreferencescores
(or rankings) to shape R and thereby guide the policy up- 3.2.6 Direct Preference Optimization (DPO)
dates.Thisensuresthatthemodelprioritizesoutputsaligned DPO [162] is a recently proposed method for training LLMs
with human-preferred behavior. The robust performance un- from human preference data without resorting to the tradi-
der conditions of noisy or partial reward signals makes PPO tional RL loop (as in RLHF with PPO). Instead of learning a
well-suitedfortextgenerationtasks,wherelargeactionspaces separate reward function and then running policy-gradient
andnuancedrewarddefinitionsarecommon. updates,DPOdirectlyintegrateshumanpreferencesignalsinto
the model’s training objective. So instead of the above PPO
3.2.4 Reinforcement Learning from AI Feedback (RLAIF) objective, DPO instead constructs an objective that directly
RLAIF [95] is an alternative to RLHF that replaces human pushes up the probability of a chosen (preferred) response
annotation with AI-generated feedback. Instead of relying (y+) while pushing down the probability of a less-preferred
on human-labeled preferences, RLAIF employs a secondary, response (y−), all within a single log-likelihood framework.
highly capable language model to generate preference labels, Rather than bounding policy changes with clip, the DPO loss
which are then used to train a reward model. This reward uses the difference between log probabilities of ‘winning’ vs.
model guides reinforcement learning-based fine-tuning of the ‘losing’responses.Thisexplicitlyencodestheuser’spreference
target model. RLAIF reduces the cost and time required for intheupdatedparameters.
data collection by eliminating the need for human annota-
Here, π is the learnable policy, π is a reference policy
tors.Itenableslarge-scalemodelalignmentwithoutrequiring θ ref
(often the SFT-trained model), σ(·) is the sigmoid function,
extensive human intervention while maintaining high perfor-
β is a scaling parameter, and D is a dataset of triplets
mance and alignment. Empirical studies indicate that RLAIF (x,y+,y−)wherey+ isthepreferr tr e a d in outputovery−.
[95,170]isascalableandefficientalternativetoRLHF,making
it a promising direction for reinforcement learning-driven h (cid:16) π (y+ |x)
languagemodeloptimization. LDPO(θ)=E ((x,y+),y−)∼Dtrain σ βlog π θ (y+ |x)
ref
The clipping mechanism constrains policy π (y− |x) (cid:17)i
−βlog θ .
updates to remain within a safe trust region, π (y− |x)
ref
whichiscrucialwhendealingwithcomplex,high-
dimensionalactionspaces. The key insight is that an LLM can be treated as a “hidden
rewardmodel”:wecanreparameterizepreferencedatasothat
themodel’sownlogprobabilitiesreflecthowpreferableonere-
3.2.5 Trust Region Policy Optimization (TRPO) sponseisoveranother.Bydirectlyadjustingthelog-likelihood
TRPO[160]isanotherwidelyusedpolicyoptimizationmethod, of more-preferred responses relative to less-preferred ones,
preceding PPO and sharing its fundamental goal: improving DPO sidesteps many complexities of RL-based methods (e.g.,
stability in reinforcement learning updates. TRPO optimizes advantagefunctionsorexplicitclipping).
10
fine-grained credit assignment. The core objective minimizes
PPO
back-prop theinconsistencyinthesoftBellmanequation:
Reference
r
q Policy o Reward GAE A V (s )−V (s )=r(s ,a )−βlog π θ (a t |s t ) ,
ϕ t ϕ t+1 t t π (a |s )
Value v ref t t
update wheres =f(s ,a )isthenextstate,risthesparsereward,
t+1 t t
andβ controlsKLregularization.Thepolicyandvaluelosses
GRPO KL are:
Reference
q Policy Reward Com G p r u ou ta p tion L V (ϕ)= T 1 T X −1 V ϕ (s t )−R t +β X log π π θ ( ( a a i | | s s i ) )
!2
,
ref i i
t=0 i≥t
KL
DPO update L π (θ)= T 1 T X −1(cid:18) V ϕ (s t )−R t +βlog π π θ ( ( a a t | | s s t ) ) (cid:19)2 +αL reg ,
ref t t
t=0
DPO where L penalizes deviations from π , and α balances
q Policy r reg ref
objective regularization.
preferences
Reference Preference
Model Data OREO’s explicit value function enables test-
timebeamsearch(e.g.,selectinghigh-valuerea-
Fig. 3: Comparison of PPO [73], GRPO [59], and DPO [162]. soningsteps)anditerativetraining,wherefailed
We highlight policy models, reference models, rewards and trajectoriesrefinethepolicy.Thiscontrastswith
optimizationflowswithcorrespondinglossfunctions. DPO implicit value function, which lacks stepwise
creditassignment.
TheadvantagefunctionA ϕ =V ϕ (s t+1 )−V ϕ (s t ) .OREO’scomputationalcostscaleswithtrajec-
quantifies per-step contributions, critical for
torylengthandvalue-modeltraining.Whileef-
identifyingkeyreasoningerrors.Thisgranularity
fective for math/agent tasks, its generalization
is lost in DPO, which treats entire trajectories
tobroader domains(e.g.,coding) requiresvali-
uniformly.
dation. Iterative training also demands careful
data curation to avoid overfitting to failure
Perplexity Filtering for Out-of-Distribution Data. To modes.
ensure DPO training data is on-distribution (aligned with ρ),
responses are filtered using perplexity. The perplexity of a
responsey=(y ,y ,...,y )isdefinedas: 3.2.8 Group Relative Policy Optimization (GRPO)
1 2 T
GRPO [59] simplifies the PPO framework by eliminating the
!
1 X T need for a separate value function. Instead, GRPO estimates
PP(y)=exp − T logP ρ (y i |y <i ) , the baseline from the average reward of multiple sampled
i=1 outputs for the same question. The primary contribution in
where y is the i-th token. Only responses with perplexity GRPO is that it removes the need for a separate value model
i (criticmodel)andinsteadestimatesthebaselinerewardfrom
below a threshold (e.g., the 95th percentile of ρ-generated
responses)areretained. a group of sampled LLM outputs. This significantly reduces
memory usage and stabilizes policy learning. The approach
also aligns well with how reward models are trained, i.e.,
The advantage function remains a core con- by comparing different LLM-generated outputs rather than
cept to determine which actions (token choices) predictinganabsolutevalue.
arebetterthanthebaselineateachstep. For each question q, GRPO samples a group of outputs
{o ,o ,...,o } from the old policy πold. A reward model
1 2 G θ
is used to score each output in the group, yielding rewards
3.2.7 Offline Reasoning Optimization (OREO) {r 1 ,r 2 ,...,r G }. The rewards are normalized by subtracting
thegroupaverageanddividingbythestandarddeviation:
OREO [171] is an offline reinforcement learning method de-
signed to enhance LLMs’ multi-step reasoning by optimizing
r¯ =
r
i
−mean(r)
.
the soft Bellman equation [109]. Unlike DPO, which relies i std(r)
on paired preference data, OREO uses sparse rewards based
on final outcomes (e.g., correctness of reasoning chains) and The advantage A ˆ for each token in the output is set as the
i,t
jointly trains a policy model π and a value function V for normalizedrewardr¯.
θ ϕ i
11
GRPOfirstsamplesaquestionq ∼ P(Q)andthensamples Inthisformulation,eachresponsey isjointlyevaluatedinthe
i
G outputs {o }G from πold(O | q). Define the per-output context of all other responses, ensuring that comparisons are
i i=1 θ
objectiveas notisolatedpairwiseeventsbutratherpartofabroaderrank-
ing framework that helps capture more nuanced preferences
J(o ,θ,q)= 1 X |oi| min n r A ˆ , andreducespotentialbiases.
i |o | ratio,i,t i,t
i
t=1
clip(cid:0) r ,1−ϵ,1+ϵ (cid:1) A ˆ o 3.3 Pure RL Based LLM Refinement
ratio,i,t i,t The work from Guo et al. (2025) [40] introduces two main
!
h i models:DeepSeek-R1-ZeroandDeepSeek-R1.
−βD π ∥π .
KL θ ref • DeepSeek-R1-Zero operates with a purely Reinforce-
mentLearningapproach,excludinganySFT.
Then,theGRPOobjectivebecomes
• DeepSeek-R1 incorporates cold-start data and applies
" 1 X G # amulti-stagetrainingpipeline.
J GRPO (θ)=E q∼P(Q) G J(o i ,θ,q) , Themethodologyencompassesseveralsteps(SeeFigure2
i=1 inGRPOformainsteps):collectingcold-startdata,perform-
wheretheprobabilityratioisdefinedas ingRLtraining,carryingoutSFT,usingdistillationtotransfer
π (o |q,o ) knowledge to smaller models, and addressing specific chal-
r ratio,i,t ≜ πo θ ld(o i,t |q,o i,<t ) . lenges such as language mixing and readability. This multi-
θ i,t i,<t stage pipeline ensures robustness and alignment with human
where ϵ is a clipping hyperparameter akin to PPO, and β preferences, while distillation enables efficient deployment of
adjuststheKL-divergencepenaltyencouragingthenewpolicy
smallermodelswithoutsignificantperformanceloss.
π not to deviate excessively from a reference policy π ,
θ ref
which is typically the initial supervised fine-tuned (SFT) 3.3.1 Cold-Start RL Phase
model[172,173].GRPOcanbeappliedintwomodes:outcome
The process begins with a cold-start RL phase, where a small
supervisionandprocesssupervision.
amount of curated data is gathered to fine-tune an initial, or
Outcome Supervision: Provides a reward only at the base,model.Followingthispreliminaryfine-tuning,RLiscon-
end of each output. The advantage A ˆ i,t for all tokens in the ducted—oftenviaalgorithmslikeGRPOuntilconvergence.The
outputissetasthenormalizedrewardr¯
i
.
cold-startphaseiscriticalforstabilizingthemodelbeforefull
r −mean(r) RL training, preventing instability that can arise from purely
r¯ i = i std(r) . RL-driven updates. The cold-start data preparation focuses
on capturing human-readable reasoning patterns to prevent
Process Supervision: Provides a reward at the end of
each reasoning step. The advantage A ˆ for each token is instabilityfrompurelyRL-drivenupdates.Thisstepgenerates
i,t CoT-styleexampleswithconsistent< reasoning_process >
calculated as the sum of the normalized rewards from the
and < summary > fields, usually involving thousands of
followingsteps:
carefully curated samples. Structured CoT formats and con-
A ˆ = X r¯ , sistentfieldsensureclarityandrobustnessinthemodel’srea-
i,t i,index(j)
soning outputs, reducing errors and improving interpretabil-
index(j)≥t
ity[8,175,176,177].
whereindex(j)istheendtokenindexofthej-thstep.
Overall,GRPOservesasanefficientalternativetoclassicactor-
critic frameworks in DeepSeekR1 [40] by leveraging group- ProvidingCoTreasoningtracesbeforeRLtrain-
level advantages, thereby reducing training costs without ingestablishesa strongerfoundationforreason-
sacrificing the capacity to distinguish fine-grained differences ing tasks, enhancing both robustness and inter-
amongcandidateresponses. pretabilityofoutputs.
Fine-grained per-step rewards enable the 3.3.2 Rejection Sampling and Fine-tuning
model to effectively identify and reinforce high- ThisconceptisalsousedinWebGPT[81].OnceRLstabilizes,
quality responses, boosting overall performance arejectionsamplingmechanismisemployedtogeneratehigh-
incomplex,multi-stepreasoningtasks. quality responses that are subsequently filtered for correct-
ness, clarity, and other quality metrics. These filtered re-
sponsesarethenblendedwithadditionaldatasetstoproduce
3.2.9 Multi-Sample Comparison Optimization
a new, larger corpus for Supervised Fine-Tuning. Rejection
Instead of relying solely on single-pair comparisons, multi-
sampling ensures that only high-quality outputs are used for
sample comparison optimization [174] approach compares
further training, enhancing the model’s overall performance
multiple responses simultaneously to promote diversity
and mitigate bias. Specifically, given a set of responses and reliability. After RL converges for high-stakes reasoning
tasks, rejection sampling is used to filter a large number of
{y ,y ,...,y }foraqueryx,theprobabilityofobservingthe
1 2 n generated outputs, expanding the training set. These newly
rankingy >y >···>y isdeterminedbytheproduct
1 2 n generated reasoning examples (potentially up to hundreds of
P(y >y >···>y ) = Y eR(x,yi) . thousands in quantity) are mixed with existing SFT data to
1 2 n P eR(x,yj) create a combined dataset of substantial size (often around
i j
12
Efficient Finetuning and Deployment
Distillationdemocratizesadvancedreasoning
Accelerators Co-Optimized Architectures
(Groq, vLLM, Triton, (FlashAttention, BlockSparse capabilities, enabling smaller models to achieve
etc.) IO, DeepSeek v3 etc.)
competitive performance with reduced compu-
Parallel Computing, Data Compression, Data tationaloverhead.
Distributed Training System Data Filtering (TokenMerging,
(LoRA, PEFT, RecapDataComp-18,
DeepSpeed,etc.) etc.)
Scaling law, Data mining
Model compression Model
(Bitsandbite, GPTQ, (Chinchilla, RETRO, C4 4 Supervised Finetuning in LLMs
data, etc.)
etc.)
As shown in Figure 2, finetuning forms a basic component of
Fig.4:ThisVenndiagramillustratestheinterplaybetweenSys- LLM post-training recipes. In this section, we summarize the
tem, Data, and Model for efficient finetuning and deployment.
differenttypesof LLMfine-tuningmechanisms.
Itcoversstrategieslikeaccelerators(Groq,vLLM),adaptation
(LoRA,PEFT),co-optimizedarchitectures(FlashAttention),data
compression (TokenMerging), scaling laws (Chinchilla), and 4.1 Instruction finetuning
modelcompression(GPTQ)toboostperformanceandscalability.
Ininstructionfinetuning,amodelistrainedoncuratedpairs
ofinstruction(prompt)andresponse(completion).Themain
800k samples). Rejection sampling and dataset expansion goal is to guide the LLM to follow a user-provided instruction
accurately and helpfully, regardless of the task domain. This
significantly enhance the model’s coverage of general tasks
usually involves compiling large, diverse instruction-response
whilepreservingitsreasoningproficiency.
datasetscoveringmanytasktypes(e.g.,summarization,QA,
classification, creative writing). Models such as T0 [178],
3.3.3 Reasoning-Oriented RL
FLAN [179], Alpaca [180], Vicuna [181] and Dolly [182]
Thereasoning-orientedRL leveragesGRPO[59],whichsamples
demonstrate how instruction-finetuned LLMs can outperform
a group of outputs from the current policy and computes
base models on zero-shot or few-shot tasks by virtue of their
rewards and advantages for each output. Rewards may be
enhancedinstruction-followingabilities.
computed via rule-based checks, e.g., ensuring correct solu-
tions in math or code tasks, enforcing structured CoT tags,
andpenalizingundesiredlanguagemixing.GRPOgroup-based 4.2 Dialogue (Multi-turn) Finetuning
sampling and reward computation ensure that the model
prioritizeshigh-quality,structuredoutputs,enhancingitsrea-
SomeLLMsundergodialogue-stylefinetuningtobetterhandle
multi-turn conversations. Different from instruction tuning
soningcapabilities.
described above, here the data takes the form of a contin-
uous dialogue (multi-turn conversations) instead of a single
3.3.4 Second RL Stage for Human Alignment
prompt-responsepair.Inthisapproach,trainingdataconsists
A second RL stage further aligns the model with broader of chat transcripts with muliple user queries and system re-
humanpreferences(helpfulness,harmlessness,creativity,etc.) sponses,ensuringthemodellearnstomaintaincontextacross
by introducing additional reward signals and prompt distri- turnsandproducecoherentreplies.ModelslikeLaMDA[183]
butions. The second RL stage ensures the model aligns with and ChatGPT [39] highlight how dialogue-tuned LLMs can
human values, making it more versatile and contextually feel more interactive and context-aware. While dialogue fine-
aware. After re-training the base model on this combined tuningcanoverlapwithinstructionfinetuning(becausemany
dataset, a second round of RL can be conducted to align instructions come in a chat format), specialized conversation
the model more closely with human preferences (e.g., for dataoftenyieldsmorenatural,multi-turnuserexperiences.
helpfulness and harmlessness). This RL stage fine-tunes the
model to better align with human values, ensuring outputs
arenotonlyaccuratebutalsocontextuallyappropriate. 4.3 CoT Reasoning finetuning
Chain-of-Thought (CoT) reasoning finetuning teaches models
3.3.5 Distillation for Smaller Models to produce step-by-step reasoning traces instead of just final
Finally,distillationtechniquesareusedtotransfertherefined answers. By exposing intermediate rationales or thoughts,
capabilities of the main model to smaller architectures, en- CoT finetuning can improve both interpretability and accu-
abling more efficient deployments without sacrificing much racy on complex tasks (e.g., math word problems, multi-
performance. It allows smaller models to inherit advanced hop QA). In practice, CoT finetuning uses supervised rea-
reasoningcapabilities,makingthemcompetitiveonchalleng- soning annotations (often handcrafted by experts) to show
ingbenchmarkswithoutthecomputationalcostsoffull-scale how a solution unfolds. Notable early work includes Chain-
RL training. Finally, distillation plays a pivotal role: the top- of-Thought Prompting [8] and Self-Consistency [184], which
performing model, DeepSeek-R1 [40], serves as a teacher to initially applied the idea to prompting; subsequent efforts
smaller architectures (e.g., Qwen or Llama families, ranging (e.g., Chain-of-Thought Distillation [185]) adapt it to a full
from1.5Bto70Bparameters).Thistransferallowsthesmaller finetuning or student-teacher paradigm. These efforts have
models to inherit advanced reasoning capabilities, making also been extended to the multimodal domain, e.g., LlaVA-
them competitive on challenging benchmarks without incur- CoT [186] and LlamaV-o1 [187] where image, QA and CoT
ringthecomputationalcostsoffull-scaleRLtraining. reasoningstepsareusedinLLMfinetuning.
13
Model Category Source Description
1.Parameter-EfficientFine-Tuning&ModelCompression
LoRA[60] Low-RankAdaptation Link Injectstrainablelow-rankadaptersforefficientfine-tuning.
QLoRA[188] QuantizedAdaptation Link Combines4-bitquantizationwithLoRAtoenablefine-tuningonconsumerGPUs
GPTQ[189] Post-TrainingQuantization Link Optimal4-bitquantizationmethodforGPT-stylemodelswithminimalloss
SparseGPT[190] Pruning Link One-shotpruningthatpreservesmodelqualitywithcompensation.
PEFT(HF)[191] UnifiedFine-Tuning Link LibraryintegratingLoRA,prefixtuning,andotherparameter-efficientmethods
BitsAndBytes[192] Low-PrecisionTraining Link Enables8-bitoptimizersand4-bitquantizationformemory-efficienttraining
AdaLoRA[193] AdaptiveAdaptation Link Dynamicallyallocatesparameterbudgetbetweenlayersduringfine-tuning
P-Tuningv2[194] PromptOptimization Link Learnscontinuouspromptembeddingsthroughdeepprompttuning
2.DataManagement&Preprocessing
HFDatasets[195] DataProcessing Link UnifiedAPIfor30k+datasetswithstreaming,versioning,andpreprocessing
WebDataset[196] DataStreaming Link Efficienttar-basedshardingformatforpetascaledistributedtraining
DVC[197] DataVersioning Link Git-likeversioncontrolfordatasetsandmachinelearningpipelines
ApacheArrow[198] MemoryFormat Link Language-agnosticcolumnarmemoryformatforzero-copydataaccess
Zstandard[199] Compression Link High-speedcompressionalgorithmfortrainingdatastorage/transfer
Cleanlab[200] DataQuality Link Automaticdetectionoflabelerrorsandoutliersintrainingdatasets
3.DistributedTraining&Optimization
DeepSpeed[201] TrainingOptimization Link ZeROparallelism,3Dparallelism,andmemoryoptimizationsforgiantmodels
Megatron-LM[202] ModelParallelism Link NVIDIA’soptimizedframeworkforlargetransformermodeltraining
Colossal-AI[203] HeterogeneousTraining Link Unifiedsystemsupportingmultipleparallelizationstrategies
Horovod[204] DistributedTraining Link MPI-inspiredframeworkformulti-GPU/multi-nodesynchronization
Ray[205] DistributedComputing Link UniversalframeworkfordistributedPythonapplicationsatscale
4.EfficientInference&Deployment
vLLM[206] ServingOptimization Link Pagedattentionimplementationforhigh-throughputLLMserving
TensorRT[207] GPUOptimization Link NVIDIA’sinferenceoptimizerwithkernelfusionandquantizationsupport
Triton[208] ServingFramework Link Production-gradeservingwithconcurrentmodelexecutionsupport
ONNX[209] Cross-Platform Link Unifiedinferenceenginewithhardware-specificoptimizations
OpenVINO[210] IntelOptimization Link RuntimeforIntelCPUs/iGPUswithpruning/quantizationsupport
XNNPACK[211] MobileInference Link Highlyoptimizedfloating-pointkernelsforARMCPUs
Groq[212] AIAccelerator Link Deterministiclow-latencyinferenceviacustomtensorstreamingprocessor
5.IntegratedDevelopmentEcosystems
HFEcosystem[213] FullStack Link Transformers+Datasets+Accelerate+InferenceEndpoints
DeepSpeed[201] Training/Inference Link Microsoft’send-to-endsolutionforbillion-parametermodels
PyTorch[214] UnifiedFramework Link NativeLLMsupportviatorch.compileandscaleddot-productattention
LLMReasoners[215] AdvancedReasoning Link EnhancesLLMreasoningcapabilitiesusingadvancedsearchalgorithms.
TABLE2:ComprehensiveOverviewofMethodsandFrameworksemployedinModernLLMs
4.4 Domain-Specific (Specialized) Finetuning with smaller datasets. This approach can yield lighter, faster
When an LLM needs to excel in a specific domain (e.g., modelsthatretainmuchoftheteacher’sperformance,evenin
biomedicine, finance, or legal), domain-specific finetuning is zero-shotorfew-shottasks[230].
used. Here, a curated corpus of domain-relevant text and la-
beledexamplesisemployedtofinetunetheLLM.Forinstance,
4.6 Preference and Alignment SFT
BioGPT [71] and BiMediX [216] specialize in biomedical
While RLHF is not purely supervised, it starts with a su-
literature, FinBERT [217] for financial texts, ClimatGPT
pervisedpreference oralignment finetuningstage.Thisstage
[218, 219] for climate and sustainability and CodeT5 [220]
uses human-labeled or human-ranked examples to teach the
for code understanding. Supervised finetuning in these do-
model about desirable vs. undesirable outputs (e.g., safe vs.
mainsoftenincludesclassification,retrieval,orQAtaskswith
toxic). By training on these explicit preferences, the model
domain-specificdata,ensuringthemodel’sparametersadapt
becomes more aligned with user values, reducing harmful or
tothespecializedlanguageandconceptsofthefield.Domain-
off-topic completions. Works like InstructGPT [58] illustrate
specificfinetuningisalsoextendedtovision-languagemodels
howsupervisedpreferencedataiscriticalbeforerewardmodel
such as, [221] finetuned on remote sensing imagery, [222] on
trainingandRLupdatesbegin.
medicalimagingmodalities,[223,224,225]onspatiotemporal
videoinputs,and[226]adaptedforchartunderstanding.
4.7 Efficient Finetuning
4.5 Distillation-Based Finetuning Fully finetuning a LLM can be computationally and memory-
Large‘teacher’modelsaresometimesusedtoproducelabeled intensive, particularly as model sizes grow into the tens or
data or rationales, which a smaller ‘student’ model finetunes hundreds of billions of parameters. To address these chal-
on, this is generally called knowledge distillation [227, 228]. lenges,parameter-efficientfinetuning(PEFT)techniquesintro-
In the context of LLMs, CoT Distillation [185] is one example duceasmallsetoftrainableparametersorlearnableprompts
where a powerful teacher LLM generates intermediate rea- while leaving most of the model weights frozen. Approaches
soning steps, and the student LLM is finetuned to reproduce such as LoRA [60], Prefix Tuning [231], and Adapters [232]
both the final answer and the reasoning chain. Step-by-step exemplify this strategy by injecting lightweight modules (or
distillation [229] generates descriptive rationales alongside prompts) in specific layers, thus significantly reducing the
final answers to train smaller models through distillation memoryfootprint.
14
probability paths. By limiting the beam width (N), it man-
Test Time Scaling
ages the exponential search space while aiming to find a
near-optimal sequence. These beams are expanded at each
Scaling Advanced Improved Sequential decoding step to find multiple probable paths. In reasoning
Strategies Sampling Reasoning revision
LLMs, such paths allow us to systematically explore multiple
Beam Confidence Chain-of-Thought Self-Consistency reasoning chains in parallel, focusing on the most promising
Search Decoding
Based Prompting ones. This ensures that high-likelihood reasoning steps are
Sampling
M Tr o e n e t S e e C a a r r c l h o Tree-of-Thoughts Self-Improvement considered,whichcanimprovethechancesoffindingacorrect
Search via Refinements and coherent solution compared to greedy decoding. It has
Against
Best-of-N
Search Verifiers compute-optimal scaling strategy traditionally been used in tasks such as translation, sum-
marization, and code generation, where the goal is a highly
Fig. 5: An overview of Test-time Scaling methods: parallel
probablecorrectsequence[93].
scaling, sequential scaling, and search-based methods. It also
While modern LLMs often favor stochastic sampling (e.g.,
showshowtheyintegrateintoacompute-optimalstrategy.
temperaturesampling)topromotediversityingeneratedtext,
beamsearchisstillavaluabletechniqueforstructuredreason-
ing problems. For example, the Tree-of-Thoughts framework
Figure4illustrateshowthesetechniquesfitintoabroader
[84] allows plugging in different search algorithms to explore
ecosystemthatinvolvessystem-leveloptimizations,dataman-
a tree of possible ‘thoughts’ or reasoning steps; usually a
agement, and evaluation strategies for LLMs. In particular,
beam search (with beam width b) is used to maintain the
PEFT approaches can be combined with quantization and
b most promising states at each reasoning step. Here, beam
pruning methods [190, 188] to further minimize memory
searchisusedtosystematicallyexploresolutionstepsfortasks
usage and compute overhead, enabling finetuning on smaller
like mathematical puzzles and planning problems, pruning
GPUsorevenconsumer-gradehardware.Forinstance,QLoRA
less promising reasoning branches and thus improving the
unifies 4-bit quantization with low-rank adaptation, while
model’s problem-solving accuracy. Beam search remains a
BitsAndBytesprovides8-bitoptimizerstomakeLLMtraining
strong baseline for test-time reasoning when one wants the
morepracticalinconstrainedenvironments(Table2).
model to output the single most likely reasoning path or
Moreover, these PEFT methods still require supervised
answerunderthemodel’slearneddistribution.
data to guide the adaptation process, but the reduction in
the number of trainable parameters makes it more feasible
to use in-domain or task-specific datasets. This is especially 5.2 Best-of-N Search (Rejection Sampling)
valuable for specialized domains (e.g., medical or software Best-of-N (BoN) [235] search generates N candidate outputs
development), where data might be limited or expensive to (usually via sampling) and then picks the best one according
annotate. As shown in Table 2, PEFT (HF) integrates several toachosencriterion(e.g.,arewardmodelorthemodel’sown
of these approaches (LoRA, prefix tuning, and more) into a likelihood)[236,237,238].Conceptually,thisisanapplication
single library, streamlining deployment in both research and ofrejectionsampling:onedrawsmultiplesamplesandrejects
productionsettings. all but the top-rated result. Unlike Beam Search [233, 234],
which incrementally expands and prunes partial hypotheses,
BoNsimplysamplesfullsolutionsindependently,allowingfor
Combining efficient tuning designs like LoRA greater diversity but at a higher computational cost. Beam
and QLoRA with system and data optimizations Search systematically aims for the most probable sequence,
(Figure4)enablescost-effectiveLLMadaptation
while BoN may capture high-quality but lower-probability
for tasks like domain-specific text generation,
solutionsthroughbrute-forcesampling.
withoutexpensivefullfine-tuning.
Beam search (effective for harder questions)
5 Test-time Scaling Methods outperforms best-of-N sampling at low compute
budgets, while best-of-N scales better for easier
WhileRLfine-tunesthemodel’spolicy,test-timescaling(TTS)
tasks.
enhances reasoning during inference typically without model
updates. Figure 5 presents a taxonomy of TTS methods,
DuringLLMinference,BoNisusedtoenhancecorrectnessor
categorizingthembasedontheirunderlyingtechniques.
alignmentwithoutretrainingthemodel.Bysamplingmultiple
answers and selecting the top candidate (e.g., via a reward
5.1 Beam Search model or a checker), BoN effectively boosts accuracy on tasks
Beam search was first introduced in the context of speech like QA or code generation. BoN is easy to understand and
recognition[233].Itgainedprominenceasadecodingstrategy implement and is almost hyper-parameter-free, with N being
forsequencemodelsandwaslateradoptedinneuralmachine the only parameter that can be adjusted at inference. In
translation and speech systems [234]. With the popularity of reinforcement learning contexts, BoN sampling can serve as
LLMs,thisalgorithmhasbeenusedforapproximatesearchin a baseline exploration mechanism i.e., to generate many roll-
manytextgenerationtasks. outs, pick the best outcome according to the learned reward,
The concept of Beam search is similar to pruned breadth- and proceed, although at increased computational overhead.
first search, where top N highest-probability partial se- OpenAI’s WebGPT used BoN to pick the best response via
quences (the ‘beam’) are kept at each step, discarding lower- a reward model, yielding strong QA performance [81]. BoN
15
isalsousedasasimplealignmentmethodthatishighlycom- Wei et al. [8] demonstrated CoT’s effectiveness on arith-
petitivewithotherpost-trainingtechniquese.g.,RLHF[58]and meticandlogictasks,showinglargegainsoverdirectprompt-
DPO[78].StudieshaveshownBoNcanapproachormatchRLHF ing. Kojima et al. [242] introduced Zero-Shot CoT, revealing
results when guided by a sufficiently robust reward model that even adding a simple phrase like “Let’s think step
[82,239].Alternativessuchasspeculativerejection[240]build by step” can trigger coherent reasoning in sufficiently large
on this idea and utilize a better reward model to improve models.Subsequentworks(e.g.,Wangetal.,2022[184])com-
efficiency. The studies also highlight issues of reward hacking bined CoT with sampling-based strategies (Self-Consistency)
if the (proxy) reward function used for BoN is imperfect [241] forevenhigheraccuracy.AsdescribedinSec.5.4,CoTformat
orinstabilityissuesiftheNparametergetsverylarge. data have also been used for SFT and are shown to help
reshapethemodelresponsestobemorestep-by-step.
Choice of either process reward models with
beam search vs best-of-N depends on the diffi- Fine-tuningmodelstoreviseanswerssequen-
cultyandcomputebudget. tiallyallowsthemtobuildonpreviousattempts,
improving accuracy over time. This approach is
particularly effective for easier questions, while
parallelsampling(exploration)provesmoreben-
5.3 Compute-Optimal Scaling
eficialforharderones.
The Compute-Optimal Scaling Strategy (COS) [83] is a dy-
namic method designed to allocate computational resources
efficientlyduringinferenceinLLMs,optimizingaccuracywith- 5.5 Self-Consistency Decoding
outunnecessaryexpense.Insteadofapplyingauniformsam- Self-Consistency is a decoding strategy introduced by Wang
pling strategy across all inputs, this approach categorizes et al. [243]. It was proposed as an alternative to simple
prompts into five difficulty levels—ranging from easy to greedy decoding for chain-of-thought prompts. It built upon
hard—eitherbyleveragingoracledifficulty(ground-truthsuc- the idea of sampling multiple distinct reasoning paths for a
cess rates) or model-predicted difficulty (e.g., verifier scores question and was the first to show that marginalizing over
from Preference Ranking Models). Once categorized, the those paths can significantly improve accuracy on arithmetic
strategy adapts compute allocation: easier prompts undergo and reasoning problems. In other words, it allows the model
sequential refinement, where the model iteratively refines to think in many ways and then trust the consensus, which
its output to improve correctness, while harder prompts improvescorrectnessinmanyreasoningscenarios.
trigger parallel sampling or beam search, which explores The self-consistency method works by sampling a diverse
multiple response variations to increase the likelihood of set of reasoning chains from the model (via prompt engi-
finding a correct solution. This dual approach balances ex- neering to encourage different CoTs, and using temperature
ploration (for challenging inputs) and refinement (for near- sampling) and then letting the model output a final answer
correct responses), ensuring optimal performance per unit foreachchain.Insteadoftrustingasinglechain,themethod
of computational effort. Remarkably, this method achieves selectstheanswerthatismostconsistentacrossthesemultiple
four times lower compute usage than traditional best-of-N reasoningpaths,effectivelyamajorityvoteorhighestprobabil-
samplingwhilemaintainingequivalentperformance.Thekey ity answer after marginalizing out the latent reasoning. The
insightisthatbymatchingcomputationalstrategytoproblem intuition is that if a complex problem has a unique correct
difficulty, it avoids wasted resources on trivial cases while answer, different valid reasoning paths should converge to
ensuring sufficient sampling diversity for complex tasks. In that same answer. By pooling the outcomes of many chains,
essence,itfunctionsasa“smartthermostat”forLLMinference, the model can “decide” which answer is most supported. In
dynamically adjusting computational effort in response to application,onemightsample,e.g.,20CoTsforamathprob-
inputcomplexity,leadingtoamoreefficientandcost-effective lem and see what final answer appears most frequently; that
deploymentoflarge-scalelanguagemodels. answer is then taken as the model’s output. This approach
turns the one-shot CoT process into an ensemble where the
model cross-verifies its answers. It is especially useful for
COS achieves 4× efficiency gains over best- arithmeticandcommonsensereasoningtaskswherereasoning
of-N baselines by optimally balancing sequen- diversityhelps.
tial/parallel compute. Beam search + revisions
outperform larger models on easy/intermediate
questions. Smaller models with test-time compute can
outperform much larger models in certain sce-
narios.
5.4 Chain-of-thought prompting
Self-consistency is often combined with other methods:
CoTpromptinginducesLLMstoproduceintermediatereason- e.g., sampling multiple chains and then applying a verifier
ing steps rather than jumping directly to the final answer. to the most common answer. Its strength lies in requiring no
By breaking down problems into logical sub-steps, CoT taps new training, only extra sampling, making it a popular test-
intoamodel’slatentabilitytoperformmulti-stepinferences, time scaling strategy to obtain more reliable answers from
significantly improving performance on tasks like math word LLMs. It has also inspired other variants, e.g., Universal Self-
problems,logicalpuzzles,andmulti-hopQA. Consistency[244]extendtheoriginalidea(whichworkedonly
16
Direct CoT Self-consistancy Multiple CoT ToT GoT
Input Input Input Input Input Input
Output
Not graded
Positive graded
... ... ... ... ... ... ...
Negative graded
Positive graded
Aggregation
Voting
Back tracking
Output Output Output Output Output
Self-refining
Fig.6:ThisfigurecomparesreasoningstrategiesinLLMs,evolvingfromDirectPrompting,whichmapsinputtooutputwithout
reasoning, to more structured approaches. Chain-of-Thought (CoT) introduces step-by-step reasoning, while Self-Consistency
(CoT-SC) generates multiple CoT paths and selects the most frequent answer. Multiple CoTs explores diverse reasoning paths
independently.Tree-of-Thoughts(ToT)structuresreasoningasatree,enablingbacktrackingandrefinement,whereasGraph-
of-Thoughts(GoT)generalizesthisbydynamicallyaggregatingandconnectingthoughts.Thelegenddecipherskeymechanisms
likegrading,backtracking,andself-refinement,crucialforoptimizingreasoningefficiency.
with majority vote on single final answer) to more general combined with ToT: differentLLM “agents” generate thoughts
generationtaskssuchassummarizationandopen-endedQA. in parallel and a validator agent prunes incorrect branches,
leadingtoimprovedaccuracyoverthesingle-agentToT[246].
5.6 Tree-of-thoughts
ToTframework[84]generalizesthechain-of-thoughtapproach Inference-time computation for LLMs can out-
by allowing the model to branch out into multiple possible performscalingmodelparameters,especiallyfor
thoughtsequencesinsteadoffollowingasinglelinearchain.It challengingreasoningtaskslikemathproblems.
thusformulatestheproblemoflanguage-modelreasoningasa
treesearch,drawingonclassicAIsearchmethodsinspiredby
humanproblem-solving[245,37].TreeofThoughtstreatsin-
5.7 Graph of Thoughts
termediatereasoningstepsas“nodes”inasearchtreeanduses
the language modelto expand possible next steps (thoughts) The Graph of Thoughts (GoT) [247] framework extends the
from a given state. Rather than sampling one long reasoning ToTbyallowingmoreflexibleandefficientreasoningprocesses
path, the model explores a tree of branching thoughts and throughgraph-basedstructuresratherthanstricthierarchical
can perform lookahead and backtracking. At each step, the trees. Thought representation differs between the two ap-
LLM might generate several candidate next thoughts, and a proaches: in ToT, each step in reasoning is structured as a
heuristic or value function evaluates each partial solution node in a tree with fixed parent-child relationships, whereas
state.Thenasearchalgorithm(e.g.,depth-first,breadth-first, GoT represents thoughts as nodes in a graph, enabling more
beam search) navigates this tree, deciding which branches to adaptabledependenciesandinterconnections.
explore further. This approach allows systematic exploration In terms of thought expansion strategies, ToT follows a
of different reasoning strategies: if one path leads to a dead- traditional approach where multiple thought candidates are
end,themodelcanreturntoanearlierstateandtryadifferent generated at each step, explored using tree-based search
branch (unlike standard CoT which commits to one line of strategies,andprunedbasedonheuristicsbeforeselectingthe
reasoning).Ineffect,ToTisaniterativepromptingprocedure mostoptimalpath.Incontrast,GoTincorporatesgraph-based
where the model generates thoughts, evaluates them, and thoughtexpansion,allowingthoughtstointerconnectdynam-
refinesitsapproach,mimickinghowahumanmightmentally ically. This enables three key transformations: aggregation
mapoutvariouswaystosolveaproblem. (mergingmultiplesolutionsintoaunifiedanswer),refinement
ToTisespeciallyusefulforcomplexproblemslikepuzzles, (iteratively improving thoughts over time), and generation
planning tasks, or games where multiple steps and strategic (producingdiversecandidates).Insteadofnavigatingthrough
explorationareneededandoutperformssimplerCoTmethods a rigid hierarchy, GoT prioritizes thoughts using a volume
bysystematicallysearchingthroughthesolutionspace.Itpro- metric and explores paths optimally, reducing unnecessary
videsaflexibleframework–onecanpluginvariousgeneration computations.
strategies(e.g.samplingvs.prompting)andsearchalgorithms A critical limitation of ToT is its restricted backtrack-
(BFS, DFS, A*, MCTS) depending on the task. Although ing—once a branch is discarded, it is not reconsidered. GoT
morecomputationallyheavy,ToTshowsthatallocatingextra overcomes this by allowing iterative refinement, where previ-
“thinking time” (compute) to explore alternatives can yield ous thoughts can be revisited, modified, and improved upon.
significantly better reasoning and planning performance. It This iterative nature is particularly useful in complex rea-
has spawned follow-up research aiming to improve or utilize soningtaskswhereinitialthoughtsmayrequireadjustments.
it for better reasoning e.g., multi-agent systems have been Moreover, computational efficiency in GoT is significantly
17
improved by reducing redundant calculations through the 2) Process Reward Models (PRM): Evaluate the reason-
mergingofpartialsolutions. ing steps (e.g., logical coherence in a thought chain),
providinggranularfeedbacktopruneinvalidpaths.
Several techniques fall under this paradigm, enhancing
GoT enhances problem-solving efficiency and
verification-based optimization. Best-of-N Sampling involves
adaptability, making it superior to ToT for tasks
generating multiple answers and ranking them via a verifier
requiringcomplexreasoning.
(ORM/PRM),selectingthehighest-scoringone,makingitasim-
ple yet effective approach for improving answer correctness.
5.8 Confidence-based Sampling Beam Search with PRM tracks top-scoring reasoning paths
(beams) and prunes low-quality steps early, similar to Tree
In confidence-based sampling, the language model generates
of Thought approaches, balancing breadth and depth in rea-
multiple candidate solutions or reasoning paths and then
soning path exploration. Monte Carlo Tree Search balances
prioritizes or selects among them based on the model’s own
explorationandexploitationbyexpandingpromisingreason-
confidence in each outcome [248]. This can happen in two
ingbranches,simulatingrollouts,andbackpropagatingscores,
ways:(a)Selection:GenerateNoutputsandpicktheonewith
providing an optimal trade-off between search depth and
the highest log probability (i.e., the model’s most confident
verification confidence. Majority Voting (Self-Consistency)
output). This is essentially best-of-N by probability – the
aggregates answers from multiple samples and selects the
model chooses the answer it thinks is most likely correct.
most frequent one, avoiding explicit verifiers, which works
(b) Guided exploration: When exploring a reasoning tree or
well in settings where consistency across multiple responses
multi-step solution, use the model’s token probabilities to
indicatescorrectness.
decide which branch to expand (higher confidence branches
areexploredfirst).Inotherwords,themodel’sprobabilityes-
timatesactasaheuristicguidingthesearchthroughsolution ORM is suitable for tasks where correctness is
space[249].Comparedtopurerandomsampling,confidence- binary(right/wrong)andcanbeeasilyassessed.
based methods bias the process toward what the model be-
lievesisright,potentiallyreducingwastedexplorationonlow-
likelihood(andoftenincorrect)paths.
PRMisusefulinmulti-stepreasoning,ensuring
Confidence-based strategies have been incorporated at
intermediatestepsfollowslogicalprogression.
inferencetimee.g.,atree-basedsearchforLLMgeneration[248]
assignseachpossiblecompletion(leaf)aconfidencescore.The
algorithm samples leaves in proportion to these confidence
5.10 Self-Improvement via Refinements
scores to decide which paths to extend [272]. Similarly, some
This approach refers to the ability of LLMs to enhance their
reasoning approaches use the model’s estimated likelihood of
outputs through self-evaluation and revision iteratively. This
an answer to decide when to halt or whether to ask a follow-
process enables models to refine their responses dynamically
up question – essentially if the model’s confidence is low,
during inference rather than relying solely on pre-trained
it might trigger further reasoning (a form of self-reflection).
weights. One notable method is Self-Refinement [252],
Confidence-based selection is also used in ensemble settings:
where an LLM generates an initial response, critiques it, and
e.g., an LLM may generate multiple answers and a secondary
then refines the output based on its self-generated feedback.
model evaluates the confidence of each answer being correct,
This iterative process continues until the model achieves a
picking the answer with the highest confidence. This was
satisfactory result. Such techniques have been shown to im-
explored in tasks like medical Q&A, where an LLM gave an
prove performance on various tasks, including mathematical
answer and a confidence score, and only high confidence
reasoningandcodegeneration.Thisprocessfollowsthesekey
answersweretrustedorreturned[250].
steps:a)InitialGeneration:Themodelproducesananswer
or reasoning path. b) Self-Critique: The model reviews its
5.9 Search Against Verifiers ownresponseandidentifieserrors,inconsistencies,orareasfor
This verification approach [251] in LLMs enhances answer improvement.c)Refinement:Themodeladjustsitsresponse
qualitybygeneratingmultiplecandidateresponsesandselect- based on the critique and generates an improved version.
ing the best one using automated verification systems. This d) Iteration: The process repeats until the output meets a
approach shifts focus from increasing pre-training compute predefinedqualitythresholdorstopsimproving.
to optimizing test-time compute, allowing models to “think Another approach is called Self-Polish [253], where the
longer” during inference through structured reasoning steps modelprogressivelyrefinesgivenproblemstomakethemmore
oriterativerefinement.Themethodinvolvestwomainsteps: comprehensible and solvable. By rephrasing or restructuring
Generation: The model (or “proposer” produces multiple problems, the model enhances its understanding and pro-
answers or reasoning paths, often using methods like high- vides more accurate solutions. Self-Polish involves progres-
temperaturesamplingordiversedecoding. sive refinement of problem statements to make them more
Verification:Averifier(e.g.,arewardmodel)evaluatesthese comprehensible and solvable. The model first rephrases or
candidates based on predefined criteria, such as correctness, restructurestheproblemforbetterclarity,thenbreaksdown
coherence, or alignment with desired processes. Verifiers are complex queries into simpler sub-problems and refines am-
categorizedbasedontheirevaluationfocus: biguous inputs to ensure precise understanding. By restruc-
1) OutcomeRewardModels(ORM):Judgeonlythefinal turing problems before solving them, the model improves its
answer(e.g.,correctnessofamathsolution). comprehensionandgeneratesmoreaccuratesolutions.
18
of possible reasoning paths and pick a high-reward answer
Self-improvement methodologies represent a path,outperformingnaivesamplinginascientificQ&Atask.
paradigm shift in LLM optimization, emphasiz- Similarly,MCTShasbeenappliedtocodegenerationwithLLMs
ing active reasoning and internal feedback over [258]–thealgorithmexploresdifferentcodepaths(usingthe
static pre-training. By iterating on their own modeltoproposecodecompletionsandetestthem)tofinda
responses, models achieve greater consistency correctsolution.AnotherlineofworkensemblesmultipleLLMs
andaccuracyacrossawiderangeofapplications. withMCTS,treatingeachmodel’soutputasabranchandusing
arewardmodeltosimulateoutcomes[259].Earlyresultsshow
thatMCTS-basedreasoningcansolveproblemsthatsingle-pass
5.11 Monte Carlo Tree Search or greedy methods often miss, although with more compute
MCTS [254] is based on the application of Monte Carlo sim- [74]. The downside is that MCTS can be significantly slower
ulations to game-tree search. It rose to prominence with than straightforward sampling or beam search, which recent
successes in games, notably, it powered AlphaGo [255] in research is addressing by improving efficiency (e.g., by state
2016 by searching possible moves guided by policy and value merging[87]).Ingeneral,MCTSbringsthestrengthofplanning
networks.This,aswellastheapplicationtootherboardand algorithmstoLLMinferenceandenablesanLLMto’lookahead’
video games, demonstrates the power of MCTS for sequential throughsimulatedrolloutsandmakemoreinformedreasoning
decision-makingunderuncertainty. choices,muchlikeithasdoneforAIingameplay.
MCTSisastochasticsearchalgorithmthatbuildsadecision
treebyperformingmanyrandomsimulations.Itisbestknown
Test-timecomputeisnota1-to-1replacement
forfindinggoodmovesingamestates,butitcanbeappliedto
for pretraining but, offers a viable alternative in
anyproblemwherewecansimulateoutcomes.Thealgorithm
manycases.
iteratively: (a) Selects a path from the root according to a
heuristic(likeUCT[256],whichpicksnodeswithahighupper-
confidence bound), (b) Expands a new node (a previously
5.12 Chain-of-Action-Thought reasoning
unvisitedstate)fromtheendofthatpath,(c)Simulatesaran-
domrolloutfromthatnewstatetogetanoutcome(e.g.,win LLMs excel in reasoning tasks but rely heavily on external
guidance (e.g., verifiers) or extensive sampling at inference
or loss in a game, or some reward), and (d) Backpropagates
theresultupthetreetoupdatethevaluesofnodesandinform time. Existing methods like CoT [8] lack mechanisms for self-
correctionandadaptiveexploration,limitingtheirautonomy
future selections. Repeating these simulations thousands of
andgeneralization.Satori[260]introducedatwo-stagetrain-
timesconcentratesthesearchonthemostpromisingbranches
ing paradigm, which works by initially tuning the model’s
ofthetree.Inessence,MCTSusesrandomsamplingtoevaluate
output format and then enhancing its reasoning capabilities
the potential of different action sequences, gradually biasing
through self-improvement. In Stage 1 (Format Tuning), the
the search towards those with better average outcomes. In
model is exposed to a large set of 10K synthetic trajectories
LLM reasoning, we can treat the generation of text as a
generatedbyamulti-agentframeworkcomprisingagenerator,
decision process and use to explore different continuations.
a critic, and a reward model. This supervised fine-tuning
For example, at a given question (root), each possible next
helps the model to produce outputs in specific reasoning
reasoning step or answer is an action; a simulation could
format using meta-action tokens, although it may still have
mean letting the LLM continue to a final answer (perhaps
difficultygeneralizingbeyondtheseexamples.InStage2(Self-
with some randomness), and a reward could be whether the
answeriscorrect.Bydoingthisrepeatedly,MCTScanidentify Improvement via RL), the model employs PPO with a Restart
and Explore strategy [260], which allows it to restart from
which chain of thoughts or answers has the highest empirical
intermediatesteps,whethertheywerecorrectornot,torefine
success rate. The appeal of MCTS for reasoning is that it can
itsreasoningprocess.Themodelreceivesrewardsbasedona
handle large search spaces by sampling intelligently rather
combinationofrule-basedcorrectness,reflectionbonuses,and
than exhaustively, and it naturally incorporates uncertainty
preference-basedOutcomeRewardModelfeedbackexplained
andexploration.
in § 5.9, thereby incentivizing the allocation of more compu-
tationalresourcestotougherproblemsandenablingextended
Train verifiers to score intermediate steps reasoningduringtestingforcomplextasks.
(via Monte Carlo rollouts) instead of just final Multi-agent frameworks and advanced fine-tuning strate-
answers. gies are increasingly being explored to enhance reasoning
in LLMs. Multi-Agent LLM Training (MALT) [261] introduces
Recent efforts have integrated MCTS with LLMs to tackle a structured approach where generation, verification, and
complexreasoninganddecision-makingtasks.Oneexampleis refinement steps are distributed across specialized agents,
using MCTS for query planning: Monte Carlo Thought Search allowing for iterative self-correction and improved reasoning
[257],whereanLLMisguidedtoaskaseriesofsub-questionsto chains. Similarly, optimizing preference alignment remains
findananswer.Jayetal.[257]usedanMCTS-basedalgorithm a crucial challenge in ensuring both safety and helpfulness
called ‘Monte Carlo Reasoner’ that treats the LLM as an in LLMs [262]. Approaches like Bi-Factorial Preference Opti-
environment: each node is a prompt (state) and each edge mization (BFPO) [263] reframe RLHF objectives into a single
is an action (e.g., a particular question to ask or step to supervised learning task, reducing human intervention while
take), and random rollouts are used to evaluate outcomes. maintaining robust alignment. Beyond text-based reason-
Thisapproachallowedthesystemtoefficientlyexploreaspace ing,multimodalapproacheslikeMultimodalVisualization-of-
19
Thought(MVoT)[264]extendCoTpromptingbyincorporating TABLE 3: Comprehensive Overview of Reasoning, RL Align-
visual representations, significantly enhancing performance ment,andMultilingualDatasets.Here,pointwiseandpairwise
refer to different methods of evaluating model performance
in spatial reasoning tasks. These advancements highlight the
acrossvarioustasks.
growingneedforstructuredmulti-agentcollaboration,safety-
aware optimization, and multimodal reasoning to address
Datasets Domain Type #Samples EvaluationCriteria
fundamentallimitationsinLLMreasoning[265,266,267]. ReasoningBenchmarks
MATH[269] MathReasoning Pointwise 7,500 Step-by-stepsolutions
GSM8K[270] MathReasoning Pointwise 8.5K Multi-stepreasoning
5.13 Pretraining vs. Test-Time Scaling M W e o t r a ld M T a r t e h e Q V A 2 [ [ 2 2 7 7 1 2 ] ] Ma S t c h ie R nc e e as Q on A ing P Po oi i n n t t w w i i s s e e 4 1 0 ,6 K 8 + 0 S M elf u - l v t e i- r h ifi o c p a e ti x o p n l , a F na O t B io A ns R
PangeaBench[273] MultimodalReasoning Pairwise 47Langs. Culturalunderstanding
PretrainingandTTSaretwodistinctstrategiesforimproving MMMU[274] Science/Math PointwiseCollege-Level Physics,Chemistry,Bilingual
TruthfulQA[275] QA/Reasoning Pointwise N/A Truthfulness
LLM performance, each with different tradeoffs in compu- MathInstruct[276] MathReasoning Pointwise 262K Correctness
MMLU[277,278] MultitaskReasoning Pointwise 57Tasks Broadknowledgeevaluation
tational cost and effectiveness. Pretraining involves scaling MMLU-Fairness[277] Fairness/Reasoning Pointwise N/A Bias/EquityAnalysis
DROP[279] Reading/Reasoning Pointwise 96K Discretereasoningoverparagraphs
model parameters or increasing training data to enhance BBH[175] HardReasoning Pairwise N/A Complexlogicalproblem-solving
VRC-Bench[187] MultimodalReasoning Pairwise N/A VisualReasoningandClassification
capabilities, requiring substantial upfront computational in- RLAlignmentBenchmarks
vestment [3]. In contrast, TTS optimizes inference-time com- H An el t p h S r t o e p e i r cH [2 H 80 - ] RLHF[121] R R L L A A l l i i g g n n m m e en nt t P P a ai ir r w w i i s s e e 3 4 7 2 K .5 + K H M a u rm lti l - e a s t s t n r e ib ss ut a e lig sc n o m ri e n n g t
UltraFeedback[281] RLAlignment Pairwise 64K Instruction-following,Truthfulness
pute (such as iterative refinements, search-based decoding, D4RL[282] RL/Control Pointwise N/A OfflineRLacrossdomains
Meta-World[283] RL/Control Pointwise N/A Multi-taskroboticRL
or adaptive sampling), allowing performance improvements MineRL[284] RL/Games Pairwise N/A Imitationlearning,rewards
withoutmodifyingthebasemodel. MultilingualEvaluation
CulturaX[285] Multilingual Pointwise 6.3T Deduplication,Quality
From a performance vs. cost perspective, TTS achieves PangeaIns[286] Multilingual Pointwise 6M Multilingualinstructions
TydiQA[287] Multilingual Pointwise N/A Cross-lingualQA
results comparable to a model 14× larger on easy to in- XGLUE[288] Multilingual Pointwise N/A Cross-linguallanguagetasks
MM-Eval[289] Multilingual Pairwise 4,981 Task-orientedmultilingualQA
termediate tasks (e.g., MATH benchmarks), while reducing ALM-Bench[289] MultilingualQA Pointwise N/A MultilingualEvaluation
DialogueandSearchBenchmarks
inference costs by 4× fewer FLOPs in compute-intensive
BigBench[290] GeneralComprehensionPointwise 200+Tasks Broadmulti-domainevaluation
scenarios [268]. However, pretraining remains superior for C M h T a B tb e o n t ch Ar [2 e 9 n 1 a ] [291] C C o o m m p p r r e e h h e e n n s s i i o o n n P P a a i ir r w wi is s e e 3 3 3 K K Mult U i-t s u er rn pr c e o f n e v re e n rs c a e tions
RewardBench[167] Comprehension Pairwise 2,998 Userpreference
the hardest tasks or when inference compute constraints are
GeneralComprehensionBenchmarks
high, as larger pretrained models inherently encode deeper ConvAI2[292] Dialogue Pointwise N/A Engagingness,Consistency
MultiWOZ[293] Dialogue Pointwise N/A Tasksuccess,Coherence
reasoningcapabilities. TrecDL21&22[294,295] Search Pointwise 1,549/2,673 Relevancescoring
BEIR[296] Search Pointwise 18Datasets Informationretrieval
Story&RecommendationBenchmarks
HANNA[297] Story Pointwise 1,056 Relevance,Coherence,Complexity
A smaller model with test-time compute can S P t K or U y - E S R afe [2 R 9 L 8] HF[299] V S a t l o u r e y s P P a a i ir r w wi is s e e 8 1 3 0 . 0 4 K K Us H er el p p r f e u f l e n r e e s n s c , e H -b a a rm se l d es r s a n n e k s i s ng
Cvalue[300] Values Pairwise 145K Safety,Responsibility
outperforma14×largermodeloneasy/interme- NaturalInst.[301,302] InstructionTuning Pointwise 1,600+ Instruction-followingevaluation
diate questions, when inference tokens (Y) are
limited(e.g.,self-improvementsettings).
generalcomprehension,anddialogueandsearchtasks.Awell-
structured evaluation framework ensures a comprehensive
In terms of use cases, TTS is useful for scenarios with understanding of an LLM strengths, and limitations across
flexibleinferencebudgetorwhenbasemodelsalreadyexhibit various tasks. These benchmarks play a crucial role in LLM
reasonablecompetenceinthetask.Conversely,pretrainingis post-processingstages,wheremodelsundergofine-tuning,cal-
essential for tasks requiring fundamentally new capabilities ibration,alignment,andoptimizationtoimproveresponseac-
(e.g.,reasoningonnoveldomains)whereinference-timeopti- curacy, robustness, and ethical compliance. Next, we explain
mizationsalonemaynotsuffice. themainbenchmarkgorups.Table3providesanoverviewof
There are notable tradeoffs between the two approaches.
keydatasetscategorizedunderthesebenchmarkgroups.
TTS reduces upfront training costs, making it attractive for ReasoningBenchmarks.ThesebenchmarksassessLLMson
flexible, on-the-go optimization, but requires dynamic com-
their ability to perform logical, mathematical, and scientific
pute allocation at inference. Pretraining, on the other hand,
reasoning.MathematicalreasoningdatasetslikeMATH[269],
incurshighinitialcostsbutguaranteesconsistentperformance
GSM8K [270], and MetaMathQA [271] test models on
without additional runtime overhead, making it ideal for
problem-solving, multi-step arithmetic, and theorem-based
large-scaleAPIdeploymentsorlatency-sensitiveapplications.
problem formulations. Scientific and multimodal reasoning
Overall, TTS and pretraining are complementary in nature. benchmarks such as WorldTree V2 [272] and MMMU [274]
Future LLM systems may adopt a hybrid approach, where evaluate knowledge in physics, chemistry, and multimodal
smaller base models are pretrained with essential knowledge,
understanding, which are crucial for fact-checking and veri-
while TTS dynamically enhances responses through adaptive, fication processes in LLM-generated responses. Additionally,
on-demand computation. This synergy enables more cost-
datasets like PangeaBench [273] extend reasoning tasks into
effectiveandefficientlarge-scalemodeldeployment.
multilingual and cultural domains, enabling models to refine
cross-lingual reasoning. These benchmarks help determine
howwellmodelscanprocessstructuredknowledgeandapply
Choose pretraining for foundational capabil-
logicaldeductions.
ities and test-time scaling for accurate context-
awarerefinement. RL Alignment Benchmarks. RL alignment benchmarks
are central to LLM alignment and post-training optimiza-
tion.Theyrefineresponsegeneration,ethicalconstraints,and
6 Benchmarks for LLM Post-training Evaluation user-aligned outputs through RLHF. Datasets such as Help-
To evaluate the success of LLM post-training phases, a di- Steer [280] and UltraFeedback [281] evaluate models based
verse set of benchmarks have been proposed covering mul- on multi-attribute scoring and alignment with user instruc-
tiple domains: reasoning tasks, alignment, multilinguality, tions. Anthropic’s HH-RLHF [121] explores how well mod-
20
els learn human preference optimization through reinforce- and uncertainty-aware [322] RL methods beyond correlation
ment learning with human feedback. D4RL [282] and Meta- with human uncertanity [323] that safeguard user trust and
World [283] focus on robotic control and offline RL, which prevent adversarial attacks. Another crucial area involves
have implications for autonomous model decision-making. personalization [324, 325] and adaptation [193] (Fig-
MineRL [284] extends RL testing into complex environments ure7e),whereeffortstotailorLLMsforspecificdomainsmust
suchasMinecraft-basedinteractions,usefulfortrainingLLMs be balanced against risks to privacy [326], particularly when
inadaptivedecision-makingsettings. enterprisedataorsensitivepersonalinformationisinvolved.
Multilingual Evaluation.Multilingualbenchmarksarees- In parallel, process [327, 328] vs. outcome reward op-
sentialforLLMpost-processingincross-lingualgeneralization, timization[329](Figure7f)remainsanopenquestion:while
translation adaptation, and fine-tuning for low-resource lan- process-based rewards help guide incremental improvements,
guages. CulturaX [285] and PangeaIns [286] evaluate tok- outcome-focusedmetricsaresimplerbutmaynotcapturecru-
enization, translation, and instruction-following in over 150 cialintermediatedecision-makingsteps.Beyondrewardstruc-
languages, ensuring fairness and diversity in model outputs. ture,fine-tuningLLMsonnewtasksstillencounterissueslike
TydiQA [287] and MM-Eval [289] target bilingual and task- catastrophic forgetting [330] and potential data leakage
oriented multilingual evaluation, enabling improvements in [331,332],underscoringtheneedforparameter-efficientmeth-
LLMfine-tuning.ThesedatasetsensurethatLLMsarenotjust ods [60] and privacy-preserving strategies such as differential
English-centricbutoptimizedformultilingualadaptability. privacy [333] and federated learning [334]. Human feedback,
General Comprehension Benchmarks. General compre- while central to alignment, is inherently costly and limited
hensionbenchmarkscontributetomodelfine-tuning,response in scope; methods like Constitutional AI [53] and RLAIF
coherence, and preference optimization. Datasets such as [95] seek to automate parts of this oversight, though they
ChatbotArena[291],MTBench[291],andRewardBench[167] introduce fresh concerns about bias calibration [335] and
test user preference modeling and conversational fluency, model self-consistency [184]. Finally, test-time scaling [111]
crucial for LLM response ranking and re-ranking methods. anddynamicreasoning[336]frameworksposefurtherchal-
BigBench[290]evaluatesbroadmulti-domaincomprehension, lenges:modelsmustlearnwhentoallocatemorecomputation
while MMLU [277, 278] measures correctness and informa- for complex queries, how to adapt verification modules [337]
tiveness. These datasets help in refining LLM fluency, factual efficiently, and how to maintain robust performance even
correctness,andopen-endedresponsegeneration. whenfacingadversarialinputs.Theseconvergingresearchdi-
Dialogue and Search Benchmarks. Dialogue and search rections—spanning rewardmodeling,decoding strategies,in-
benchmarksplayakeyroleinoptimizing LLMretrieval-based terpretability,personalization,andsafefine-tuning—highlight
responses,multi-turncoherence,andinformationretrievalac- themultifacetedroleof RLinLLMsandcollectivelyshapethe
curacy.DatasetssuchasConvAI2[292]andMultiWOZ[293] future trajectory of large-scale language model development.
evaluate multi-turn conversational models, essential for di- Below,wedelveintosomeofthesedirectionsingreaterdetail.
alogue history tracking and adaptive response fine-tuning. Fine-tuning challenges. Fine-tuning remains one of the
For search relevance assessment, BEIR [296] provides large- most direct post-training methods to adapt LLMs to specific
scale human-annotated judgments for retrieval fine-tuning, tasks or domains, yet it faces several open challenges. One
ensuringLLMsgenerateandrankresponseseffectively.TREC fundamentalissueiscatastrophicforgetting–whenupdating
DL21/22[294,295]contributestodocumentrelevanceranking an LLM on new data causes it to lose or degrade previously
andfactretrieval. learned capabilities. Even advanced PEFT methods like LoRA
[60], which greatly reduce the number of trainable weights,
do not fully solve this problem [330]. Future work can ex-
7 Future Directions
plore better continual learning strategies and regularization
We gathered all papers related to post-training methods techniques so that models can acquire new skills without
in LLMs and analyzed their trends, as shown in Figure 7. erasing old ones. For example, new fine-tuning algorithms
Application of RL techniques [303, 57, 40] for refining the (e.g. CURLoRA [330]) explicitly aim to stabilize training and
LLMs have a noticeable increase in prominence since 2020 preserve prior knowledge while adding new tasks. Promising
(Figure 7a), emphasizing the demand for interactive ap- researchdirectionsincludecurriculum-basedfine-tuning[338]
proaches such as human-in-the-loop [35, 304] reinforcement (introducing new facts gradually or in context with known
and scalability [111, 82, 305]. At the same time, reward facts)andhybridtrainingthatcombinesretrievalorexternal
modeling [306, 166, 167] (Figure 7b) has seen a steady rise knowledge bases. For instance, rather than solely adjusting
in interest due to the emergence of self-rewarding language the model’s weights, one could fine-tune LLMs to consult a
models, yet the field still struggles with reward hacking knowledge repository or perform tool use (such as database
[307, 308] and the design of robust [309], failure-aware re- queries or computations) when faced with queries outside
wardfunctionsbeyondrewardhacking[310].Decoding and their original training distribution [339, 340]. This retrieval-
search(Figure7c)methodsincludetree-of-thoughts[84]and augmented fine-tuning [341] could let models incorporate
Monte Carlo [311, 257] strategies aiming to enhance model freshinformationatinferencetime,reducingtheneedtoover-
reasoning through iterative self-critique [312, 304, 29], but writetheirinternalweightswithnewfacts.Anotherapproach
these techniques also demand reliable uncertainty estima- is training models to explicitly represent uncertainty about
tors to prevent excessive computational overhead [313, 111]. new knowledge, thereby enabling them to say ‘I don’t know’
Safety [299, 314, 315], robustness [316], and interpretability or defer to an external source if a query concerns content
[317, 318, 319] have likewise become central concerns (Fig- not seen in pre-training. By blending weight updates with
ure 7d), motivating the development of bias-aware [320, 321] external knowledge integration, future fine-tuned LLMs will
21
(b) Reward modeling trends show RLHF (c) Decoding strategies like Tree-of-
(a) Growing trend in RL for LLMs, with stabilization, with Self-Rewarding Models ThoughtsandMCTSareimprovingLLM
afocusonHuman-in-the-LoopRL. leading,butRewardHackingpersists. reasoninganddecision-making.
(d) Safety and Robustness research is (e)PersonalizationandAdaptationfocus (f) Process Reward Modeling dominates
growing, with Uncertainty-Aware RL en- on Privacy-Preserving RLHF. On-device Outcome-Based Optimization, favoring
suringRLHFmodelreliability. adaptationremainsachallenge. iterativestrategiesforRL-basedLLMs.
Fig.7:YearlyTrendsinRLspecificpost-trainingmethodsforLLMsandemergingresearchdirections.
maintainhigherfactualaccuracyandlowerhallucinationrates itself aligned and correct? There is a risk of feedback loops
onemerginginformation. oranechochamberofbiasesiftheautomatedpreferencesare
Safe Fine-tuning. From an ethical and safety perspective, flawed. An open gap is the creation of robust AI feedback
fine-tuning raises important open research questions. Fine- systems that are calibrated to human values (perhaps peri-
tuning data often contains sensitive or proprietary informa- odically ‘grounded’ by human oversight or by a diverse set
tion [326], which can lead to privacy risks if the model mem- of constitutional principles). The blending of human and AI
orizes and later regurgitates that data. A recent comprehen- feedbackinahierarchicalschemecouldprovideascalableyet
sive survey [342] highlights vulnerabilities in the fine-tuning reliableRLparadigmforLLMs.
stage, such as membership inference attacks (detecting if a
specificrecordwasinthefine-tuningset)anddataextraction Test-time scaling challenges. Open challenges in TTS re-
(recovering parts of the fine-tuning data from the model’s volve around how to orchestrate the inference-time processes
outputs).Mitigatingtheserisksisanopenproblem:methods efficientlyandreliably.Akeyquestionishowmuchcomputing
like differential privacy fine-tuning [333] (adding noise to the is enough for a given query, and how to determine this on
weight updates) and federated fine-tuning (where data never thefly?Usinglessresourcescanresultinmistakes,butusing
leavesuserdevicesandonlyaggregatedupdatesaresenttothe too much is inefficient and could introduce inconsistencies.
model) are being actively explored. However, these methods Recent research by Snell et al. [83] tackled it by proposing
often come at the cost of model utility or require careful a unified framework with a ‘Proposer’ and a ‘Verifier’ to
calibrationtoavoiddegradingperformance. systematically explore and evaluate answers. In their frame-
Limitations of Human Feedback. Human feedback is work,theProposer(usuallythebaseLLM)generatesmultiple
costly and subjective. One promising avenue to address the candidate solutions, and the Verifier (another model or a
limitations of human feedback is using AI feedback and heuristic) judges and selects the best. The optimal strategy
automation to assist or replace human evaluators. Constitu- can vary by problem difficulty: for easier queries, generat-
tionalAI[53],introducedbyAnthropic,isanotableexample: ing many answers in parallel and picking the top might be
instead of relying on extensive human feedback for every sufficient, whereas for harder problems, sequential, step-by-
harmful or helpful behavior, the model is guided by a set of step reasoning with verification at each step works better.
written principles (a ‘constitution’) andis trained to critique An important future direction is building adaptive systems
and refine its own responses using another AI model as the where the LLM dynamically allocates computation based on
judge [343]. Emerging directions here include RLAIF [95] and an estimate of the question’s complexity. This idea connects
othersemi-automatedfeedbacktechniques[344]:usingstrong tometa-cognitioninAI[314],enablingmodelstohaveasense
models to evaluate or guide weaker models, or even having of what they don’t know or what deserves more thought.
multiple AI agents debate a question and using their agree- Developingreliableconfidencemetricsordifficultypredictors
ment as a reward signal [345, 346]. Such AI-aided feedback for LLMs is an open research area, but progress here would
could vastly scale the tuning process and help overcome the make TTS far more practical i.e., the model would only ‘slow
bottleneck of limited human expert time. However, it raises downandthink’whennecessary,muchlikeahumanspending
new theoretical questions: how do we ensure the AI judge is extra time on a hard problem. Additionally, By reframing
22
inference-timescalingasaprobabilisticinferenceproblemand encryption during inference; differential privacy via reward
employingparticle-basedMonteCarlomethods[?],thesmall noising[354],whichintroducesmathematicallyboundednoise
modelsachievedo1levelaccuracyinonly32rollouts,a4–16x intoRLHFpreferencerankingsduringalignment;andfederated
improvementinscalingefficiencyacrossvariousmathematical distillation, which aggregates knowledge from decentralized
reasoning tasks. Recent study [347] shows distilling test-time user-specificmodelswithoutsharingrawdata.
computations into synthetic training data creates synergistic Collaborative Multi-Model Systems. As single-model
pretrainingbenefitswhichcanalsobefurtherexplored. [355, 356, 357] scaling approaches physical limits, alterna-
Reward Modeling and Credit Assignment. Current RL tive paradigms such as multi-agent LLM collaboration [358,
approachessufferfromrewardmisgeneralization,wheremod- 359, 178] become necessary. Researchers are investigating
elsover-optimizesuperficialproxymetricsratherthangenuine emergent communication protocols that train models to de-
reasoning quality. The sparse nature of terminal rewards velop lossy compression “languages” for inter-model knowl-
in multi-step tasks increases credit assignment challenges, edgetransfersuchasGenAINet[360],robustensembleswhere
particularly in long-horizon reasoning scenarios. Traditional stress-testinducedspecializationdrivesautomaticdivisionof
methods like DPO require inefficient pairwise preference data problem spaces based on failure analysis [361], and gradient-
andfailtoutilizefailuretrajectorieseffectively.Hybridreward freesynergylearningthroughevolutionarystrategiesdesigned
models can be investigated by integrating process supervi- todiscovercomplementarymodelcombinationswithoutrely-
sion with outcome-based rewards using contrastive stepwise ingonbackpropagation[362].
evaluation [348]. This approach enables a more granular as- Multimodal RL Integration. Multimodal reinforcement
sessmentofintermediatedecision-makingstepswhilealigning learning [363, 49, 364] faces the obstacle of a combinatorial
with long-term objectives. Recent work [171] suggests step- stateexplosion,especiallyincontextsexceeding128ktokens.
level policy optimization could improve value function ac- Pioneering methods to overcome this include hierarchical
curacy while maintaining safety constraints. Dynamic credit attention frameworks that employ modality-specific policies
assignment mechanisms can be explored through temporal with cross-attention gating [365], adaptive truncation strate-
difference learning adapted for transformers [349, 350]. Such giesthatcompresscontextwhilepreservingcriticalreasoning
adaptationsmayenhancethemodel’sabilitytocapturelong- segments[366],andflashcurriculumapproachesthatleverage
range dependencies and optimize reward propagation over self-supervised[367,368,369]complexitypredictiontofacili-
extended sequences. Failure-aware training strategies can be tateprogressivemultimodalintegration.
developedbyincorporatingnegativeexamplesintotheRLloop Efficient RL Training. Efficient RL training paradigms con-
via adversarial data augmentation [351]. This can improve tinue to be a critical research frontier as current meth-
modelrobustnessbysystematicallyexposingittochallenging ods exhibit significant sample inefficiency and computational
scenariosandencouragingmoreresilientpolicylearning. overhead. Addressing issues like the overthinking [370, 371]
Efficient RL Training and Distillation. Current RL meth- phenomenon, where excessive reasoning chains waste valu-
odsforLLMsrequireprohibitivecomputationalresources[352] able computation [145], requires approaches such as partial
while often underperforming knowledge distillation tech- rollout strategies [372], adaptive length penalty mechanisms
niques [93]. This inefficiency limits scalability and practical employing learned compression transformers, and hybrid ar-
deployment,asdistilledmodelsfrequentlysurpassRL-trained chitecturesthatcombineMCTSwithadvancedRLoptimizers.
counterparts despite requiring less training overhead. Addi- These innovations are essential for scaling RL to long-context
tionally, pure RL approaches struggle to balance language taskswhileminimizingwastedcomputationalresources.
quality with reasoning improvement [97, 93], creating a per-
formanceceiling.
The development of hybrid frameworks that initialize RL
.OverthinkingPhenomenon:Analysisreveals
policieswithdistilledknowledgefromlargemodels,combining 22% wasted computation is in reasoning chains
theexploratorybenefitsof RLwiththestabilityofsupervised exceedingoptimalreasoninglength.
learningisaninterestingdirection.Similarly,curriculumsam-
pling strategies that progressively increase task complexity RLmethodsexhibitsampleinefficiencyandcomputational
while using distillation to preserve linguistic coherence can overhead, particularly when scaling to contexts exceeding
also help. PEFT methods [60] can be leveraged during RL up- 128k tokens. The ‘overthinking’ phenomenon, where models
datestomaintainbasecapabilitieswhileenhancingreasoning. generate excessively long reasoning chains, further reduces
token efficiency and increases deployment costs [373]. Inves-
tigate partial rollout strategies with flash attention mech-
Integration: Combining PRM-guided tree anisms for long-context processing. Develop length penalty
search with online distillation achieves 4× effi- mechanismsusinglearnedcompressiontransformersforitera-
ciency gains over baseline methods, while main- tive long2short distillation. Hybrid architectures combining
taining94%solutionaccuracyonMATHdataset. MCTS [74] with GRPO [59] could enable better exploration-
exploitationtradeoffs.ParallelworkbyXieet.al.[74]demon-
Privacy-Preserving Personalization. Customizing mod- strates promising results through adaptive tree search prun-
els for enterprise and individual use cases raises the risk of ing. Several open challenges persist in the field. Uncertainty
exposingprivatetrainingdatathroughmemorization,making propagation remains problematic as current confidence esti-
privacy-preserving [326] adaptation essential. Promising so- matorsaddapproximately18%latencyoverhead,whilecatas-
lutions include homomorphic instruction tuning [353], which trophic forgetting rresults in a degradation of 29% of base
processesencrypteduserquerieswhilemaintainingend-to-end capabilitiesduringRLfine-tuning[374].Moreover,benchmark
23
saturation is an issue, with MMLU scores correlating poorly (r [13] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,
=0.34)withreal-worldperformance[375]. A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan,
et al., “The llama 3 herd of models,” arXiv preprint
arXiv:2407.21783,2024. 1,5
. Adversarial Vulnerabilities: Stress tests re- [14] G.Team,M.Riviere,S.Pathak,P.G.Sessa,C.Hardin,S.Bhu-
patiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ramé,
veal a high success rate on gradient-based
etal.,“Gemma2:Improvingopenlanguagemodelsatapracti-
promptinjections. calsize,”arXivpreprintarXiv:2408.00118,2024. 1,5
[15] G.Team,R.Anil,S.Borgeaud,J.-B.Alayrac,J.Yu,R.Soricut,
J.Schalkwyk,A.M.Dai,A.Hauth,K.Millican,etal.,“Gemini:
afamilyofhighlycapablemultimodalmodels,”arXivpreprint
8 Conclusion
arXiv:2312.11805,2023. 1,5
Thissurveyandtutorialprovidesasystematicreviewofpost- [16] A.Liu,B.Feng,B.Wang,B.Wang,B.Liu,C.Zhao,C.Dengr,
C. Ruan, D. Dai, D. Guo, et al., “Deepseek-v2: A strong,
training methodologies for LLMs, focusing on fine-tuning, re-
economical,andefficientmixture-of-expertslanguagemodel,”
inforcementlearning,andscaling.Weanalyzekeytechniques,
arXivpreprintarXiv:2405.04434,2024. 1,2,5,6
along with strategies for improving efficiency and alignment [17] M.Abdin,J.Aneja,H.Awadalla,A.Awadallah,A.A.Awan,
with human preferences. Additionally, we explore the role of N.Bach,A.Bahree,A.Bakhtiari,J.Bao,H.Behl,etal.,“Phi-
3technicalreport:Ahighlycapablelanguagemodellocallyon
RLinenhancingLLMsthroughreasoning,planning,andmulti-
yourphone,”arXivpreprintarXiv:2404.14219,2024. 1,5
task generalization, categorizing their functionalities within [18] A.Fan,M.Lewis,andY.Dauphin,“Hierarchicalneuralstory
the agent-environment paradigm. Recent advancements in generation,”arXivpreprintarXiv:1805.04833,2018. 1
[19] C.Chhun,P.Colombo,C.Clavel,andF.M.Suchanek,“Ofhu-
reinforcementlearningandtest-timescalinghavesignificantly
mancriteriaandautomaticmetrics:Abenchmarkoftheeval-
improvedLLMsreasoningcapabilities,enablingthemtotackle
uationofstorygeneration,”arXivpreprintarXiv:2208.11646,
increasingly complex tasks. By consolidating the latest re- 2022. 1
searchandidentifyingopenchallenges,weaimtoguidefuture [20] S. Arif, S. Farid, A. H. Azeemi, A. Athar, and A. A. Raza,
“Thefellowshipofthellms:Multi-agentworkflowsforsynthetic
effortsinoptimizingLLMsforreal-worldapplications.
preference optimization dataset generation,” arXiv preprint
arXiv:2408.08688,2024. 1
[21] S.Ye,Y.Jo,D.Kim,S.Kim,H.Hwang,andM.Seo,“Selfee:
References
Iterative self-revising llm empowered by self-feedback genera-
[1] A. Vaswani, “Attention is all you need,” Advances in Neural tion,”Blogpost,2023. 1
InformationProcessingSystems,2017. 1 [22] M. Musolesi, “Creative beam search: Llm-as-a-judge for im-
[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, provingresponsegeneration,”ICCC,2024. 1
P.Dhariwal,A.Neelakantan,P.Shyam,G.Sastry,A.Askell, [23] J.Ren,Y.Zhao,T.Vu,P.J.Liu,andB.Lakshminarayanan,
et al., “Language models are few-shot learners,” Advances in “Self-evaluationimprovesselectivegenerationinlargelanguage
neuralinformationprocessingsystems,vol.33,pp.1877–1901, models,”inProceedingson,pp.49–64,PMLR,2023. 1
2020. 1 [24] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,
[3] Z.Yang,“Xlnet:Generalizedautoregressivepretrainingforlan- N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rock-
guageunderstanding,”arXivpreprintarXiv:1906.08237,2019. täschel,etal.,“Retrieval-augmentedgenerationforknowledge-
1,3,19 intensivenlptasks,”AdvancesinNeuralInformationProcess-
[4] J.Devlin,“Bert:Pre-trainingofdeepbidirectionaltransformers ingSystems,vol.33,pp.9459–9474,2020. 1,2
forlanguageunderstanding,”arXivpreprintarXiv:1810.04805, [25] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer,
2018. 1,2,3 W.-t.Yih,D.Fried,S.Wang,andT.Yu,“Ds-1000:Anatural
[5] Z. Lan, “Albert: A lite bert for self-supervised learning of and reliable benchmark for data science code generation,” in
language representations,” arXiv preprint arXiv:1909.11942, International Conference on Machine Learning, pp. 18319–
2019. 1 18345,PMLR,2023. 1
[6] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, [26] B. Zhu,E. Frick,T. Wu, H. Zhu, K.Ganesan, W.-L.Chiang,
M.Matena,Y.Zhou,W.Li,andP.J.Liu,“Exploringthelimits J. Zhang, and J. Jiao, “Starling-7b: Improving helpfulness
of transfer learning with a unified text-to-text transformer,” andharmlessnesswithrlaif,”inFirstConferenceonLanguage
Journalofmachinelearningresearch,vol.21,no.140,pp.1– Modeling,2024. 1,5
67,2020. 1 [27] D.Paul,M.Ismayilzada,M.Peyrard,B.Borges,A.Bosselut,
[7] P. Verga, S. Hofstatter, S. Althammer, Y. Su, A. Piktus, R.West,andB.Faltings,“Refiner:Reasoningfeedbackonin-
A.Arkhangorodsky,M.Xu,N.White,andP.Lewis,“Replacing termediaterepresentations,”arXivpreprintarXiv:2304.01904,
judgeswithjuries:Evaluatingllmgenerationswithapanelof 2023. 1
diversemodels,”arXivpreprintarXiv:2404.18796,2024. 1 [28] Y.Xie,K.Kawaguchi,Y.Zhao,J.X.Zhao,M.-Y.Kan,J.He,
[8] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, andM.Xie,“Self-evaluationguidedbeamsearchforreasoning,”
Q.V.Le,D.Zhou,etal.,“Chain-of-thoughtpromptingelicits Advances in Neural Information Processing Systems, vol. 36,
reasoninginlargelanguagemodels,”Advancesinneuralinfor- 2024. 1
mationprocessingsystems,vol.35,pp.24824–24837,2022. 1, [29] H. Ye and H. T. Ng, “Self-judge: Selective instruction
2,3,11,12,15,18 following with alignment self-evaluation,” arXiv preprint
[9] C.Wang,Y.Deng,Z.Lyu,L.Zeng,J.He,S.Yan,andB.An, arXiv:2409.00935,2024. 1,20
“Q*:Improvingmulti-stepreasoningforllmswithdeliberative [30] Z. Luo, H. Wu, D. Li, J. Ma, M. Kankanhalli, and J. Li,
planning,”arXivpreprintarXiv:2406.14283,2024. 1 “Videoautoarena: An automated arena for evaluating large
[10] Y. Wu, X. Han, W. Song, M. Cheng, and F. Li, “Mindmap: multimodalmodelsinvideoanalysisthroughusersimulation,”
Constructingevidencechainsformulti-stepreasoninginlarge arXivpreprintarXiv:2411.13281,2024. 1
languagemodels,”inProceedingsoftheAAAIConference on [31] S. Deng, W. Zhao, Y.-J. Li, K. Wan, D. Miranda, A. Kale,
ArtificialIntelligence,vol.38,pp.19270–19278,2024. 1 and Y. Tian, “Efficient self-improvement in multimodal large
[11] Y.Ding,Z.Wang,W.Ahmad,H.Ding,M.Tan,N.Jain,M.K. language models: A model-level judge-free approach,” arXiv
Ramanathan,R.Nallapati,P.Bhatia,D.Roth,etal.,“Cross- preprintarXiv:2411.17760,2024. 1,2
codeeval: A diverse and multilingual benchmark for cross-file [32] T.Xiong,X.Wang,D.Guo,Q.Ye,H.Fan,Q.Gu,H.Huang,
codecompletion,”AdvancesinNeuralInformationProcessing andC.Li,“Llava-critic:Learningtoevaluatemultimodalmod-
Systems,vol.36,2024. 1 els,”arXivpreprintarXiv:2410.02712,2024.
[12] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, [33] D. Chen, R. Chen, S. Zhang, Y. Liu, Y. Wang, H. Zhou,
F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, Q.Zhang,Y.Wan,P.Zhou,andL.Sun,“Mllm-as-a-judge:As-
S. Anadkat, et al., “Gpt-4 technical report,” arXiv preprint sessingmultimodalllm-as-a-judgewithvision-languagebench-
arXiv:2303.08774,2023. 1
24
mark,”arXivpreprintarXiv:2402.04788,2024. 1 Linguistics(Volume1:LongPapers),pp.7601–7614,2024. 2
[34] T. Hagendorff, S. Fabi, and M. Kosinski, “Human-like intu- [57] R.Rafailov,A.Sharma,E.Mitchell,C.D.Manning,S.Ermon,
itivebehaviorandreasoningbiasesemergedinlargelanguage and C. Finn, “Direct preference optimization: Your language
models but disappeared in chatgpt,” Nature Computational modelissecretlyarewardmodel,”AdvancesinNeuralInfor-
Science,vol.3,no.10,pp.833–838,2023. 1 mationProcessingSystems,vol.36,2024. 2,3,6,20
[35] Q. Pan, Z. Ashktorab, M. Desmond, M. S. Cooper, J. John- [58] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,
son, R. Nair, E. Daly, and W. Geyer, “Human-centered P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al.,
design recommendations for llm-as-a-judge,” arXiv preprint “Training language models to follow instructions with human
arXiv:2407.03479,2024. 1,20 feedback,”Advancesinneuralinformationprocessingsystems,
[36] G.H.Chen,S.Chen,Z.Liu,F.Jiang,andB.Wang,“Humans vol.35,pp.27730–27744,2022. 2,3,5,9,13,15
or llms as the judge? a study on judgement biases,” arXiv [59] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang,
preprintarXiv:2402.10669,2024. 1 M. Zhang, Y. Li, Y. Wu, et al., “Deepseekmath: Pushing the
[37] A. Newell, “Human problem solving,” Upper Saddle limits of mathematical reasoning in open language models,”
River/PrentiveHall,1972. 1,16 arXivpreprintarXiv:2402.03300,2024. 2,3,6,8,10,12,22
[38] X.Wang,H.Kim,S.Rahman,K.Mitra,andZ.Miao,“Human- [60] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,
llm collaborative annotation through effective verification of L. Wang, and W. Chen, “Lora: Low-rank adaptation of large
llm labels,” in Proceedings of the CHI Conference on Human languagemodels,”arXivpreprintarXiv:2106.09685,2021. 2,
FactorsinComputingSystems,pp.1–21,2024. 1 13,20,22
[39] R. OpenAI, “Gpt-4 technical report. arxiv 2303.08774,” View [61] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-
inArticle,vol.2,no.5,2023. 1,5,12 rag: Learning to retrieve, generate, and critique through self-
[40] D.Guo,D.Yang,H.Zhang,J.Song,R.Zhang,R.Xu,Q.Zhu, reflection,”arXivpreprintarXiv:2310.11511,2023. 2
S. Ma, P. Wang, X. Bi, et al., “DeepSeek-R1: Incentivizing [62] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai,
reasoningcapabilityinllmsviareinforcementlearning,”arXiv J. Sun, M. Wang, and H. Wang, “Retrieval-augmented gen-
preprintarXiv:2501.12948,2025. 1,5,11,12,20 eration for large language models: A survey,” arXiv preprint
[41] M.T.Hicks,J.Humphries,andJ.Slater,“Chatgptisbullshit,” arXiv:2312.10997,2023. 2
Ethics and Information Technology, vol. 26, no. 2, pp. 1–10, [63] Z. Hu, L. Song, J. Zhang, Z. Xiao, J. Wang, Z. Chen, and
2024. 1 H.Xiong,“Rethinkingllm-basedpreferenceevaluation,”arXiv
[42] N.Maleki,B.Padmanabhan,andK.Dutta,“Aihallucinations: preprintarXiv:2407.01085,2024. 2
Amisnomerworthclarifying,”2024. 1 [64] S. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y. Zhou,
[43] A. Bruno, P. L. Mazzeo, A. Chetouani, M. Tliba, and M. A. Y. Xiao, S. Yun, X. Huang, et al., “Disc-lawllm: Fine-tuning
Kerkouri,“Insightsintoclassifyingandmitigatingllms’hallu- large language models for intelligent legal services,” arXiv
cinations,”arXivpreprintarXiv:2311.08117,2023. 1 preprintarXiv:2309.11325,2023. 2
[44] S.Farquhar,J.Kossen,L.Kuhn,andY.Gal,“Detectinghal- [65] Y. Luo, Z. Yang, F. Meng, Y. Li, J. Zhou, and Y. Zhang,
lucinationsinlargelanguagemodelsusingsemanticentropy,” “An empirical study of catastrophic forgetting in large lan-
Nature,vol.630,no.8017,pp.625–630,2024. 1 guage models during continual fine-tuning,” arXiv preprint
[45] F. Leiser, S. Eckhardt, V. Leuthe, M. Knaeble, A. Maedche, arXiv:2308.08747,2023. 2
G. Schwabe, and A. Sunyaev, “Hill: A hallucination identifier [66] H.Li,J.Chen,W.Su,Q.Ai,andY.Liu,“Towardsbetterweb
forlargelanguagemodels,”inProceedingsoftheCHIConfer- search performance: Pre-training, fine-tuning and learning to
enceonHumanFactorsinComputingSystems,pp.1–13,2024. rank,”2023. 2
1 [67] OpenAI,“Reinforcementfine-tuning.”(Accessed:2025-12-6).2
[46] A.Gunjal,J.Yin,andE.Bas,“Detectingandpreventinghallu- [68] W.Zhang,Y.Deng,B.Liu,S.J.Pan,andL.Bing,“Sentiment
cinationsinlargevisionlanguagemodels,”inProceedingsofthe analysisintheeraoflargelanguagemodels:Arealitycheck,”
AAAIConferenceonArtificialIntelligence,vol.38,pp.18135– arXivpreprintarXiv:2305.15005,2023. 2
18143,2024. 1 [69] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,
[47] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, andC.Potts,“Learningwordvectorsforsentimentanalysis,”
and Z. Hu, “Reasoning with language model is planning with in Proceedings of the 49th annual meeting of the association
worldmodel,”inProceedingsofthe2023ConferenceonEmpir- for computational linguistics: Human language technologies,
ical Methods in Natural Language Processing, EMNLP 2023, pp.142–150,2011. 2
Singapore, December 6-10, 2023 (H. Bouamor, J. Pino, and [70] Y. Wang, S. Wang, Y. Li, and D. Dou, “Recognizing medical
K. Bali, eds.), pp. 8154–8173, Association for Computational searchqueryintentbyfew-shotlearning,”inProceedingsofthe
Linguistics,2023. 1 45th International ACM SIGIR Conference on Research and
[48] Y.Ye,Z.Huang,Y.Xiao,E.Chern,S.Xia,andP.Liu,“Limo: DevelopmentinInformationRetrieval,pp.502–512,2022. 2
Lessismoreforreasoning,”2025. 1,3 [71] R.Luo,L.Sun,Y.Xia,T.Qin,S.Zhang,H.Poon,andT.-Y.
[49] C. Li, W. Wu, H. Zhang, Y. Xia, S. Mao, L. Dong, I. Vulić, Liu, “Biogpt: generative pre-trained transformer for biomedi-
and F. Wei, “Imagine while reasoning in space: Multimodal cal text generation and mining,” Briefings in bioinformatics,
visualization-of-thought,”2025. 1,3,22 vol.23,no.6,p.bbac409,2022. 2,13
[50] T. Xia, B. Yu, Y. Wu, Y. Chang, and C. Zhou, “Language [72] O. Wysocki, M. Wysocka, D. Carvalho, A. T. Bogatu, D. M.
models can evaluate themselves via probability discrepancy,” Gusicuma,M.Delmas,H.Unsworth,andA.Freitas,“Anllm-
arXivpreprintarXiv:2405.10516,2024. 1 basedknowledgesynthesisandscientificreasoningframework
[51] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, for biomedical discovery,” arXiv preprint arXiv:2406.18626,
Y.J.Bang,A.Madotto,andP.Fung,“Surveyofhallucination 2024. 2
in natural language generation,” ACM Computing Surveys, [73] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
vol.55,no.12,pp.1–38,2023. 1,3 O. Klimov, “Proximal policy optimization algorithms,” arXiv
[52] H. He andW. J. Su,“A lawof next-tokenprediction in large preprintarXiv:1707.06347,2017. 2,5,8,10
languagemodels,”2024. 2 [74] Y. Xie, A. Goyal, W. Zheng, M.-Y. Kan, T. P. Lillicrap,
[53] Y.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,A.Jones, K.Kawaguchi,andM.Shieh,“Montecarlotreesearchboosts
A.Chen,A.Goldie,A.Mirhoseini,C.McKinnon,etal.,“Con- reasoning via iterative preference learning,” arXiv preprint
stitutionalai:Harmlessnessfromaifeedback,”arXivpreprint arXiv:2405.00451,2024. 2,18,22
arXiv:2212.08073,2022. 2,20,21 [75] M.Shanahan,K.McDonell,andL.Reynolds,“Roleplaywith
[54] interconnects.ai, “blob reinforcement fine-tuning.” (Accessed: largelanguagemodels,”Nature,vol.623,no.7987,pp.493–498,
2025-12-6). 2 2023. 2
[55] E. Lobo, C. Agarwal, and H. Lakkaraju, “On the impact [76] T.Xu, E.Helenowski,K. A. Sankararaman,D.Jin, K.Peng,
of fine-tuning on chain-of-thought reasoning,” arXiv preprint E.Han,S.Nie,C.Zhu,H.Zhang,W.Zhou,etal.,“Theperfect
arXiv:2411.15382,2024. 2,3 blend:Redefiningrlhfwithmixtureofjudges,”arXivpreprint
[56] L. Trung, X. Zhang, Z. Jie, P. Sun, X. Jin, and H. Li, “Reft: arXiv:2409.20370,2024. 2
Reasoning with reinforced fine-tuning,” in Proceedings of the [77] M. Cao, L. Shu, L. Yu, Y. Zhu, N. Wichers, Y. Liu, and
62nd Annual Meeting of the Association for Computational L. Meng, “Beyond sparse rewards: Enhancing reinforcement
25
learningwithlanguagemodelcritiqueintextgeneration,”2024. J. Coady, D. Peng, Y. Qiao, L. Benson, L. Sun, A. Wardle-
2 Solano, H. Szabo, E. Zubova, M. Burtell, J. Fan, Y. Liu,
[78] Y. Dubois, C. X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, B.Wong,M.Sailor,A.Ni,L.Nan,J.Kasai,T.Yu,R.Zhang,
C.Guestrin,P.S.Liang,andT.B.Hashimoto,“Alpacafarm:A A. R. Fabbri, W. Kryscinski, S. Yavuz, Y. Liu, X. V. Lin,
simulationframeworkformethodsthatlearnfromhumanfeed- S.Joty,Y.Zhou,C.Xiong,R.Ying,A.Cohan,andD.Radev,
back,” Advances in Neural Information Processing Systems, “Folio:Naturallanguagereasoningwithfirst-orderlogic,”2024.
vol.36,2024. 2,15 3
[79] L. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, [99] Z.Xi,S.Jin,Y.Zhou,R.Zheng,S.Gao,T.Gui,Q.Zhang,and
A. Zipori, H. Noga, O. Keller, B. Piot, I. Szpektor, et al., X. Huang, “Self-polish: Enhance reasoning in large language
“Multi-turn reinforcement learning from preference human modelsviaproblemrefinement,”2024. 3
feedback,”arXivpreprintarXiv:2405.14655,2024. 2 [100] A.SaparovandH.He,“Languagemodelsaregreedyreasoners:
[80] Z. Li, Y. He, L. He, J. Wang, T. Shi, B. Lei, Y. Li, Asystematicformalanalysisofchain-of-thought,”2023. 3
and Q. Chen, “Falcon: Feedback-driven adaptive long/short- [101] J.Liu,C.Wang,C.Y.Liu,L.Zeng,R.Yan,Y.Sun,Y.Liu,
term memory reinforced coding optimization system,” arXiv andY.Zhou,“Improvingmulti-stepreasoningabilitiesoflarge
preprintarXiv:2410.21349,2024. 2 language models with direct advantage policy optimization,”
[81] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, arXivpreprintarXiv:2412.18279,2024. 3
C.Hesse,S.Jain,V.Kosaraju,W.Saunders,etal.,“Webgpt: [102] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa,
Browser-assisted question-answering with human feedback,” “Large language models are zero-shot reasoners,” Advances
arXivpreprintarXiv:2112.09332,2021. 2,11,14 in neural information processing systems, vol. 35, pp. 22199–
[82] L. Gao, J. Schulman, and J. Hilton, “Scaling laws for reward 22213,2022. 3
modeloveroptimization,”inInternationalConferenceonMa- [103] X. Deng, Y. Su, A. Lees, Y. Wu, C. Yu, and H. Sun, “Rea-
chineLearning,pp.10835–10866,PMLR,2023. 2,15,20 sonbert:Pre-trainedtoreasonwithdistantsupervision,”arXiv
[83] C.Snell,J.Lee,K.Xu,andA.Kumar,“Scalingllmtest-time preprintarXiv:2109.04912,2021. 3
compute optimally can be more effective than scaling model [104] T. Sawada, D. Paleka, A. Havrilla, P. Tadepalli, P. Vidas,
parameters,”arXivpreprintarXiv:2408.03314,2024. 2,15,21 A. Kranias, J. J. Nay, K. Gupta, and A. Komatsuzaki, “Arb:
[84] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and Advanced reasoning benchmark for large language models,”
K.Narasimhan,“Treeofthoughts:Deliberateproblemsolving 2023. 3
withlargelanguagemodels,”AdvancesinNeuralInformation [105] A.Radford,“Improvinglanguageunderstandingbygenerative
ProcessingSystems,vol.36,2024. 2,14,16,20 pre-training,”2018. 3
[85] J.Jiang,D.He,andJ.Allan,“Searching,browsing,andclicking [106] I. J. Myung, “Tutorial on maximum likelihood estimation,”
inasearchsession:Changesinuserbehaviorbytaskandover JournalofmathematicalPsychology,vol.47,no.1,pp.90–100,
time,” in Proceedings of the 37th international ACM SIGIR 2003. 3
conferenceonResearch&developmentininformationretrieval, [107] Y. Shao, J. Mao, Y. Liu, W. Ma, K. Satoh, M. Zhang, and
pp.607–616,2014. 2 S. Ma, “Bert-pli: Modeling paragraph-level interactions for
[86] Y.Xie,K.Kawaguchi,Y.Zhao,J.X.Zhao,M.-Y.Kan,J.He, legalcaseretrieval.,”inIJCAI,pp.3501–3507,2020. 3
andM.Xie,“Self-evaluationguidedbeamsearchforreasoning,” [108] J.Wei,Y.Tay,R.Bommasani,C.Raffel,B.Zoph,S.Borgeaud,
Advances in Neural Information Processing Systems, vol. 36, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al.,
2024. 2 “Emergentabilitiesoflargelanguagemodels,”arXivpreprint
[87] Y. Tian, B. Peng, L. Song, L. Jin, D. Yu, H. Mi, and D. Yu, arXiv:2206.07682,2022. 3
“Toward self-improvement of llms via imagination, searching, [109] R.Bellman,“Amarkoviandecisionprocess,”Journalofmath-
andcriticizing,”arXivpreprintarXiv:2404.12253,2024. 2,18 ematicsandmechanics,pp.679–684,1957. 3,10
[88] K. Gandhi, D. H. J. Lee, G. Grand, M. Liu, W. Cheng, [110] S.Hao,Y.Gu,H.Luo,T.Liu,X.Shao,X.Wang,S.Xie,H.Ma,
A.Sharma,andN.Goodman,“Streamofsearch(SoS):Learn- A. Samavedhi, Q. Gao, Z. Wang, and Z. Hu, “Llm reasoners:
ing to search in language,” in First Conference on Language Newevaluation,library,andanalysisofstep-by-stepreasoning
Modeling,2024. 2 withlargelanguagemodels,”2024. 3
[89] K.Yang,A.M.Swope,A.Gu,R.Chalamala,P.Song,S.Yu, [111] J.Geiping,S.McLeish,N.Jain,J.Kirchenbauer,S.Singh,B.R.
S. Godil, R. Prenger, and A. Anandkumar, “Leandojo: Theo- Bartoldson,B.Kailkhura,A.Bhatele,andT.Goldstein,“Scal-
remprovingwithretrieval-augmentedlanguagemodels,”2023. ing up test-time compute with latent reasoning: A recurrent
2 depthapproach,”2025. 3,20
[90] C.Sun,S.Huang,andD.Pompili,“Retrieval-augmentedhier- [112] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker,
archicalin-contextreinforcementlearningandhindsightmod- T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe,
ularreflectionsfortaskplanningwithllms,”2024. 2 “Let’s verify step by step,” arXiv preprint arXiv:2305.20050,
[91] E.Davis,“Testinggpt-4-o1-previewonmathandscienceprob- 2023. 4
lems: A follow-up study,” arXiv preprint arXiv:2410.22340, [113] G.Chen,M.Liao,C.Li,andK.Fan,“Step-levelvalueprefer-
2024. 2 enceoptimizationformathematicalreasoning,”arXivpreprint
[92] J. Y. Wang, N. Sukiennik, T. Li, W. Su, Q. Hao, J. Xu, arXiv:2406.10858,2024. 4
Z.Huang,F.Xu,andY.Li,“Asurveyonhuman-centricllms,” [114] K.Nguyen,H.DauméIII,andJ.Boyd-Graber,“Reinforcement
arXivpreprintarXiv:2411.14491,2024. 3 learningforbanditneuralmachinetranslationwithsimulated
[93] J.Wu,S.Yang,R.Zhan,Y.Yuan,L.S.Chao,andD.F.Wong, humanfeedback,”arXivpreprintarXiv:1707.07402,2017. 4,5
“Asurveyonllm-generatedtextdetection:Necessity,methods, [115] G.Williams,“Substantiveandadjectivallaw,”inLearningthe
and future directions,” Computational Linguistics, pp. 1–65, Law,pp.19–23,LawBookCompany,Limited,1982. 4
2025. 3,14,22 [116] M.Ranzato,S.Chopra,M.Auli,andW.Zaremba,“Sequence
[94] Y.Chang,X.Wang,J.Wang,Y.Wu,L.Yang,K.Zhu,H.Chen, leveltrainingwithrecurrentneuralnetworks,”2016. 4,5
X. Yi, C. Wang, Y. Wang, et al., “A survey on evaluation [117] S.J.Rennie,E.Marcheret,Y.Mroueh,J.Ross,andV.Goel,
of large language models,” ACM Transactions on Intelligent “Self-critical sequence training for image captioning,” in Pro-
SystemsandTechnology,vol.15,no.3,pp.1–45,2024. 3 ceedingsoftheIEEEconferenceoncomputervisionandpattern
[95] H.Lee,S.Phatale,H.Mansoor,K.R.Lu,T.Mesnard,J.Ferret, recognition,pp.7008–7024,2017. 4,5
C.Bishop,E.Hall,V.Carbune,andA.Rastogi,“Rlaif:Scaling [118] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a
reinforcementlearningfromhumanfeedbackwithaifeedback,” method for automatic evaluation of machine translation,” in
2023. 3,9,20,21 Proceedings of the 40th annual meeting of the Association for
[96] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, ComputationalLinguistics,pp.311–318,2002. 4
J. Xu, and Z. Sui, “A survey on in-context learning,” arXiv [119] R.Vedantam,C.L.Zitnick,andD.Parikh,“Cider:Consensus-
preprintarXiv:2301.00234,2022. 3 basedimagedescriptionevaluation,”2015. 4
[97] W.X.Zhao,K.Zhou,J.Li,T.Tang,X.Wang,Y.Hou,Y.Min, [120] OpenAI,“Openaigpt-4.5systemcard,”2025. Accessed:2025-
B.Zhang,J.Zhang,Z.Dong,etal.,“Asurveyoflargelanguage 02-28. 5
models,”arXivpreprintarXiv:2303.18223,2023. 3,22 [121] Anthropic,“Claude3.7sonnet,”2025.Accessed:2025-02-26.5,
[98] S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, W. Zhou, 19
26
[122] R.Team,A.Ormazabal,C.Zheng,C.d.M.d’Autume,D.Yo- [148] N.Dey,G.Gosal,Zhiming,Chen,H.Khachane,W.Marshall,
gatama,D.Fu,D.Ong,E.Chen,E.Lamprecht,H.Pham,etal., R. Pathria, M. Tom, and J. Hestness, “Cerebras-gpt: Open
“Reka core, flash, and edge: A series of powerful multimodal compute-optimal language models trained on the cerebras
languagemodels,”arXivpreprintarXiv:2404.12387,2024. 5 wafer-scalecluster,”2023. 5
[123] B.Adler,N.Agarwal,A.Aithal,D.H.Anh,P.Bhattacharya, [149] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze,
A. Brundyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann,
et al., “Nemotron-4 340b technical report,” arXiv preprint “Bloomberggpt:Alargelanguagemodelforfinance,”2023. 5
arXiv:2406.11704,2024. 5 [150] J.Hoffmann,S.Borgeaud,A.Mensch,E.Buchatskaya,T.Cai,
[124] E.Almazrouei,H.Alobeidli,A.Alshamsi,A.Cappelli,R.Co- E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl,
jocaru, M. Debbah, Étienne Goffinet, D. Hesslow, J. Lau- A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den
nay, Q. Malartic, D. Mazzotta, B. Noune, B. Pannier, and Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan,
G.Penedo,“Thefalconseriesofopenlanguagemodels,”2023. E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, “Training
5 compute-optimallargelanguagemodels,”2022. 5
[125] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, [151] S.Shen,Y.Cheng,Z.He,W.He,H.Wu,M.Sun,andY.Liu,
J. Zhang, B. Yu, K. Lu, et al., “Qwen2. 5-coder technical “Minimumrisktrainingforneuralmachinetranslation,”2016.
report,”arXivpreprintarXiv:2409.12186,2024. 5 4,5
[126] A.Défossez,L.Mazaré,M.Orsini,A.Royer,P.Pérez,H.Jégou, [152] V.KondaandJ.Tsitsiklis,“Actor-criticalgorithms,”Advances
E.Grave,andN.Zeghidour,“Moshi:aspeech-textfoundation inneuralinformationprocessingsystems,vol.12,1999. 4
modelforreal-timedialogue,”tech.rep.,2024. 5 [153] S.Bhatnagar,M.Ghavamzadeh,M.Lee,andR.S.Sutton,“In-
[127] Nexusflow,“Athene:Anrlhf-enhancedlanguagemodel,”2024. crementalnaturalactor-criticalgorithms,”Advancesinneural
Accessed:2025-02-26. 5 informationprocessingsystems,vol.20,2007. 4
[128] R.Teknium,J.Quesnelle,andC.Guang,“Hermes3technical [154] V.Mnih,“Asynchronousmethodsfordeepreinforcementlearn-
report,”arXivpreprintarXiv:2408.11857,2024. 5 ing,”arXivpreprintarXiv:1602.01783,2016. 4,5
[129] Z.AI,“Zed,”2025. 500B,ZedAI,RLHF,Multi-modal,Open. [155] M.Babaeizadeh,I.Frosio,S.Tyree,J.Clemons,andJ.Kautz,
5 “Reinforcement learning through asynchronous advantage
[130] R.Anil,A.M.Dai,O.Firat,M.Johnson,D.Lepikhin,A.Pas- actor-criticonagpu,”arXivpreprintarXiv:1611.06256,2016.
sos,S.Shakeri,E.Taropa,P.Bailey,Z.Chen,etal.,“Palm2 4,5
technicalreport,”arXivpreprintarXiv:2305.10403,2023. 5 [156] S.M.Kakade,“Anaturalpolicygradient,”Advancesinneural
[131] Z.Cai,M.Cao,H.Chen,K.Chen,K.Chen,X.Chen,X.Chen, informationprocessingsystems,vol.14,2001. 4
Z.Chen,Z.Chen,P.Chu,etal.,“Internlm2technicalreport,” [157] A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike
arXivpreprintarXiv:2403.17297,2024. 5 adaptiveelementsthatcansolvedifficultlearningcontrolprob-
[132] S. Labs, “Supernova,” 2025. 220B, Supernova Labs, RLHF, lems,” IEEE transactions on systems, man, and cybernetics,
Multi-modal,Open. 5 no.5,pp.834–846,1983. 5
[133] xAI,“Grok-3:Thenextgenerationaimodelbyxai,”tech.rep., [158] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence
xAI,2025. Accessed:2025-02-24. 5 generativeadversarialnetswithpolicygradient,”2017. 5
[134] P.Agrawal,S.Antoniak,etal.,“Pixtral12b,”2024. 5 [159] A. Yang, B.Yang, B. Zhang, B. Hui,B. Zheng, B.Yu,C. Li,
[135] MiniMax,A.Li,etal.,“Minimax-01:Scalingfoundationmodels D.Liu,F.Huang,H.Wei,etal.,“Qwen2.5technicalreport,”
withlightningattention,”2025. 5 arXivpreprintarXiv:2412.15115,2024. 5,6
[136] A. A. G. Intelligence, “The amazon nova family of models: [160] J.Schulman,S.Levine,P.Moritz,M.I.Jordan,andP.Abbeel,
Technicalreportandmodelcard,”AmazonTechnicalReports, “Trustregionpolicyoptimization,”2017. 6,8,9
2024. 5 [161] N. Vieillard, T. Kozuno, B. Scherrer, O. Pietquin, R. Munos,
[137] FujitsuandT.I.ofTechnology,“Fugaku-llm:Thelargestcpu- and M. Geist, “Leverage the average: an analysis of kl regu-
onlytrainedlanguagemodel,”FujitsuResearchPressRelease, larization in reinforcement learning,” Advances in Neural In-
May2024. 5 formationProcessingSystems,vol.33,pp.12163–12174,2020.
[138] R.AI,“Nova:Afamilyofaimodelsbyrubik’sai,”Rubik’sAI 6
Research,October2024. 5 [162] S. Guo, B. Zhang, T. Liu, T. Liu, M. Khalman, F. Llinares,
[139] OpenAI,“Openaio3systemcard,”technicalreport,OpenAI, A. Rame, T. Mesnard, Y. Zhao, B. Piot, et al., “Direct lan-
2025. 5 guagemodelalignmentfromonlineaifeedback,”arXivpreprint
[140] M. R. Team et al., “Introducing dbrx: a new state-of-the-art arXiv:2402.04792,2024. 6,9,10
openllm,”MosaicAIResearch,2024. 5 [163] C. An, M. Zhong, Z. Wu, Q. Zhu, X. Huang, and X. Qiu,
[141] A.Köpf,Y.Kilcher,D.vonRütte,S.Anagnostidis,Z.-R.Tam, “Colo: A contrastive learning based re-ranking framework for
K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, one-stage summarization,” arXiv preprint arXiv:2209.14569,
S. ES, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire, 2022. 6
C. Schuhmann, H. Nguyen, and A. Mattick, “Openassistant [164] R. A. Bradley and M. E. Terry, “Rank analysis of incom-
conversations – democratizing large language model align- plete block designs: I. the method of paired comparisons,”
ment,”2023. 5 Biometrika,vol.39,no.3/4,pp.324–345,1952. 6
[142] T.GLM,A.Zeng,B.Xu,B.Wang,C.Zhang,D.Yin,D.Zhang, [165] R.L.Plackett,“Theanalysisofpermutations,”Journalofthe
D. Rojas, G. Feng, H. Zhao, et al., “Chatglm: A family of Royal Statistical Society Series C: Applied Statistics, vol. 24,
largelanguagemodelsfromglm-130btoglm-4alltools,”arXiv no.2,pp.193–202,1975. 6
preprintarXiv:2406.12793,2024. 5 [166] A.Setlur,C.Nagpal,A.Fisch,X.Geng,J.Eisenstein,R.Agar-
[143] A. Bartolome, J. Hong, N. Lee, K. Rasul, and L. Tunstall, wal, A. Agarwal, J. Berant, and A. Kumar, “Rewarding
“Zephyr141ba39b,”2024. 5 progress:Scalingautomatedprocessverifiersforllmreasoning,”
[144] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1: arXivpreprintarXiv:2410.08146,2024. 7,20
Technical details and evaluation,” White Paper. AI21 Labs, [167] N. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin,
vol.1,no.9,pp.1–17,2021. 5 K.Chandu,N.Dziri,S.Kumar,T.Zick,Y.Choi,etal.,“Re-
[145] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, wardbench:Evaluatingrewardmodelsforlanguagemodeling,”
C.Xiao,C.Du,C.Liao,etal.,“Kimik1.5:Scalingreinforce- arXivpreprintarXiv:2403.13787,2024. 7,19,20
ment learning with llms,” arXiv preprint arXiv:2501.12599, [168] J.Hong,N.Lee,andJ.Thorne,“Orpo:Monolithicpreference
2025. 5,22 optimizationwithoutreferencemodel,”2024. 8
[146] M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gu- [169] J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.Abbeel,
nasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauff- “High-dimensionalcontinuouscontrolusinggeneralizedadvan-
mann, J. R. Lee, Y. T. Lee, Y. Li, W. Liu, C. C. T. Mendes, tageestimation,”2018. 8
A. Nguyen, E. Price, G. de Rosa, O. Saarikivi, A. Salim, [170] R. Rafailov, Y. Chittepu, R. Park, H. S. Sikchi, J. Hejna,
S. Shah, X. Wang, R. Ward, Y. Wu, D. Yu, C. Zhang, and B. Knox, C. Finn, and S. Niekum, “Scaling laws for re-
Y.Zhang,“Phi-4technicalreport,”2024. 5 wardmodeloveroptimizationindirectalignmentalgorithms,”
[147] C. Team, “Chameleon: Mixed-modal early-fusion foundation ArXiv,vol.abs/2406.02900,2024. 9
models,”arXivpreprintarXiv:2405.09818,2024. 5 [171] H. Wang, S. Hao, H. Dong, S. Zhang, Y. Bao, Z. Yang, and
27
Y.Wu,“Offlinereinforcementlearningforllmmulti-steprea- “Llm.int8(): 8-bit matrix multiplication for transformers at
soning,”2024. 10,22 scale,”arXivpreprintarXiv:2208.07339,2022. 13
[172] G.Mukobi,P.Chatain,S.Fong,R.Windesheim,G.Kutyniok, [193] Q.Zhang,M.Chen,A.Bukharin,P.He,Y.Cheng,W.Chen,
K.Bhatia,andS.Alberti,“Superhf:Supervisediterativelearn- and T. Zhao, “Adaptive budget allocation for parameter-
ing from human feedback,” arXiv preprint arXiv:2310.16763, efficientfine-tuning,”inTheEleventhInternationalConference
2023. 11 onLearningRepresentations,2023. 13,20
[173] C. Li, H. Zhou, G. Glavaš, A. Korhonen, and I. Vulić, [194] X.Liu,K.Ji,Y.Fu,Z.Du,Z.Yang,andJ.Tang,“P-tuningv2:
“On task performance and model calibration with super- Prompt tuning can be comparable to fine-tuning universally
vised and self-ensembled in-context learning,” arXiv preprint acrossscalesandtasks,”CoRR,vol.abs/2110.07602,2021. 13
arXiv:2312.13772,2023. 11 [195] Q.Lhoest,A.VillanovadelMoral,Y.Jernite,A.Thakur,P.von
[174] C. Wang, Z. Zhao, C. Zhu, K. A. Sankararaman, M. Valko, Platen,S.Patil,J.Chaumond,M.Drame,J.Plu,L.Tunstall,
X. Cao, Z. Chen, M. Khabsa, Y. Chen, H. Ma, and S. Wang, J. Davison, M. Šaško, G. Chhablani, B. Malik, S. Brandeis,
“Preference optimization with multi-sample comparisons,” T. Le Scao, V. Sanh, C. Xu, N. Patry, A. McMillan-Major,
2024. 11 P. Schmid, S. Gugger, C. Delangue, T. Matussière, L. Debut,
[175] M.Suzgun,N.Scales,N.Schärli,S.Gehrmann,Y.Tay,H.W. S.Bekman,P.Cistac,T.Goehringer,V.Mustar,F.Lagunas,
Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, , and A.Rush,andT.Wolf,“Datasets:Acommunitylibraryfornatu-
J. Wei, “Challenging big-bench tasks and whether chain-of- rallanguageprocessing,”inProceedingsofthe2021Conference
thought can solve them,” arXiv preprint arXiv:2210.09261, onEmpiricalMethodsinNaturalLanguageProcessing:System
2022. 11,19 Demonstrations,(OnlineandPuntaCana,DominicanRepub-
[176] X. Zhang, C. Du, T. Pang, Q. Liu, W. Gao, and M. Lin, lic), pp. 175–184, Association for Computational Linguistics,
“Chainofpreferenceoptimization:Improvingchain-of-thought Nov.2021. 13
reasoninginLLMs,”inTheThirty-eighthAnnualConference [196] A.TorralbaandOthers,“Webdataset:Aformatforpetascale
onNeuralInformationProcessingSystems,2024. 11 deeplearning.”Efficienttar-basedshardingformatforpetascale
[177] Z.Zhang,A.Zhang,M.Li,andA.Smola,“Automaticchainof distributedtraining. 13
thought prompting in large language models,” arXiv preprint [197] I.Iterative,“Dvc:Dataversioncontrol.”Git-likeversioncontrol
arXiv:2210.03493,2022. 11 fordatasetsandmachinelearningpipelines. 13
[178] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, [198] N. Richardson, I. Cook, N. Crane, D. Dunnington,
Z.Alyafeai,A.Chaffin,A.Stiegler,T.L.Scao,A.Raja,etal., R. François, J. Keane, D. Moldovan-Grünfeld, J. Ooms,
“Multitask prompted training enables zero-shot task general- J. Wujciak-Jens, and Apache Arrow, arrow: Integration
ization,”arXivpreprintarXiv:2110.08207,2021. 12,22 to ’Apache’ ’Arrow’, 2025. R package version 19.0.0,
[179] J.Wei,M.Bosma,V.Y.Zhao,K.Guu,A.W.Yu,B.Lester, https://arrow.apache.org/docs/r/. 13
N.Du,A.M.Dai,andQ.V.Le,“Finetunedlanguagemodels [199] I. Facebook, “Zstandard: High-speed compression algorithm.”
arezero-shotlearners,”arXivpreprintarXiv:2109.01652,2021. High-speed compression algorithm for training data storage/-
12 transfer. 13
[180] R.Taori,I.Gulrajani,T.Zhang,Y.Dubois,X.Li,C.Guestrin, [200] C.Team,“Cleanlab:Thestandarddata-centricaipackagefor
P. Liang, and T. B. Hashimoto, “Stanford alpaca: An machine learning with noisy labels.” Automatic detection of
instruction-followingllamamodel,”2023. 12 labelerrorsandoutliersintrainingdatasets. 13
[181] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, [201] R. Y. Aminabadi, S. Rajbhandari, M. Zhang, A. A. Awan,
L.Zheng,S.Zhuang,Y.Zhuang,J.E.Gonzalez,I.Stoica,and C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, and
E.P.Xing,“Vicuna:Anopen-sourcechatbotimpressinggpt-4 Y. He, “Deepspeed inference: Enabling efficient inference of
with90%*chatgptquality,”March2023. 12 transformermodelsatunprecedentedscale,”2022. 13
[182] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, [202] M.Shoeybi,M.Patwary,R.Puri,P.LeGresley,J.Casper,and
A. Ghodsi, P. Wendell, M. Zaharia, and R. Xin, “Free dolly: B.Catanzaro,“Megatron-lm:Trainingmulti-billionparameter
Introducingtheworld’sfirsttrulyopeninstruction-tunedllm,” languagemodelsusingmodelparallelism,”2020. 13
2023. 12 [203] S. Li, H. Liu, Z. Bian, J. Fang, H. Huang, Y. Liu, B. Wang,
[183] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul- and Y. You, “Colossal-ai: A unified deep learning system for
shreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, large-scaleparalleltraining,”2023. 13
etal.,“Lamda:Languagemodelsfordialogapplications,”arXiv [204] A. Sergeev and M. D. Balso, “Horovod: fast and easy dis-
preprintarXiv:2201.08239,2022. 12 tributeddeeplearningintensorflow,”2018. 13
[184] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, [205] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw,
S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency E. Liang, M. Elibol, Z. Yang, W. Paul, M. I. Jordan, and
improves chain of thought reasoning in language models,” in I. Stoica, “Ray: A distributed framework for emerging ai ap-
TheEleventhInternationalConferenceonLearningRepresen- plications,”2018. 13
tations,2023. 12,15,20 [206] W.Kwon,Z.Li,S.Zhuang,Y.Sheng,L.Zheng,C.H.Yu,J.E.
[185] L. C. Magister, J. Mallinson, J. Adamek, E. Malmi, and Gonzalez,H.Zhang,andI.Stoica,“Efficientmemorymanage-
A.Severyn,“Teachingsmalllanguagemodelstoreason,”arXiv ment for large language model serving with pagedattention,”
preprintarXiv:2212.08410,2022. 12,13 2023. 13
[186] G.Xu,P.Jin,H.Li,Y.Song,L.Sun,andL.Yuan,“Llava-cot: [207] Y. Zhou and K. Yang, “Exploring tensorrt to improve real-
Letvisionlanguagemodelsreasonstep-by-step,”2024. 12 timeinferencefordeeplearning,”in2022IEEE24thIntConf
[187] O.Thawakar,D.Dissanayake,K.More,R.Thawkar,A.Heakl, onHighPerformanceComputing&Communications;8thInt
N. Ahsan, Y. Li, M. Zumri, J. Lahoud, R. M. Anwer, ConfonDataScience&Systems;20thIntConfonSmartCity;
H. Cholakkal, I. Laptev, M. Shah, F. S. Khan, and S. Khan, 8th Int Conf on Dependability in Sensor, Cloud & Big Data
“Llamav-o1:Rethinkingstep-by-stepvisualreasoninginllms,” Systems & Application (HPCC/DSS/SmartCity/DependSys),
2025. 12,19 pp.2011–2018,2022. 13
[188] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, [208] P.Tillet,H.-T.Kung,andD.Cox,“Triton:anintermediatelan-
“Qlora:Efficientfinetuningofquantizedllms,”arXivpreprint guageandcompilerfortiledneuralnetworkcomputations,”in
arXiv:2305.14314,2023. 13,14 Proceedingsofthe3rdACMSIGPLANInternationalWorkshop
[189] E.Frantar,S.Ashkboos,T.Hoefler,andD.Alistarh,“GPTQ: onMachineLearningandProgrammingLanguages,pp.10–19,
Accurate post-training compression for generative pretrained 2019. 13
transformers,”arXivpreprintarXiv:2210.17323,2022. 13 [209] O.Community,“Onnx:Openneuralnetworkexchange.” Uni-
[190] E. Frantar and D. Alistarh, “SparseGPT: Massive language fiedinferenceenginewithhardware-specificoptimizations. 13
modelscanbeaccuratelyprunedinone-shot,”arXivpreprint [210] I. Corporation, “Openvino: Intel optimization toolkit,” 2025.
arXiv:2301.00774,2023. 13,14 Runtime for Intel CPUs/iGPUs with pruning/quantization
[191] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul, support. 13
andB.Bossan,“Peft:State-of-the-artparameter-efficientfine- [211] M.Dukhan,“Theindirectconvolutionalgorithm,”2019. 13
tuningmethods,”2022. 13 [212] I. Groq, “Groq: Ai accelerator,” 2025. Deterministic low-
[192] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, latencyinferenceviacustomtensorstreamingprocessor. 13
28
[213] J.Castaño,S.Martínez-Fernández,X.Franch,andJ.Bogner, America,vol.59,no.S1,pp.S97–S97,1976. 14
“Analyzing the evolution and maintenance of ml models on [234] A.Graves,“Sequencetransductionwithrecurrentneuralnet-
huggingface,”2024. 13 works,”arXivpreprintarXiv:1211.3711,2012. 14
[214] A.Paszke,S.Gross,S.Chintala,G.Chanan,E.Yang,Z.De- [235] H. Sun, M. Haider, R. Zhang, H. Yang, J. Qiu, M. Yin,
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Auto- M.Wang,P.Bartlett,andA.Zanette,“Fastbest-of-ndecoding
maticdifferentiationinpytorch,”inNIPS-W,2017. 13 viaspeculativerejection,”2024. 14
[215] S.Hao,Y.Gu,H.Luo,T.Liu,X.Shao,X.Wang,S.Xie,H.Ma, [236] A.Askell,Y.Bai,A.Chen,D.Drain,D.Ganguli,T.Henighan,
A.Samavedhi,Q.Gao,etal.,“Llmreasoners:Newevaluation, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al., “A gen-
library, and analysis of step-by-step reasoning with large lan- eral language assistant as a laboratory for alignment,” arXiv
guagemodels,”arXivpreprintarXiv:2404.05221,2024. 13 preprintarXiv:2112.00861,2021. 14
[216] S.Pieri,S.S.Mullappilly,F.S.Khan,R.M.Anwer,S.Khan, [237] A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu,
T. Baldwin, and H. Cholakkal, “Bimedix: Bilingual medical T.Ewalds,M.Rauh,L.Weidinger,M.Chadwick,P.Thacker,
mixture of experts llm,” arXiv preprint arXiv:2402.13253, et al., “Improving alignment of dialogue agents via targeted
2024. 13 human judgements,” arXiv preprint arXiv:2209.14375, 2022.
[217] Y. Yang, M. C. S. Uy, and A. Huang, “Finbert: A pretrained 14
languagemodelforfinancialcommunications,”arXivpreprint [238] N.Stiennon,L.Ouyang,J.Wu,D.Ziegler,R.Lowe,C.Voss,
arXiv:2006.08097,2020. 13 A. Radford, D. Amodei, and P. F. Christiano, “Learning to
[218] D. Thulke, Y. Gao, P. Pelser, R. Brune, R. Jalota, F. Fok, summarize with human feedback,” Advances in Neural Infor-
M. Ramos, I. van Wyk, A. Nasir, H. Goldstein, et al., “Cli- mationProcessingSystems,vol.33,pp.3008–3021,2020. 14
mategpt:Towardsaisynthesizinginterdisciplinaryresearchon [239] J. Q. Yang, S. Salamatian, Z. Sun, A. T. Suresh, and
climatechange,”arXivpreprintarXiv:2401.09646,2024. 13 A.Beirami,“Asymptoticsoflanguagemodelalignment,”arXiv
[219] S.S.Mullappilly,A.Shaker,O.Thawakar,H.Cholakkal,R.M. preprintarXiv:2404.01730,2024. 15
Anwer,S.Khan,andF.S.Khan,“Arabicmini-climategpt:A [240] H. Sun, M. Haider, R. Zhang, H. Yang, J. Qiu, M. Yin,
climate change and sustainability tailored arabic llm,” arXiv M.Wang,P.Bartlett,andA.Zanette,“Fastbest-of-ndecoding
preprintarXiv:2312.09366,2023. 13 via speculative rejection,” arXiv preprint arXiv:2410.20290,
[220] Y.Wang,W.Wang,S.Joty,andS.C.Hoi,“Codet5:Identifier- 2024. 15
awareunifiedpre-trainedencoder-decodermodelsforcodeun- [241] J. Hilton and L. Gao, “Measuring goodhart’s law,” OpenAI
derstandingandgeneration,”arXivpreprintarXiv:2109.00859, ResearchBlog,2022. 15
2021. 13 [242] L.Wang,W.Xu,Y.Lan,Z.Hu,Y.Lan,R.K.-W.Lee,andE.-
[221] K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and P.Lim,“Plan-and-solveprompting:Improvingzero-shotchain-
F.S.Khan,“Geochat:Groundedlargevision-languagemodel of-thoughtreasoningbylargelanguagemodels,”arXivpreprint
forremotesensing,”inProceedingsoftheIEEE/CVFConfer- arXiv:2305.04091,2023. 15
enceonComputerVisionandPatternRecognition,pp.27831– [243] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith,
27840,2024. 13 D. Khashabi, and H. Hajishirzi, “Self-instruct: Aligning lan-
[222] S. S. Mullappilly, M. I. Kurpath, S. Pieri, S. Y. Alseiari, guagemodelswithself-generatedinstructions,”arXivpreprint
S. Cholakkal, K. Aldahmani, F. Khan, R. Anwer, S. Khan, arXiv:2212.10560,2022. 15
T. Baldwin, et al., “Bimedix2: Bio-medical expert lmm for [244] X. Chen, R. Aksitov, U. Alon, J. Ren, K. Xiao, P. Yin,
diversemedicalmodalities,”arXivpreprintarXiv:2412.07769, S.Prakash,C.Sutton,X.Wang,andD.Zhou,“Universalself-
2024. 13 consistency for large language models,” in ICML 2024 Work-
[223] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, “Video- shoponIn-ContextLearning. 15
chatgpt:Towardsdetailedvideounderstandingvialargevision [245] A. Newell, “On theanalysisof human problem solving proto-
andlanguagemodels,”arXivpreprintarXiv:2306.05424,2023. cols,”1966. 16
13 [246] F. Haji, M. Bethany, M. Tabar, J. Chiang, A. Rios, and
[224] B. Lin, Y. Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan, P.Najafirad,“Improvingllmreasoningwithmulti-agenttree-
“Video-llava: Learning united visual representation by align- of-thoughtvalidatoragent,”arXivpreprintarXiv:2409.11527,
ment before projection,” arXiv preprint arXiv:2311.10122, 2024. 16
2023. 13 [247] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Pod-
[225] H. Zhang, X. Li, and L. Bing, “Video-llama: An instruction- stawski,L.Gianinazzi,J.Gajda,T.Lehmann,H.Niewiadom-
tuned audio-visual language model for video understanding,” ski, P. Nyczyk, and T. Hoefler, “Graph of thoughts: Solving
arXivpreprintarXiv:2306.02858,2023. 13 elaborate problems with large language models,” Proceedings
[226] Y.Han,C.Zhang,X.Chen,X.Yang,Z.Wang,G.Yu,B.Fu, of the AAAI Conference on Artificial Intelligence, vol. 38,
and H. Zhang, “Chartllama: A multimodal llm for chart un- p.17682–17690,Mar.2024. 16
derstandingandgeneration,”arXivpreprintarXiv:2311.16483, [248] D. Wilson, “Llm tree search,” arXiv preprint
2023. 13 arXiv:2410.19117,2024. 17
[227] X.Zhu,J.Li,Y.Liu,C.Ma,andW.Wang,“Asurveyonmodel [249] D. Hendrycks and K. Gimpel, “A baseline for detecting mis-
compression for large language models,” Transactions of the classifiedandout-of-distributionexamplesinneuralnetworks,”
Association for Computational Linguistics, vol. 12, pp. 1556– arXivpreprintarXiv:1610.02136,2016. 17
1577,2024. 13 [250] G.PortilloWightman,A.DeLucia,andM.Dredze,“Strength
[228] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, J. Liu, Z. Qu, in numbers: Estimating confidence of large language models
S. Yan, Y. Zhu, Q. Zhang, et al., “Efficient large language bypromptagreement,”inProceedingsofthe3rdWorkshopon
models:Asurvey,”arXivpreprintarXiv:2312.03863,2023. 13 Trustworthy Natural Language Processing (TrustNLP 2023),
[229] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, pp.326–362,2023. 17
A. Ratner, R. Krishna, C.-Y. Lee, and T. Pfister, “Distill- [251] J. Qi, H. Tang, and Z. Zhu, “Verifierq: Enhancing llm test
ing step-by-step! outperforming larger language models with time compute with q-learning-based verifiers,” arXiv preprint
less training data and smaller model sizes,” arXiv preprint arXiv:2410.08048,2024. 17
arXiv:2305.02301,2023. 13 [252] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,
[230] Y. Gu, L. Dong, F. Wei, and M. Huang, “Minillm: Knowl- S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,
edge distillation of large language models,” arXiv preprint S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yaz-
arXiv:2306.08543,2023. 13 danbakhsh,andP.Clark,“Self-refine:Iterativerefinementwith
[231] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continu- self-feedback,”2023. 17
ouspromptsforgeneration,”arXivpreprintarXiv:2101.00190, [253] Z.Xi,W.Chen,X.Guo,W.He,Y.Ding,B.Hong,M.Zhang,
2021. 13 J. Wang, S. Jin, E. Zhou, et al., “The rise and potential of
[232] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, largelanguagemodelbasedagents:Asurvey,”arXivpreprint
Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, arXiv:2309.07864,2023. 17
“Parameter-efficienttransferlearningfornlp,”2019. 13 [254] R. Coulom, “Efficient selectivity and backup operators in
[233] B. P. Lowerre and B. R. Reddy, “Harpy, a connected speech monte-carlotreesearch,”inInternationalconferenceoncom-
recognition system,” The Journal of the Acoustical Society of putersandgames,pp.72–83,Springer,2006. 18
29
[255] J.X.Chen,“Theevolutionofcomputing:Alphago,”Comput- R.Sun,M.Yin,B.Zheng,Z.Yang,Y.Liu,W.Huang,H.Sun,
inginScience&Engineering,vol.18,no.4,pp.4–7,2016. 18 Y. Su, and W. Chen, “Mmmu: A massive multi-discipline
[256] L.KocsisandC.Szepesvári,“Banditbasedmonte-carloplan- multimodalunderstandingandreasoningbenchmarkforexpert
ning,” in European conference on machine learning, pp. 282– agi,”inProceedingsofCVPR,2024. 19
293,Springer,2006. 18 [275] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measur-
[257] H.W.Sprueill,C.Edwards,M.V.Olarte,U.Sanyal,H.Ji,and ing how models mimic human falsehoods,” arXiv preprint
S. Choudhury, “Monte carlo thought search: Large language arXiv:2109.07958,2021. 19
model querying for complex scientific reasoning in catalyst [276] X. Yue et al., “Mammoth: Building math generalist mod-
design,”arXivpreprintarXiv:2310.14420,2023. 18,20 els through hybrid instruction tuning,” arXiv preprint
[258] M. DeLorenzo, A. B. Chowdhury, V. Gohil, S. Thakur, arXiv:2309.05653,2023. 19
R.Karri,S.Garg,andJ.Rajendran,“Makeeverymovecount: [277] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,
Llm-basedhigh-qualityrtlcodegenerationusingmcts,”arXiv D.Song,andJ.Steinhardt,“Measuringmassivemultitasklan-
preprintarXiv:2402.03289,2024. 18 guage understanding,” Proceedings of the International Con-
[259] S. Park, X. Liu, Y. Gong, and E. Choi, “Ensembling large ferenceonLearningRepresentations(ICLR),2021. 19,20
language models with process reward-guided tree search for [278] D.Hendrycks,C.Burns,S.Basart,A.Critch,J.Li,D.Song,
better complex reasoning,” arXiv preprint arXiv:2412.15797, and J. Steinhardt, “Aligning ai with shared human values,”
2024. 18 ProceedingsoftheInternationalConferenceonLearningRep-
[260] M. Shen, G. Zeng, Z. Qi, Z.-W. Hong, Z. Chen, W. Lu, resentations(ICLR),2021. 19,20
G.Wornell,S.Das,D.Cox,andC.Gan,“Satori:Reinforcement [279] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and
learning with chain-of-action-thought enhances llm reasoning M. Gardner, “DROP: A reading comprehension benchmark
via autoregressive search,” arXiv preprint arXiv:2502.02508, requiring discrete reasoning over paragraphs,” in Proc. of
2025. 18 NAACL,2019. 19
[261] S. R. Motwani, C. Smith, R. J. Das, R. Rafailov, I. Laptev, [280] Z. Wang, Y. Dong, J. Zeng, V. Adams, M. N. Sreedhar,
P.H.S.Torr,F.Pizzati,R.Clark,andC.S.deWitt,“Malt: D. Egert, O. Delalleau, J. P. Scowcroft, N. Kant, A. Swope,
Improvingreasoningwithmulti-agentllmtraining,”2025. 18 et al., “Helpsteer: Multi-attribute helpfulness dataset for
[262] U.Anwar,A.Saparov,J.Rando,D.Paleka,M.Turpin,P.Hase, steerlm,”arXivpreprintarXiv:2311.09528,2023. 19
E.S.Lubana,E.Jenner,S.Casper,O.Sourbut,etal.,“Foun- [281] G.Cui,L.Yuan,N.Ding,G.Yao,W.Zhu,Y.Ni,G.Xie,Z.Liu,
dational challenges in assuring alignment and safety of large and M. Sun, “Ultrafeedback: Boosting language models with
languagemodels,”arXivpreprintarXiv:2404.09932,2024. 18 high-qualityfeedback,”2023. 19
[263] W.Zhang,P.H.Torr,M.Elhoseiny,andA.Bibi,“Bi-factorial [282] J.Fu,A.Kumar,O.Nachum,G.Tucker,andS.Levine,“D4rl:
preference optimization: Balancing safety-helpfulness in lan- Datasets for deep data-driven reinforcement learning,” 2020.
guagemodels,”arXivpreprintarXiv:2408.15313,2024. 18 19,20
[264] C. Li, W. Wu, H. Zhang, Y. Xia, S. Mao, L. Dong, I. Vulić, [283] T. Schmied, M. Hofmarcher, F. Paischer, R. Pascanu, and
and F. Wei, “Imagine while reasoning in space: Multimodal S.Hochreiter,“Learningtomodulatepre-trainedmodelsinrl,”
visualization-of-thought,” arXiv preprint arXiv:2501.07542, Advances in Neural Information Processing Systems, vol. 36,
2025. 19 2024. 19,20
[265] F. Nowak, A. Svete, A. Butoi, and R. Cotterell, “On the [284] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel,
representationalcapacityofneurallanguagemodelswithchain- M.Veloso,andR.Salakhutdinov,“Minerl:Alarge-scaledataset
of-thoughtreasoning,”arXivpreprintarXiv:2406.14197,2024. ofminecraftdemonstrations,”2019. 19,20
19 [285] T. Nguyen, C. V. Nguyen, V. D. Lai, H. Man, N. T. Ngo,
[266] Z. Li, H. Liu, D. Zhou, and T. Ma, “Chain of thought em- F. Dernoncourt, R. A. Rossi, and T. H. Nguyen, “CulturaX:
powerstransformerstosolveinherentlyserialproblems,”arXiv A cleaned, enormous, and multilingual dataset for large lan-
preprintarXiv:2402.12875,vol.1,2024. 19 guage models in 167 languages,” in Proceedings of the 2024
[267] W. Merrill and A. Sabharwal, “The expressive power JointInternationalConferenceonComputationalLinguistics,
of transformers with chain of thought,” arXiv preprint Language Resources and Evaluation (LREC-COLING 2024)
arXiv:2310.07923,2023. 19 (N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, and
[268] C. V. Snell, J. Lee, K. Xu, and A. Kumar, “Scaling test- N.Xue,eds.),(Torino,Italia),pp.4226–4237,ELRAandICCL,
timecomputeoptimallycanbemoreeffectivethanscalingllm May2024. 19,20
parameters,” in The Thirteenth International Conference on [286] X. Yue, Y. Song, A. Asai, S. Kim, J. de Dieu Nyandwi,
LearningRepresentations. 19 S. Khanuja, A. Kantharuban, L. Sutawika, S. Ramamoorthy,
[269] Saxton et al., “Analysing mathematical reasoning abilities of andG.Neubig,“Pangea:Afullyopenmultilingualmultimodal
neuralmodels,”arXiv:1904.01557,2019. 19 llmfor39languages,”arXivpreprintarXiv:2410.16153,2024.
[270] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, 19,20
L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, [287] J.Devlin,M.Chang,K.Lee,andK.Toutanova,“BERT:pre-
C. Hesse, and J. Schulman, “Training verifiers to solve math trainingofdeepbidirectionaltransformersforlanguageunder-
wordproblems,”arXivpreprintarXiv:2110.14168,2021. 19 standing,”CoRR,vol.abs/1810.04805,2018. 19,20
[271] L.Yu,W.Jiang,H.Shi,J.Yu,Z.Liu,Y.Zhang,J.T.Kwok, [288] Y.Liang,N.Duan,Y.Gong,N.Wu,F.Guo,W.Qi,M.Gong,
Z. Li, A. Weller, and W. Liu, “Metamath: Bootstrap your L. Shou, D. Jiang, G. Cao, X. Fan, R. Zhang, R. Agrawal,
ownmathematicalquestionsforlargelanguagemodels,”arXiv E.Cui,S.Wei,T.Bharti,Y.Qiao,J.-H.Chen,W.Wu,S.Liu,
preprintarXiv:2309.12284,2023. 19 F. Yang, D. Campos, R. Majumder, and M. Zhou, “Xglue: A
[272] Z. Xie, S. Thiem, J. Martin, E. Wainwright, S. Marmorstein, new benchmark dataset for cross-lingual pre-training, under-
and P. Jansen, “WorldTree v2: A corpus of science-domain standingandgeneration,”arXiv,vol.abs/2004.01401,2020.19
structured explanations and inference patterns supporting [289] G.Son,D.Yoon,J.Suk,J.Aula-Blasco,M.Aslan,V.T.Kim,
multi-hop inference,” in Proceedings of the Twelfth Language S.B.Islam,J.Prats-Cristià,L.Tormo-Bañuelos,andS.Kim,
ResourcesandEvaluationConference(N.Calzolari,F.Béchet, “Mm-eval:Amultilingualmeta-evaluationbenchmarkforllm-
P.Blache,K.Choukri,C.Cieri,T.Declerck,S.Goggi,H.Isa- as-a-judgeandrewardmodels,”2024. 19,20
hara,B.Maegaard,J.Mariani,H.Mazo,A.Moreno,J.Odijk, [290] S.et.al,“Beyondtheimitationgame:Quantifyingandextrap-
and S. Piperidis, eds.), (Marseille, France), pp. 5456–5473, olatingthecapabilitiesoflanguagemodels,”2022. 19,20
EuropeanLanguageResourcesAssociation,May2020. 17,19 [291] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu,
[273] F. Liu, E. Bugliarello, E. M. Ponti, S. Reddy, N. Collier, and Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al., “Judging
D.Elliott,“Visuallygroundedreasoningacrosslanguagesand llm-as-a-judge with mt-bench and chatbot arena,” Advances
cultures,”inProceedingsofthe2021ConferenceonEmpirical inNeuralInformationProcessingSystems,vol.36,pp.46595–
Methods in Natural Language Processing, (Online and Punta 46623,2023. 19,20
Cana,DominicanRepublic),pp.10467–10485,Associationfor [292] E.Dinan,V.Logacheva,V.Malykh,A.H.Miller,K.Shuster,
ComputationalLinguistics,Nov.2021. 19 J. Urbanek, D. Kiela, A. Szlam, I. Serban, R. Lowe, S. Prab-
[274] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, humoye,A.W.Black,A.I.Rudnicky,J.Williams,J.Pineau,
S.Stevens,D.Jiang,W.Ren,Y.Sun,C.Wei,B.Yu,R.Yuan, M. S. Burtsev, and J. Weston, “The second conversational
30
intelligence challenge (convai2),” CoRR, vol. abs/1902.00098, 2025. 20
2019. 19,20 [311] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I.
[293] M. Eric, R. Goel, S. Paul, A. Sethi, S. Agarwal, S. Gao, Cowling,P.Rohlfshagen,S.Tavener,D.Perez,S.Samothrakis,
and D. Hakkani-Tur, “MultiWOZ 2.1: A consolidated multi- andS.Colton,“Asurveyofmontecarlotreesearchmethods,”
domaindialoguedatasetwithstatecorrectionsandstatetrack- IEEE Transactions on Computational Intelligence and AI in
ingbaselines,”inProceedingsofthe12thLanguageResources games,vol.4,no.1,pp.1–43,2012. 20
and Evaluation Conference, (Marseille, France), pp. 422–428, [312] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,
EuropeanLanguageResourcesAssociation,May2020. 19,20 S.Wiegreffe,U.Alon,N.Dziri,S.Prabhumoye,Y.Yang,etal.,
[294] X.LiandD.Roth,“Learningquestionclassifiers,”inCOLING “Self-refine:Iterativerefinementwithself-feedback,”Advances
2002: The 19th International Conference on Computational inNeuralInformationProcessingSystems,vol.36,2024. 20
Linguistics,2002. 19,20 [313] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot,
[295] E.Hovy,L.Gerber,U.Hermjakob,C.-Y.Lin,andD.Ravichan- “Complexity-based prompting for multi-step reasoning,” in
dran, “Toward semantics-based answer pinpointing,” in Pro- TheEleventhInternationalConferenceonLearningRepresen-
ceedingsoftheFirstInternationalConferenceonHumanLan- tations,2022. 20
guageTechnologyResearch,2001. 19,20 [314] B. Johnson, “Metacognition for artificial intelligence system
[296] N. Thakur, N. Reimers, A. Rücklé, A. Srivastava, and safety–an approach to safe and desired behavior,” Safety Sci-
I. Gurevych, “BEIR: A heterogeneous benchmark for zero- ence,vol.151,p.105743,2022. 20,21
shot evaluation of information retrieval models,” in Thirty- [315] OpenAI,“Earlyaccessforsafetytesting,”2024. 20
fifth Conference on Neural Information Processing Systems [316] Y. Yan, X. Lou, J. Li, Y. Zhang, J. Xie, C. Yu, Y. Wang,
DatasetsandBenchmarksTrack(Round2),2021. 19,20 D. Yan, and Y. Shen, “Reward-robust rlhf in llms,” ArXiv,
[297] C.Chhun,F.M.Suchanek,andC.Clavel,“Dolanguagemodels vol.abs/2409.15360,2024. 20
enjoytheirownstories?Promptinglargelanguagemodelsfor [317] W. Yu, Z. Sun, J. Xu, Z. Dong, X. Chen, H. Xu, and J.-
automatic story evaluation,” Transactions of the Association R.Wen,“Explainablelegalcasematchingviainverseoptimal
forComputationalLinguistics,vol.12,pp.1122–1142,2024.19 transport-based rationale extraction,” in Proceedings of the
[298] H.Chen,D.M.Vo,H.Takamura,Y.Miyao,andH.Nakayama, 45th International ACM SIGIR Conference on Research and
“Storyer: Automatic story evaluation via ranking, rating and DevelopmentinInformationRetrieval,pp.657–668,2022. 20
reasoning,”2022. 19 [318] A. Amini, S. Gabriel, P. Lin, R. Koncel-Kedziorski, Y. Choi,
[299] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen, andH.Hajishirzi,“Mathqa:Towardsinterpretablemathword
R.Sun,Y.Wang,andY.Yang,“Beavertails:Towardsimproved problemsolvingwithoperation-basedformalisms,”2019. 20
safety alignment of llm via a human-preference dataset,” Ad- [319] L.Chen,O.Sinavski,J.Hünermann,A.Karnsund,A.J.Will-
vancesinNeuralInformationProcessingSystems,vol.36,2024. mott,D.Birch,D.Maund,andJ.Shotton,“Drivingwithllms:
19,20 Fusingobject-levelvectormodalityforexplainableautonomous
[300] G. Xu, J. Liu, M. Yan, H. Xu, J. Si, Z. Zhou, P. Yi, X. Gao, driving,”2024IEEEInternationalConferenceonRoboticsand
J. Sang, R. Zhang, et al., “Cvalues: Measuring the values of Automation(ICRA),pp.14093–14100,2023. 20
chinese large language models from safety to responsibility,” [320] R.Poulain,H.Fayyaz,andR.Beheshti,“Biaspatternsinthe
arXivpreprintarXiv:2307.09705,2023. 19 applicationofllmsforclinicaldecisionsupport:Acomprehen-
[301] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross- sivestudy,”arXivpreprintarXiv:2404.15149,2024. 20
taskgeneralizationvianaturallanguagecrowdsourcinginstruc- [321] Z. Fan, R. Chen, R. Xu, and Z. Liu, “Biasalert: A plug-and-
tions,”inACL,2022. 19 play tool for social bias detection in llms,” arXiv preprint
[302] Y.Wang,S.Mishra,P.Alipoormolabashi,Y.Kordi,A.Mirzaei, arXiv:2407.10241,2024. 20
A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, [322] M. Li, T. Shi, C. Ziems, M.-Y. Kan, N. F. Chen, Z. Liu, and
D. Stap, et al., “Super-naturalinstructions:generalization via D. Yang, “Coannotating: Uncertainty-guided work allocation
declarativeinstructionson1600+tasks,”inEMNLP,2022. 19 between human and large language models for data annota-
[303] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, tion,”arXivpreprintarXiv:2310.15638,2023. 20
P.Mishkin,C.Zhang,S.Agarwal,K.Slama,A.Ray,J.Schul- [323] A. Elangovan, J. Ko, L. Xu, M. Elyasi, L. Liu, S. Bodapati,
man, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, and D. Roth, “Beyond correlation: The impact of human un-
P.Welinder,P.F.Christiano,J.Leike,andR.Lowe,“Training certaintyinmeasuringtheeffectivenessofautomaticevaluation
languagemodelstofollowinstructionswithhumanfeedback,” andllm-as-a-judge,”arXivpreprintarXiv:2410.03775,2024.20
in Advances in Neural Information Processing Systems 35: [324] Y.R.Dong,T.Hu,andN.Collier,“Canllmbeapersonalized
AnnualConferenceonNeuralInformationProcessingSystems judge?,”arXivpreprintarXiv:2406.11657,2024. 20
2022, NeurIPS 2022, New Orleans, LA, USA, November 28 [325] D. Wang, K. Yang, H. Zhu, X. Yang, A. Cohen, L. Li,
- December 9, 2022 (S. Koyejo, S. Mohamed, A. Agarwal, and Y. Tian, “Learning personalized story evaluation,” arXiv
D.Belgrave,K.Cho,andA.Oh,eds.),2022. 20 preprintarXiv:2310.03304,2023. 20
[304] W.Saunders,C.Yeh,J.Wu,S.Bills,L.Ouyang,J.Ward,and [326] H.Du,S.Liu,L.Zheng,Y.Cao,A.Nakamura,andL.Chen,
J. Leike, “Self-critiquing models for assisting human evalua- “Privacy in fine-tuning large language models: Attacks, de-
tors,”arXivpreprintarXiv:2206.05802,2022. 20 fenses,andfuturedirections,”2024. 20,21,22
[305] J.Kaplan,S.McCandlish,T.Henighan,T.B.Brown,B.Chess, [327] L.Yuan,W.Li,H.Chen,G.Cui,N.Ding,K.Zhang,B.Zhou,
R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, Z. Liu, and H. Peng, “Free process rewards without process
“Scaling laws for neural language models,” arXiv preprint labels,”arXivpreprintarXiv:2412.01981,2024. 20
arXiv:2001.08361,2020. 20 [328] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani,
[306] S. Roy and D. Roth, “Solving general arithmetic word prob- D.Jayaraman,Y.Zhu,L.Fan,andA.Anandkumar,“Eureka:
lems,”2016. 20 Human-levelrewarddesignviacodinglargelanguagemodels,”
[307] Y. Jinnai, T. Morimura, K. Ariu, and K. Abe, “Regularized ArXiv,vol.abs/2310.12931,2023. 20
best-of-n sampling to mitigate reward hacking for language [329] A. Havrilla, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu,
modelalignment,”arXivpreprintarXiv:2404.01054,2024. 20 M.Zhuravinskyi,E.Hambro,andR.Railneau,“Glore:When,
[308] L. Chen, C. Zhu, D. Soselia, J. Chen, T. Zhou, T. Goldstein, where,andhowtoimprovellmreasoningviaglobalandlocal
H.Huang,M.Shoeybi,andB.Catanzaro,“Odin:Disentangled refinements,”ArXiv,vol.abs/2402.10963,2024. 20
rewardmitigateshackinginrlhf,”ArXiv,vol.abs/2402.07319, [330] M.Fawi,“Curlora:Stablellmcontinualfine-tuningandcatas-
2024. 20 trophicforgettingmitigation,”2024. 20
[309] T. Liu, W. Xiong, J. Ren, L. Chen, J. Wu, R. Joshi, Y. Gao, [331] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu,
J.Shen,Z.Qin,T.Yu,D.Sohn,A.Makarova,J.Liu,Y.Liu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji, “Mme:
B. Piot, A. Ittycheriah, A. Kumar, and M. Saleh, “Rrm: Ro- A comprehensive evaluation benchmark for multimodal large
bustrewardmodeltrainingmitigatesrewardhacking,”ArXiv, languagemodels,”ArXiv,vol.abs/2306.13394,2023. 20
vol.abs/2409.13156,2024. 20 [332] Y.Wang,Z.Yu,Z.Zeng,L.Yang,C.Wang,H.Chen,C.Jiang,
[310] C.Wang,Z.Zhao,Y.Jiang,Z.Chen,C.Zhu,Y.Chen,J.Liu, R. Xie, J. Wang, X. Xie, W. Ye, S.-B. Zhang, and Y. Zhang,
L. Zhang, X. Fan, H. Ma, and S.-Y. Wang, “Beyond reward “Pandalm:Anautomaticevaluationbenchmarkforllminstruc-
hacking:Causalrewardsforlargelanguagemodelalignment,” tiontuningoptimization,”ArXiv,vol.abs/2306.05087,2023.20
31
[333] Y.Sun,Z.Li,Y.Li,andB.Ding,“Improvinglorainprivacy- [353] S.Lee,G.Lee,J.W.Kim,J.Shin,andM.-K.Lee,“Hetal:Ef-
preserving federated learning,” ArXiv, vol. abs/2403.12313, ficientprivacy-preservingtransferlearningwithhomomorphic
2024. 20,21 encryption,”2024. 22
[334] Y.He,Y.Kang,L.Fan,andQ.Yang,“Fedeval-llm:Federated [354] Y.Wei,J.Jia,Y.Wu,C.Hu,C.Dong,Z.Liu,X.Chen,Y.Peng,
evaluationoflargelanguagemodelsondownstreamtaskswith and S. Wang, “Distributed differential privacy via shuffling
collectivewisdom,”arXivpreprintarXiv:2404.12273,2024. 20 versus aggregation: A curious study,” IEEE Transactions on
[335] J. Park, S. Jwa, M. Ren, D. Kim, and S. Choi, “Offsetbias: Information Forensics and Security, vol. 19, pp. 2501–2516,
Leveragingdebiaseddatafortuningevaluators,”arXivpreprint 2024. 22
arXiv:2407.06551,2024. 20 [355] W.Zhang,K.Tang,H.Wu,M.Wang,Y.Shen,G.Hou,Z.Tan,
[336] P.Lu,L.Qiu,K.-W.Chang,Y.N.Wu,S.-C.Zhu,T.Rajpuro- P.Li,Y.Zhuang,andW.Lu,“Agent-pro:Learningtoevolve
hit, P. Clark, and A. Kalyan, “Dynamic prompt learning via via policy-level reflection and optimization,” arXiv preprint
policy gradient for semi-structured mathematical reasoning,” arXiv:2402.17574,2024. 22
2023. 20 [356] C. Ma, J. Zhang, Z. Zhu, C. Yang, Y. Yang, Y. Jin,
[337] L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, Z. Lan, L. Kong, and J. He, “Agentboard: An analytical
and R. Agarwal, “Generative verifiers: Reward modeling as evaluation board of multi-turn llm agents,” arXiv preprint
next-tokenprediction,”inThe4thWorkshoponMathematical arXiv:2401.13178,2024. 22
ReasoningandAIatNeurIPS’24,2024. 20 [357] J.Zhang,J.Xiang,Z.Yu,F.Teng,X.Chen,J.Chen,M.Zhuge,
[338] S. Yang and D. Song, “FPC: Fine-tuning with prompt cur- X.Cheng,S.Hong,J.Wang,etal.,“Aflow:Automatingagentic
riculum for relation extraction,” in Proceedings of the 2nd workflowgeneration,”arXivpreprintarXiv:2410.10762,2024.
ConferenceoftheAsia-PacificChapteroftheAssociationfor 22
Computational Linguistics and the 12th International Joint [358] H.D.Le,X.Xia,andZ.Chen,“Multi-agentcausaldiscoveryus-
ConferenceonNaturalLanguageProcessing(Volume1:Long inglargelanguagemodels,”arXivpreprintarXiv:2407.15073,
Papers) (Y. He, H. Ji, S. Li, Y. Liu, and C.-H. Chang, eds.), 2024. 22
(Online only), pp. 1065–1077, Association for Computational [359] D. M. Owens, R. A. Rossi, S. Kim, T. Yu, F. Dernon-
Linguistics,Nov.2022. 20 court, X. Chen, R. Zhang, J. Gu, H. Deilamsalehy, and
[339] Y.Yu,W.Ping,Z.Liu,B.Wang,J.You,C.Zhang,M.Shoeybi, N. Lipka, “A multi-llm debiasing framework,” arXiv preprint
and B. Catanzaro, “Rankrag: Unifying context ranking with arXiv:2409.13884,2024. 22
retrieval-augmented generation in llms,” Advances in Neural [360] H.Zou,Q.Zhao,L.Bariah,Y.Tian,M.Bennis,S.Lasaulce,
Information Processing Systems, vol. 37, pp. 121156–121184, M.Debbah,andF.Bader,“Genainet:Enablingwirelesscollec-
2025. 20 tiveintelligenceviaknowledgetransferandreasoning,”ArXiv,
[340] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, vol.abs/2402.16631,2024. 22
P. Rodriguez, J. Kahn, G. Szilvasy, M. Lewis, et al., “Ra-dit: [361] R. Lee, O. J. Mengshoel, A. Saksena, R. Gardner, D. Genin,
Retrieval-augmenteddualinstructiontuning,”inTheTwelfth J. Silbermann, M. Owen, and M. J. Kochenderfer, “Adaptive
International Conference on Learning Representations, 2023. stresstesting:Findinglikelyfailureeventswithreinforcement
20 learning,”2020. 22
[341] T.Zhang,S.G.Patil,N.Jain,S.Shen,M.Zaharia,I.Stoica, [362] A. G. Baydin, B. A. Pearlmutter, D. Syme, F. Wood, and
andJ.E.Gonzalez,“Raft:Adaptinglanguagemodeltodomain P.Torr,“Gradientswithoutbackpropagation,”2022. 22
specificrag,”inFirstConferenceonLanguageModeling,2024. [363] Y.Liu,C.Cai,X.Zhang,X.Yuan,andC.Wang,“Arondight:
20 Redteaminglargevisionlanguagemodelswithauto-generated
[342] T.Huang,S.Hu,F.Ilhan,S.F.Tekin,andL.Liu,“Harmful multi-modaljailbreakprompts,”inACMMultimedia,2024. 22
fine-tuning attacks and defenses for large language models: A [364] D.Kim,K.Lee,J.Shin,andJ.Kim,“Aligninglargelanguage
survey,”arXivpreprintarXiv:2409.18169,2024. 21 models with self-generated preference data,” arXiv preprint
[343] S.R.Bowman,J.Hyun,E.Perez,E.Chen,C.Pettit,S.Heiner, arXiv:2406.04412,2024. 22
K.Lukošiu¯te˙,A.Askell,A.Jones,A.Chen,etal.,“Measuring [365] S. Ebrahimi, S. Ö. Arik, T. Nama, and T. Pfister, “Crome:
progressonscalableoversightforlargelanguagemodels,”arXiv Cross-modal adapters for efficient multimodal llm,” ArXiv,
preprintarXiv:2211.03540,2022. 21 vol.abs/2408.06610,2024. 2,22
[344] N. Hollmann, S. Müller, and F. Hutter, “Llms for semi- [366] H.Xia,Y.Li,C.T.Leong,W.Wang,andW.Li,“Tokenskip:
automated data science: Introducing caafe for context-aware Controllablechain-of-thoughtcompressioninllms,”2025. 22
automatedfeatureengineering,”CoRR,2023. 21 [367] Z. Ma, W. Wu, Z. Zheng, Y. Guo, Q. Chen, S. Zhang, and
[345] S.R.Motwani,C.Smith,R.J.Das,M.Rybchuk,P.H.Torr, X.Chen,“Leveragingspeechptm,textllm,andemotionaltts
I. Laptev, F. Pizzati, R. Clark, and C. S. de Witt, “Malt: for speech emotion recognition,” ICASSP 2024 - 2024 IEEE
Improving reasoning with multi-agent llm training,” arXiv InternationalConferenceonAcoustics,SpeechandSignalPro-
preprintarXiv:2412.01928,2024. 21 cessing(ICASSP),pp.11146–11150,2023. 22
[346] A. Estornell, J.-F. Ton, Y. Yao, and Y. Liu, “Acc-debate: An [368] Z. Xi, W. Chen, B. Hong, S. Jin, R. Zheng, W. He, Y. Ding,
actor-critic approach to multi-agent debate,” arXiv preprint S.Liu,X.Guo,J.Wang,H.Guo,W.Shen,X.Fan,Y.Zhou,
arXiv:2411.00053,2024. 21 S. Dou, X. Wang, X. Zhang, P. Sun, T. Gui, Q. Zhang,
[347] L. Luo, Y. Liu, R. Liu, S. Phatale, H. Lara, Y. Li, L. Shu, andX.Huang,“Traininglargelanguagemodelsforreasoning
Y. Zhu, L. Meng, J. Sun, et al., “Improve mathematical rea- through reverse curriculum reinforcement learning,” ArXiv,
soninginlanguagemodelsbyautomatedprocesssupervision,” vol.abs/2402.05808,2024. 22
arXivpreprintarXiv:2406.06592,2024. 22 [369] O. Y. Lee, A. Xie, K. Fang, K. Pertsch, and C. Finn,
[348] W. Shen, X. Zhang, Y. Yao, R. Zheng, H. Guo, and Y. Liu, “Affordance-guided reinforcement learning via visual prompt-
“Improvingreinforcementlearningfromhumanfeedbackusing ing,”ArXiv,vol.abs/2407.10341,2024. 22
contrastive rewards,” arXiv preprint arXiv:2403.07708, 2024. [370] H.Xu,Z.Zhu,D.Ma,S.Zhang,S.Fan,L.Chen,andK.Yu,
22 “Rejection improves reliability: Training llms to refuse un-
[349] M. Ma, P. D’Oro, Y. Bengio, and P.-L. Bacon, “Long-term known questions using rl from knowledge feedback,” ArXiv,
credit assignment via model-based temporal shortcuts,” in vol.abs/2403.18349,2024. 22
DeepRLWorkshopNeurIPS2021,2021. 22 [371] X.Chen,J.Xu,T.Liang,Z.He,J.Pang,D.Yu,L.Song,Q.Liu,
[350] E. Pignatelli, J. Ferret, M. Geist, T. Mesnard, H. van Has- M.Zhou,Z.Zhang,R.Wang,Z.Tu,H.Mi,andD.Yu,“Donot
selt, O. Pietquin, and L. Toni, “A survey of temporal credit thinkthatmuchfor2+3=?ontheoverthinkingofo1-likellms,”
assignment in deep reinforcement learning,” arXiv preprint ArXiv,vol.abs/2412.21187,2024. 22
arXiv:2312.01072,2023. 22 [372] M. Kemmerling, D. Lütticke, and R. H. Schmitt, “Beyond
[351] H.ZhangandY.Guo,“Generalizationofreinforcementlearn- games: a systematic review of neural monte carlo tree search
ingwithpolicy-awareadversarialdataaugmentation,”2021.22 applications,” Applied Intelligence, vol. 54, no. 1, pp. 1020–
[352] A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, 1046,2024. 22
O. Pietquin, A. Üstün, and S. Hooker, “Back to basics: Re- [373] Y. Li, H. Wen, W. Wang, X. Li, Y. Yuan, G. Liu, J. Liu,
visiting reinforce style optimization for learning from human W.Xu,X.Wang,Y.Sun,R.Kong,Y.Wang,H.Geng,J.Luan,
feedbackinllms,”arXivpreprintarXiv:2402.14740,2024. 22 X.Jin,Z.-L.Ye,G.Xiong,F.Zhang,X.Li,M.Xu,Z.Li,P.Li,
32
Y. Liu, Y. Zhang, and Y. Liu, “Personal llm agents: Insights
andsurveyaboutthecapability,efficiencyandsecurity,”ArXiv,
vol.abs/2401.05459,2024. 22
[374] H.Li,L.Ding,M.Fang,andD.Tao,“Revisitingcatastrophic
forgettinginlargelanguagemodeltuning,”2024. 22
[375] N.Alzahrani,H.A.Alyahya,Y.Alnumay,S.Alrashed,S.Al-
subaie,Y.Almushaykeh,F.Mirza,N.Alotaibi,N.Altwairesh,
A. Alowisheq, M. S. Bari, and H. Khan, “When benchmarks
are targets: Revealing the sensitivity of large language model
leaderboards,”2024. 23
